{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import chdir\n",
    "chdir('/home/jovyan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/kubernetes.txt\", \"r\", encoding='utf8') as file:\n",
    "    data = file.read()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brendan Burns,\\nJoe Beda & Kelsey Hightower\\nKubernetes\\n Up & Running\\nDive into the Future of Infrastructure\\nSecond\\nEdition\\n\\nBrendan Burns, Joe Beda, and Kelsey Hightower\\nKubernetes: Up and Running\\nDive into the Future of Infrastructure\\nSECOND EDITION\\nBeijing Boston Farnham Sebastopol Tokyo\\n978-1-492-04653-0\\n[LSI]\\nKubernetes: Up and Running\\nby Brendan Burns, Joe Beda, and Kelsey Hightower\\nCopyright © 2019 Brendan Burns, Joe Beda, and Kelsey Hightower. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com.\\nAcquisition Editor: John Devins\\nDevelopment Editor: Virginia Wilson\\nProduction Editor: Kristen Brown\\nCopyeditor: Kim Cofer\\nProofreader: Rachel Head\\nIndexer: Ellen Troutman-Zaig\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Rebecca Demarest\\nSeptember 2017: First Edition\\nAugust 2019: Second Edition\\nRevision History for the Second Edition\\n2019-07-15: First Release\\n2019-10-04: Second Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492046530 for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Kubernetes: Up and Running, the cover\\nimage, and related trade dress are trademarks of O’Reilly Media, Inc.\\nThe views expressed in this work are those of the authors, and do not represent the publisher’s views.\\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.\\nThis work is part of a collaboration between O’Reilly and Microsoft. See our statement of editorial inde‐\\npendence.\\nFor Robin, Julia, Ethan, and everyone who bought cookies to pay for that Commodore\\n64 in my third-grade class.\\n—Brendan Burns\\nFor my Dad, who helped me fall in love with computers by bringing home punch cards\\nand dot matrix banners.\\n—Joe Beda\\nFor Klarissa and Kelis, who keep me sane. And for my Mom, who taught me a strong\\nwork ethic and how to rise above all odds.\\n—Kelsey Hightower\\n\\nTable of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii\\n1. Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\nVelocity 2\\nThe Value of Immutability 3\\nDeclarative Configuration 4\\nSelf-Healing Systems 5\\nScaling Your Service and Your Teams 5\\nDecoupling 6\\nEasy Scaling for Applications and Clusters 6\\nScaling Development Teams with Microservices 7\\nSeparation of Concerns for Consistency and Scaling 8\\nAbstracting Your Infrastructure 9\\nEfficiency 10\\nSummary 11\\n2. Creating and Running Containers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\nContainer Images 14\\nThe Docker Image Format 15\\nBuilding Application Images with Docker 16\\nDockerfiles 16\\nOptimizing Image Sizes 18\\nImage Security 19\\nMultistage Image Builds 20\\nStoring Images in a Remote Registry 22\\nThe Docker Container Runtime 23\\nRunning Containers with Docker 23\\nExploring the kuard Application 23\\nv\\nLimiting Resource Usage 24\\nCleanup 24\\nSummary 25\\n3. Deploying a Kubernetes Cluster. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\nInstalling Kubernetes on a Public Cloud Provider 28\\nGoogle Kubernetes Engine 28\\nInstalling Kubernetes with Azure Kubernetes Service 28\\nInstalling Kubernetes on Amazon Web Services 29\\nInstalling Kubernetes Locally Using minikube 29\\nRunning Kubernetes in Docker 30\\nRunning Kubernetes on Raspberry Pi 31\\nThe Kubernetes Client 31\\nChecking Cluster Status 31\\nListing Kubernetes Worker Nodes 32\\nCluster Components 34\\nKubernetes Proxy 34\\nKubernetes DNS 34\\nKubernetes UI 35\\nSummary 36\\n4. Common kubectl Commands. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\nNamespaces 37\\nContexts 37\\nViewing Kubernetes API Objects 38\\nCreating, Updating, and Destroying Kubernetes Objects 39\\nLabeling and Annotating Objects 40\\nDebugging Commands 40\\nCommand Autocompletion 42\\nAlternative Ways of Viewing Your Cluster 42\\nSummary 43\\n5. Pods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\nPods in Kubernetes 46\\nThinking with Pods 46\\nThe Pod Manifest 47\\nCreating a Pod 48\\nCreating a Pod Manifest 48\\nRunning Pods 49\\nListing Pods 49\\nPod Details 50\\nDeleting a Pod 51\\nvi | Table of Contents\\nAccessing Your Pod 52\\nUsing Port Forwarding 52\\nGetting More Info with Logs 52\\nRunning Commands in Your Container with exec 53\\nCopying Files to and from Containers 53\\nHealth Checks 54\\nLiveness Probe 54\\nReadiness Probe 55\\nTypes of Health Checks 56\\nResource Management 56\\nResource Requests: Minimum Required Resources 56\\nCapping Resource Usage with Limits 58\\nPersisting Data with Volumes 59\\nUsing Volumes with Pods 59\\nDifferent Ways of Using Volumes with Pods 60\\nPersisting Data Using Remote Disks 61\\nPutting It All Together 61\\nSummary 63\\n6. Labels and Annotations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\nLabels 65\\nApplying Labels 67\\nModifying Labels 68\\nLabel Selectors 68\\nLabel Selectors in API Objects 70\\nLabels in the Kubernetes Architecture 71\\nAnnotations 71\\nDefining Annotations 72\\nCleanup 73\\nSummary 73\\n7. Service Discovery. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\\nWhat Is Service Discovery? 75\\nThe Service Object 76\\nService DNS 77\\nReadiness Checks 78\\nLooking Beyond the Cluster 79\\nCloud Integration 81\\nAdvanced Details 82\\nEndpoints 82\\nManual Service Discovery 83\\nkube-proxy and Cluster IPs 84\\nTable of Contents | vii\\nCluster IP Environment Variables 85\\nConnecting with Other Environments 86\\nCleanup 86\\nSummary 86\\n8. HTTP Load Balancing with Ingress. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\\nIngress Spec Versus Ingress Controllers 90\\nInstalling Contour 91\\nConfiguring DNS 92\\nConfiguring a Local hosts File 92\\nUsing Ingress 92\\nSimplest Usage 93\\nUsing Hostnames 94\\nUsing Paths 95\\nCleaning Up 96\\nAdvanced Ingress Topics and Gotchas 96\\nRunning Multiple Ingress Controllers 97\\nMultiple Ingress Objects 97\\nIngress and Namespaces 97\\nPath Rewriting 98\\nServing TLS 98\\nAlternate Ingress Implementations 99\\nThe Future of Ingress 100\\nSummary 101\\n9. ReplicaSets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\\nReconciliation Loops 104\\nRelating Pods and ReplicaSets 104\\nAdopting Existing Containers 105\\nQuarantining Containers 105\\nDesigning with ReplicaSets 105\\nReplicaSet Spec 106\\nPod Templates 106\\nLabels 107\\nCreating a ReplicaSet 107\\nInspecting a ReplicaSet 108\\nFinding a ReplicaSet from a Pod 108\\nFinding a Set of Pods for a ReplicaSet 108\\nScaling ReplicaSets 109\\nImperative Scaling with kubectl scale 109\\nDeclaratively Scaling with kubectl apply 109\\nAutoscaling a ReplicaSet 110\\nviii | Table of Contents\\nDeleting ReplicaSets 111\\nSummary 112\\n10. Deployments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\\nYour First Deployment 114\\nDeployment Internals 114\\nCreating Deployments 116\\nManaging Deployments 117\\nUpdating Deployments 118\\nScaling a Deployment 118\\nUpdating a Container Image 119\\nRollout History 120\\nDeployment Strategies 123\\nRecreate Strategy 123\\nRollingUpdate Strategy 123\\nSlowing Rollouts to Ensure Service Health 126\\nDeleting a Deployment 128\\nMonitoring a Deployment 128\\nSummary 129\\n11. DaemonSets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\\nDaemonSet Scheduler 132\\nCreating DaemonSets 132\\nLimiting DaemonSets to Specific Nodes 134\\nAdding Labels to Nodes 135\\nNode Selectors 135\\nUpdating a DaemonSet 136\\nRolling Update of a DaemonSet 136\\nDeleting a DaemonSet 137\\nSummary 138\\n12. Jobs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\\nThe Job Object 139\\nJob Patterns 140\\nOne Shot 140\\nParallelism 144\\nWork Queues 146\\nCronJobs 150\\nSummary 151\\n13. Con\\x80gMaps and Secrets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\\nConfigMaps 153\\nTable of Contents | ix\\nCreating ConfigMaps 153\\nUsing a ConfigMap 154\\nSecrets 157\\nCreating Secrets 158\\nConsuming Secrets 159\\nPrivate Docker Registries 160\\nNaming Constraints 161\\nManaging ConfigMaps and Secrets 162\\nListing 162\\nCreating 163\\nUpdating 163\\nSummary 165\\n14. Role-Based Access Control for Kubernetes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\nRole-Based Access Control 168\\nIdentity in Kubernetes 168\\nUnderstanding Roles and Role Bindings 169\\nRoles and Role Bindings in Kubernetes 169\\nTechniques for Managing RBAC 172\\nTesting Authorization with can-i 172\\nManaging RBAC in Source Control 172\\nAdvanced Topics 172\\nAggregating ClusterRoles 173\\nUsing Groups for Bindings 173\\nSummary 175\\n15. Integrating Storage Solutions and Kubernetes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\\nImporting External Services 178\\nServices Without Selectors 179\\nLimitations of External Services: Health Checking 181\\nRunning Reliable Singletons 181\\nRunning a MySQL Singleton 181\\nDynamic Volume Provisioning 185\\nKubernetes-Native Storage with StatefulSets 186\\nProperties of StatefulSets 187\\nManually Replicated MongoDB with StatefulSets 187\\nAutomating MongoDB Cluster Creation 189\\nPersistent Volumes and StatefulSets 192\\nOne Final Thing: Readiness Probes 193\\nSummary 194\\nx | Table of Contents\\n16. Extending Kubernetes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\\nWhat It Means to Extend Kubernetes 195\\nPoints of Extensibility 196\\nPatterns for Custom Resources 204\\nJust Data 204\\nCompilers 205\\nOperators 205\\nGetting Started 205\\nSummary 205\\n17. Deploying Real-World Applications. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\\nJupyter 207\\nParse 209\\nPrerequisites 209\\nBuilding the parse-server 209\\nDeploying the parse-server 209\\nTesting Parse 210\\nGhost 211\\nConfiguring Ghost 211\\nRedis 214\\nConfiguring Redis 215\\nCreating a Redis Service 216\\nDeploying Redis 217\\nPlaying with Our Redis Cluster 218\\nSummary 219\\n18. Organizing Your Application. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221\\nPrinciples to Guide Us 221\\nFilesystems as the Source of Truth 222\\nThe Role of Code Review 222\\nFeature Gates and Guards 223\\nManaging Your Application in Source Control 224\\nFilesystem Layout 224\\nManaging Periodic Versions 225\\nStructuring Your Application for Development, Testing, and Deployment 227\\nGoals 227\\nProgression of a Release 227\\nParameterizing Your Application with Templates 229\\nParameterizing with Helm and Templates 229\\nFilesystem Layout for Parameterization 230\\nDeploying Your Application Around the World 230\\nArchitectures for Worldwide Deployment 230\\nTable of Contents | xi\\nImplementing Worldwide Deployment 232\\nDashboards and Monitoring for Worldwide Deployments 233\\nSummary 233\\nA. Building a Raspberry Pi Kubernetes Cluster. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\\nxii | Table of Contents\\nPreface\\nKubernetes: A Dedication\\nKubernetes would like to thank every sysadmin who has woken up at 3 a.m. to restart\\na process. Every developer who pushed code to production only to find that it didn’t\\nrun like it did on their laptop. Every systems architect who mistakenly pointed a load\\ntest at the production service because of a leftover hostname that they hadn’t updated.\\nIt was the pain, the weird hours, and the weird errors that inspired the development\\nof Kubernetes. In a single sentence: Kubernetes intends to radically simplify the task\\nof building, deploying, and maintaining distributed systems. It has been inspired by\\ndecades of real-world experience building reliable systems and it has been designed\\nfrom the ground up to make that experience if not euphoric, at least pleasant. We\\nhope you enjoy the book!\\nWho Should Read This Book\\nWhether you are new to distributed systems or have been deploying cloud-native sys‐\\ntems for years, containers and Kubernetes can help you achieve new levels of velocity,\\nagility, reliability, and efficiency. This book describes the Kubernetes cluster orches‐\\ntrator and how its tools and APIs can be used to improve the development, delivery,\\nand maintenance of distributed applications. Though no previous experience with\\nKubernetes is assumed, to make maximal use of the book you should be comfortable\\nbuilding and deploying server-based applications. Familiarity with concepts like load\\nbalancers and network storage will be useful, though not required. Likewise, experi‐\\nence with Linux, Linux containers, and Docker, though not essential, will help you\\nmake the most of this book.\\nxiii\\nWhy We Wrote This Book\\nWe have been involved with Kubernetes since its very beginnings. It has been truly\\nremarkable to watch it transform from a curiosity largely used in experiments to a\\ncrucial production-grade infrastructure that powers large-scale production applica‐\\ntions in varied fields, from machine learning to online services. As this transition\\noccurred, it became increasingly clear that a book that captured both how to use the\\ncore concepts in Kubernetes and the motivations behind the development of those\\nconcepts would be an important contribution to the state of cloud-native application\\ndevelopment. We hope that in reading this book, you not only learn how to build reli‐\\nable, scalable applications on top of Kubernetes but also receive insight into the core\\nchallenges of distributed systems that led to its development.\\nWhy We Updated This Book\\nIn the few years that have passed since we wrote the first edition of this book, the\\nKubernetes ecosystem has blossomed and evolved. Kubernetes itself has had many\\nreleases, and many more tools and patterns for using Kubernetes have become de\\nfacto standards. In updating the book we added material on HTTP load balancing,\\nrole-based access control (RBAC), extending the Kubernetes API, how to organize\\nyour application in source control, and more. We also updated all of the existing\\nchapters to reflect the changes and evolution in Kubernetes since the first edition. We\\nfully expect to revise this book again in a few years (and look forward to doing so) as\\nKubernetes continues to evolve.\\nA Word on Cloud-Native Applications Today\\nFrom the first programming languages, to object-oriented programming, to the\\ndevelopment of virtualization and cloud infrastructure, the history of computer sci‐\\nence is a history of the development of abstractions that hide complexity and\\nempower you to build ever more sophisticated applications. Despite this, the develop‐\\nment of reliable, scalable applications is still dramatically more challenging than it\\nought to be. In recent years, containers and container orchestration APIs like Kuber‐\\nnetes have proven to be an important abstraction that radically simplifies the devel‐\\nopment of reliable, scalable distributed systems. Though containers and orchestrators\\nare still in the process of entering the mainstream, they are already enabling develop‐\\ners to build and deploy applications with a speed, agility, and reliability that would\\nhave seemed like science fiction only a few years ago.\\nxiv | Preface\\nNavigating This Book\\nThis book is organized as follows. Chapter 1 outlines the high-level benefits of Kuber‐\\nnetes without diving too deeply into the details. If you are new to Kubernetes, this is a\\ngreat place to start to understand why you should read the rest of the book.\\nChapter 2 provides a detailed introduction to containers and containerized applica‐\\ntion development. If you’ve never really played around with Docker before, this chap‐\\nter will be a useful introduction. If you are already a Docker expert, it will likely be\\nmostly review.\\nChapter 3 covers how to deploy Kubernetes. While most of this book focuses on how\\nto use Kubernetes, you need to get a cluster up and running before you start using it.\\nAlthough running a cluster for production is out of the scope of this book, this chap‐\\nter presents a couple of easy ways to create a cluster so that you can understand how\\nto use Kubernetes. Chapter 4 covers a selection of common commands used to inter‐\\nact with a Kubernetes cluster.\\nStarting with Chapter 5, we dive into the details of deploying an application using\\nKubernetes. We cover Pods (Chapter 5), labels and annotations (Chapter 6), services\\n(Chapter 7), Ingress (Chapter 8), and ReplicaSets (Chapter 9). These form the core\\nbasics of what you need to deploy your service in Kubernetes. We then cover deploy‐\\nments (Chapter 10), which tie together the lifecycle of a complete application.\\nAfter those chapters, we cover some more specialized objects in Kubernetes: Dae‐\\nmonSets (Chapter 11), Jobs (Chapter 12), and ConfigMaps and secrets (Chapter 13).\\nWhile these chapters are essential for many production applications, if you are just\\nlearning Kubernetes you can skip them and return to them later, after you gain more\\nexperience and expertise.\\nNext we cover integrating storage into Kubernetes (Chapter 15). We discuss extend‐\\ning Kubernetes in Chapter 16. Finally, we conclude with some examples of how to\\ndevelop and deploy real-world applications in Kubernetes (Chapter 17) and a discus‐\\nsion of how to organize your applications in source control (Chapter 18).\\nOnline Resources\\nYou will want to install Docker. You likely will also want to familiarize yourself with\\nthe Docker documentation if you have not already done so.\\nLikewise, you will want to install the kubectl command-line tool. You may also want\\nto join the Kubernetes Slack channel, where you will find a large community of users\\nwho are willing to talk and answer questions at nearly any hour of the day.\\nFinally, as you grow more advanced, you may want to engage with the open source\\nKubernetes repository on GitHub.\\nPreface | xv\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nThis icon signifies a tip, suggestion, or general note.\\nThis icon indicates a warning or caution.\\nUsing Code Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/kubernetes-up-and-running/examples.\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. You do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nxvi | Preface\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “Kubernetes: Up and Running, 2nd\\nedition, by Brendan Burns, Joe Beda, and Kelsey Hightower (O’Reilly). Copyright\\n2019 Brendan Burns, Joe Beda, and Kelsey Hightower, 978-1-492-04653-0.”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com.\\nO’Reilly Online Learning\\nFor almost 40 years, O’Reilly Media has provided technology\\nand business training, knowledge, and insight to help compa‐\\nnies succeed.\\nOur unique network of experts and innovators share their knowledge and expertise\\nthrough books, articles, conferences, and our online learning platform. O’Reilly’s\\nonline learning platform gives you on-demand access to live training courses, indepth learning paths, interactive coding environments, and a vast collection of text\\nand video from O’Reilly and 200+ other publishers. For more information, please\\nvisit http://oreilly.com.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. You can access this page at http://bit.ly/kubernetesUR_2e.\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com.\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com.\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nPreface | xvii\\nWatch us on YouTube: http://www.youtube.com/oreillymedia\\nAcknowledgments\\nWe would like to acknowledge everyone who helped us develop this book. This\\nincludes our editor Virginia Wilson and all of the great folks at O’Reilly, as well as the\\ntechnical reviewers who provided tremendous feedback that significantly improved\\nthe book. Finally, we would like to thank all of our first edition readers who took the\\ntime to report errata that were found and fixed in this second edition. Thank you all!\\nWe’re very grateful.\\nxviii | Preface\\n1 Brendan Burns et al., “Borg, Omega, and Kubernetes: Lessons Learned from Three Container-Management\\nSystems over a Decade,” ACM Queue 14 (2016): 70–93, available at http://bit.ly/2vIrL4S.\\nCHAPTER 1\\nIntroduction\\nKubernetes is an open source orchestrator for deploying containerized applications. It\\nwas originally developed by Google, inspired by a decade of experience deploying\\nscalable, reliable systems in containers via application-oriented APIs.1\\nSince its introduction in 2014, Kubernetes has grown to be one of the largest and\\nmost popular open source projects in the world. It has become the standard API for\\nbuilding cloud-native applications, present in nearly every public cloud. Kubernetes\\nis a proven infrastructure for distributed systems that is suitable for cloud-native\\ndevelopers of all scales, from a cluster of Raspberry Pi computers to a warehouse full\\nof the latest machines. It provides the software necessary to successfully build and\\ndeploy reliable, scalable distributed systems.\\nYou may be wondering what we mean when we say “reliable, scalable distributed sys‐\\ntems.” More and more services are delivered over the network via APIs. These APIs\\nare often delivered by a distributed system, the various pieces that implement the API\\nrunning on different machines, connected via the network and coordinating their\\nactions via network communication. Because we rely on these APIs increasingly for\\nall aspects of our daily lives (e.g., finding directions to the nearest hospital), these sys‐\\ntems must be highly reliable. They cannot fail, even if a part of the system crashes or\\notherwise stops working. Likewise, they must maintain availability even during soft‐\\nware rollouts or other maintenance events. Finally, because more and more of the\\nworld is coming online and using such services, they must be highly scalable so that\\nthey can grow their capacity to keep up with ever-increasing usage without radical\\nredesign of the distributed system that implements the services.\\n1\\nDepending on when and why you have come to hold this book in your hands, you\\nmay have varying degrees of experience with containers, distributed systems, and\\nKubernetes. You may be planning on building your application on top of public cloud\\ninfrastructure, in private data centers, or in some hybrid environment. Regardless of\\nwhat your experience is, we believe this book will enable you to make the most of\\nyour use of Kubernetes.\\nThere are many reasons why people come to use containers and container APIs like\\nKubernetes, but we believe they can all be traced back to one of these benefits:\\n• Velocity\\n• Scaling (of both software and teams)\\n• Abstracting your infrastructure\\n• Efficiency\\nIn the following sections, we describe how Kubernetes can help provide each of these\\nfeatures.\\nVelocity\\nVelocity is the key component in nearly all software development today. The software\\nindustry has evolved from shipping products as boxed CDs or DVDs to software that\\nis delivered over the network via web-based services that are updated hourly. This\\nchanging landscape means that the difference between you and your competitors is\\noften the speed with which you can develop and deploy new components and fea‐\\ntures, or the speed with which you can respond to innovations developed by others.\\nIt is important to note, however, that velocity is not defined in terms of simply raw\\nspeed. While your users are always looking for iterative improvement, they are more\\ninterested in a highly reliable service. Once upon a time, it was OK for a service to be\\ndown for maintenance at midnight every night. But today, all users expect constant\\nuptime, even if the software they are running is changing constantly.\\nConsequently, velocity is measured not in terms of the raw number of features you\\ncan ship per hour or day, but rather in terms of the number of things you can ship\\nwhile maintaining a highly available service.\\nIn this way, containers and Kubernetes can provide the tools that you need to move\\nquickly, while staying available. The core concepts that enable this are:\\n• Immutability\\n• Declarative configuration\\n• Online self-healing systems\\n2 | Chapter 1: Introduction\\nThese ideas all interrelate to radically improve the speed with which you can reliably\\ndeploy software.\\nThe Value of Immutability\\nContainers and Kubernetes encourage developers to build distributed systems that\\nadhere to the principles of immutable infrastructure. With immutable infrastructure,\\nonce an artifact is created in the system it does not change via user modifications.\\nTraditionally, computers and software systems have been treated as mutable infra‐\\nstructure. With mutable infrastructure, changes are applied as incremental updates to\\nan existing system. These updates can occur all at once, or spread out across a long\\nperiod of time. A system upgrade via the apt-get update tool is a good example of\\nan update to a mutable system. Running apt sequentially downloads any updated\\nbinaries, copies them on top of older binaries, and makes incremental updates to\\nconfiguration files. With a mutable system, the current state of the infrastructure is\\nnot represented as a single artifact, but rather an accumulation of incremental\\nupdates and changes over time. On many systems these incremental updates come\\nfrom not just system upgrades, but operator modifications as well. Furthermore, in\\nany system run by a large team, it is highly likely that these changes will have been\\nperformed by many different people, and in many cases will not have been recorded\\nanywhere.\\nIn contrast, in an immutable system, rather than a series of incremental updates and\\nchanges, an entirely new, complete image is built, where the update simply replaces\\nthe entire image with the newer image in a single operation. There are no incremental\\nchanges. As you can imagine, this is a significant shift from the more traditional\\nworld of configuration management.\\nTo make this more concrete in the world of containers, consider two different ways to\\nupgrade your software:\\n1. You can log in to a container, run a command to download your new software,\\nkill the old server, and start the new one.\\n2. You can build a new container image, push it to a container registry, kill the exist‐\\ning container, and start a new one.\\nAt first blush, these two approaches might seem largely indistinguishable. So what is\\nit about the act of building a new container that improves reliability?\\nThe key differentiation is the artifact that you create, and the record of how you cre‐\\nated it. These records make it easy to understand exactly the differences in some new\\nversion and, if something goes wrong, to determine what has changed and how to fix\\nit.\\nVelocity | 3\\nAdditionally, building a new image rather than modifying an existing one means the\\nold image is still around, and can quickly be used for a rollback if an error occurs. In\\ncontrast, once you copy your new binary over an existing binary, such a rollback is\\nnearly impossible.\\nImmutable container images are at the core of everything that you will build in\\nKubernetes. It is possible to imperatively change running containers, but this is an\\nanti-pattern to be used only in extreme cases where there are no other options (e.g., if\\nit is the only way to temporarily repair a mission-critical production system). And\\neven then, the changes must also be recorded through a declarative configuration\\nupdate at some later time, after the fire is out.\\nDeclarative Con\\x80guration\\nImmutability extends beyond containers running in your cluster to the way you\\ndescribe your application to Kubernetes. Everything in Kubernetes is a declarative\\nconfiguration object that represents the desired state of the system. It is the job of\\nKubernetes to ensure that the actual state of the world matches this desired state.\\nMuch like mutable versus immutable infrastructure, declarative configuration is an\\nalternative to imperative configuration, where the state of the world is defined by the\\nexecution of a series of instructions rather than a declaration of the desired state of\\nthe world. While imperative commands define actions, declarative configurations\\ndefine state.\\nTo understand these two approaches, consider the task of producing three replicas of\\na piece of software. With an imperative approach, the configuration would say “run\\nA, run B, and run C.” The corresponding declarative configuration would be “replicas\\nequals three.”\\nBecause it describes the state of the world, declarative configuration does not have to\\nbe executed to be understood. Its impact is concretely declared. Since the effects of\\ndeclarative configuration can be understood before they are executed, declarative\\nconfiguration is far less error-prone. Further, the traditional tools of software devel‐\\nopment, such as source control, code review, and unit testing, can be used in declara‐\\ntive configuration in ways that are impossible for imperative instructions. The idea of\\nstoring declarative configuration in source control is often referred to as “infrastruc‐\\nture as code.”\\nThe combination of declarative state stored in a version control system and the ability\\nof Kubernetes to make reality match this declarative state makes rollback of a change\\ntrivially easy. It is simply restating the previous declarative state of the system. This is\\nusually impossible with imperative systems, because although the imperative instruc‐\\ntions describe how to get you from point A to point B, they rarely include the reverse\\ninstructions that can get you back.\\n4 | Chapter 1: Introduction\\nSelf-Healing Systems\\nKubernetes is an online, self-healing system. When it receives a desired state configu‐\\nration, it does not simply take a set of actions to make the current state match the\\ndesired state a single time. It continuously takes actions to ensure that the current state\\nmatches the desired state. This means that not only will Kubernetes initialize your\\nsystem, but it will guard it against any failures or perturbations that might destabilize\\nthe system and affect reliability.\\nA more traditional operator repair involves a manual series of mitigation steps, or\\nhuman intervention performed in response to some sort of alert. Imperative repair\\nlike this is more expensive (since it generally requires an on-call operator to be avail‐\\nable to enact the repair). It is also generally slower, since a human must often wake up\\nand log in to respond. Furthermore, it is less reliable because the imperative series of\\nrepair operations suffers from all of the problems of imperative management\\ndescribed in the previous section. Self-healing systems like Kubernetes both reduce\\nthe burden on operators and improve the overall reliability of the system by perform‐\\ning reliable repairs more quickly.\\nAs a concrete example of this self-healing behavior, if you assert a desired state of\\nthree replicas to Kubernetes, it does not just create three replicas—it continuously\\nensures that there are exactly three replicas. If you manually create a fourth replica,\\nKubernetes will destroy one to bring the number back to three. If you manually\\ndestroy a replica, Kubernetes will create one to again return you to the desired state.\\nOnline self-healing systems improve developer velocity because the time and energy\\nyou might otherwise have spent on operations and maintenance can instead be spent\\non developing and testing new features.\\nIn a more advanced form of self-healing, there has been significant recent work in the\\noperator paradigm for Kubernetes. With operators, more advanced logic needed to\\nmaintain, scale, and heal a specific piece of software (MySQL, for example) is enco‐\\nded into an operator application that runs as a container in the cluster. The code in\\nthe operator is responsible for more targeted and advanced health detection and heal‐\\ning than can be achieved via Kubernetes’s generic self-healing. Often this is packaged\\nup as “operators,” which are discussed in a later section.\\nScaling Your Service and Your Teams\\nAs your product grows, it’s inevitable that you will need to scale both your software\\nand the teams that develop it. Fortunately, Kubernetes can help with both of these\\ngoals. Kubernetes achieves scalability by favoring decoupled architectures.\\nScaling Your Service and Your Teams | 5\\nDecoupling\\nIn a decoupled architecture, each component is separated from other components by\\ndefined APIs and service load balancers. APIs and load balancers isolate each piece of\\nthe system from the others. APIs provide a buffer between implementer and con‐\\nsumer, and load balancers provide a buffer between running instances of each\\nservice.\\nDecoupling components via load balancers makes it easy to scale the programs that\\nmake up your service, because increasing the size (and therefore the capacity) of the\\nprogram can be done without adjusting or reconfiguring any of the other layers of\\nyour service.\\nDecoupling servers via APIs makes it easier to scale the development teams because\\neach team can focus on a single, smaller microservice with a comprehensible surface\\narea. Crisp APIs between microservices limit the amount of cross-team communica‐\\ntion overhead required to build and deploy software. This communication overhead\\nis often the major restricting factor when scaling teams.\\nEasy Scaling for Applications and Clusters\\nConcretely, when you need to scale your service, the immutable, declarative nature of\\nKubernetes makes this scaling trivial to implement. Because your containers are\\nimmutable, and the number of replicas is merely a number in a declarative config,\\nscaling your service upward is simply a matter of changing a number in a configura‐\\ntion file, asserting this new declarative state to Kubernetes, and letting it take care of\\nthe rest. Alternatively, you can set up autoscaling and let Kubernetes take care of it for\\nyou.\\nOf course, that sort of scaling assumes that there are resources available in your clus‐\\nter to consume. Sometimes you actually need to scale up the cluster itself. Again,\\nKubernetes makes this task easier. Because many machines in a cluster are entirely\\nidentical to other machines in that set and the applications themselves are decoupled\\nfrom the details of the machine by containers, adding additional resources to the\\ncluster is simply a matter of imaging a new machine of the same class and joining it\\ninto the cluster. This can be accomplished via a few simple commands or via a pre‐\\nbaked machine image.\\nOne of the challenges of scaling machine resources is predicting their use. If you are\\nrunning on physical infrastructure, the time to obtain a new machine is measured in\\ndays or weeks. On both physical and cloud infrastructure, predicting future costs is\\ndifficult because it is hard to predict the growth and scaling needs of specific\\napplications.\\nKubernetes can simplify forecasting future compute costs. To understand why this is\\ntrue, consider scaling up three teams, A, B, and C. Historically you have seen that\\n6 | Chapter 1: Introduction\\neach team’s growth is highly variable and thus hard to predict. If you are provisioning\\nindividual machines for each service, you have no choice but to forecast based on the\\nmaximum expected growth for each service, since machines dedicated to one team\\ncannot be used for another team. If instead you use Kubernetes to decouple the teams\\nfrom the specific machines they are using, you can forecast growth based on the\\naggregate growth of all three services. Combining three variable growth rates into a\\nsingle growth rate reduces statistical noise and produces a more reliable forecast of\\nexpected growth. Furthermore, decoupling the teams from specific machines means\\nthat teams can share fractional parts of one another’s machines, reducing even further\\nthe overheads associated with forecasting growth of computing resources.\\nScaling Development Teams with Microservices\\nAs noted in a variety of research, the ideal team size is the “two-pizza team,” or\\nroughly six to eight people. This group size often results in good knowledge sharing,\\nfast decision making, and a common sense of purpose. Larger teams tend to suffer\\nfrom issues of hierarchy, poor visibility, and infighting, which hinder agility and\\nsuccess.\\nHowever, many projects require significantly more resources to be successful and\\nachieve their goals. Consequently, there is a tension between the ideal team size for\\nagility and the necessary team size for the product’s end goals.\\nThe common solution to this tension has been the development of decoupled,\\nservice-oriented teams that each build a single microservice. Each small team is\\nresponsible for the design and delivery of a service that is consumed by other small\\nteams. The aggregation of all of these services ultimately provides the implementation\\nof the overall product’s surface area.\\nKubernetes provides numerous abstractions and APIs that make it easier to build\\nthese decoupled microservice architectures:\\n• Pods, or groups of containers, can group together container images developed by\\ndifferent teams into a single deployable unit.\\n• Kubernetes services provide load balancing, naming, and discovery to isolate one\\nmicroservice from another.\\n• Namespaces provide isolation and access control, so that each microservice can\\ncontrol the degree to which other services interact with it.\\n• Ingress objects provide an easy-to-use frontend that can combine multiple micro‐\\nservices into a single externalized API surface area.\\nFinally, decoupling the application container image and machine means that different\\nmicroservices can colocate on the same machine without interfering with one\\nanother, reducing the overhead and cost of microservice architectures. The healthScaling Your Service and Your Teams | 7\\nchecking and rollout features of Kubernetes guarantee a consistent approach to appli‐\\ncation rollout and reliability that ensures that a proliferation of microservice teams\\ndoes not also result in a proliferation of different approaches to service production\\nlifecycle and operations.\\nSeparation of Concerns for Consistency and Scaling\\nIn addition to the consistency that Kubernetes brings to operations, the decoupling\\nand separation of concerns produced by the Kubernetes stack lead to significantly\\ngreater consistency for the lower levels of your infrastructure. This enables you to\\nscale infrastructure operations to manage many machines with a single small, focused\\nteam. We have talked at length about the decoupling of application container and\\nmachine/operating system (OS), but an important aspect of this decoupling is that\\nthe container orchestration API becomes a crisp contract that separates the responsi‐\\nbilities of the application operator from the cluster orchestration operator. We call\\nthis the “not my monkey, not my circus” line. The application developer relies on the\\nservice-level agreement (SLA) delivered by the container orchestration API, without\\nworrying about the details of how this SLA is achieved. Likewise, the container\\norchestration API reliability engineer focuses on delivering the orchestration API’s\\nSLA without worrying about the applications that are running on top of it.\\nThis decoupling of concerns means that a small team running a Kubernetes cluster\\ncan be responsible for supporting hundreds or even thousands of teams running\\napplications within that cluster (Figure 1-1). Likewise, a small team can be responsi‐\\nble for dozens (or more) of clusters running around the world. It’s important to note\\nthat the same decoupling of containers and OS enables the OS reliability engineers to\\nfocus on the SLA of the individual machine’s OS. This becomes another line of sepa‐\\nrate responsibility, with the Kubernetes operators relying on the OS SLA, and the OS\\noperators worrying solely about delivering that SLA. Again, this enables you to scale a\\nsmall team of OS experts to a fleet of thousands of machines.\\nOf course, devoting even a small team to managing an OS is beyond the scale of\\nmany organizations. In these environments, a managed Kubernetes-as-a-Service\\n(KaaS) provided by a public cloud provider is a great option. As Kubernetes has\\nbecome increasingly ubiquitous, KaaS has become increasingly available as well, to\\nthe point where it is now offered on nearly every public cloud. Of course, using a\\nKaaS has some limitations, since the operator makes decisions for you about how the\\nKubernetes clusters are built and configured. For example, many KaaS platforms dis‐\\nable alpha features because they can destabilize the managed cluster.\\n8 | Chapter 1: Introduction\\nFigure 1-1. An illustration of how different operations teams are decoupled using APIs\\nIn addition to a fully managed Kubernetes service, there is a thriving ecosystem of\\ncompanies and projects that help to install and manage Kubernetes. There is a full\\nspectrum of solutions between doing it “the hard way” and a fully managed service.\\nConsequently, the decision of whether to use KaaS or manage it yourself (or some‐\\nthing in between) is one each user needs to make based on the skills and demands of\\ntheir situation. Often for small organizations, KaaS provides an easy-to-use solution\\nthat enables them to focus their time and energy on building the software to support\\ntheir work rather than managing a cluster. For a larger organization that can afford a\\ndedicated team for managing its Kubernetes cluster, it may make sense to manage it\\nyourself since it enables greater flexibility in terms of cluster capabilities and\\noperations.\\nAbstracting Your Infrastructure\\nThe goal of the public cloud is to provide easy-to-use, self-service infrastructure for\\ndevelopers to consume. However, too often cloud APIs are oriented around mirror‐\\ning the infrastructure that IT expects, not the concepts (e.g., “virtual machines”\\ninstead of “applications”) that developers want to consume. Additionally, in many\\ncases the cloud comes with particular details in implementation or services that are\\nspecific to the cloud provider. Consuming these APIs directly makes it difficult to run\\nyour application in multiple environments, or spread between cloud and physical\\nenvironments.\\nThe move to application-oriented container APIs like Kubernetes has two concrete\\nbenefits. First, as we described previously, it separates developers from specific\\nmachines. This makes the machine-oriented IT role easier, since machines can simply\\nAbstracting Your Infrastructure | 9\\nbe added in aggregate to scale the cluster, and in the context of the cloud it also ena‐\\nbles a high degree of portability since developers are consuming a higher-level API\\nthat is implemented in terms of the specific cloud infrastructure APIs.\\nWhen your developers build their applications in terms of container images and\\ndeploy them in terms of portable Kubernetes APIs, transferring your application\\nbetween environments, or even running in hybrid environments, is simply a matter of\\nsending the declarative config to a new cluster. Kubernetes has a number of plug-ins\\nthat can abstract you from a particular cloud. For example, Kubernetes services know\\nhow to create load balancers on all major public clouds as well as several different pri‐\\nvate and physical infrastructures. Likewise, Kubernetes PersistentVolumes and\\nPersistentVolumeClaims can be used to abstract your applications away from spe‐\\ncific storage implementations. Of course, to achieve this portability you need to avoid\\ncloud-managed services (e.g., Amazon’s DynamoDB, Azure’s CosmosDB, or Google’s\\nCloud Spanner), which means that you will be forced to deploy and manage open\\nsource storage solutions like Cassandra, MySQL, or MongoDB.\\nPutting it all together, building on top of Kubernetes’s application-oriented abstrac‐\\ntions ensures that the effort you put into building, deploying, and managing your\\napplication is truly portable across a wide variety of environments.\\nEfficiency\\nIn addition to the developer and IT management benefits that containers and Kuber‐\\nnetes provide, there is also a concrete economic benefit to the abstraction. Because\\ndevelopers no longer think in terms of machines, their applications can be colocated\\non the same machines without impacting the applications themselves. This means\\nthat tasks from multiple users can be packed tightly onto fewer machines.\\nEfficiency can be measured by the ratio of the useful work performed by a machine or\\nprocess to the total amount of energy spent doing so. When it comes to deploying\\nand managing applications, many of the available tools and processes (e.g., bash\\nscripts, apt updates, or imperative configuration management) are somewhat ineffi‐\\ncient. When discussing efficiency it’s often helpful to think of both the cost of run‐\\nning a server and the human cost required to manage it.\\nRunning a server incurs a cost based on power usage, cooling requirements, datacenter space, and raw compute power. Once a server is racked and powered on (or\\nclicked and spun up), the meter literally starts running. Any idle CPU time is money\\nwasted. Thus, it becomes part of the system administrator’s responsibilities to keep\\nutilization at acceptable levels, which requires ongoing management. This is where\\ncontainers and the Kubernetes workflow come in. Kubernetes provides tools that\\nautomate the distribution of applications across a cluster of machines, ensuring\\nhigher levels of utilization than are possible with traditional tooling.\\n10 | Chapter 1: Introduction\\nA further increase in efficiency comes from the fact that a developer’s test environ‐\\nment can be quickly and cheaply created as a set of containers running in a personal\\nview of a shared Kubernetes cluster (using a feature called namespaces). In the past,\\nturning up a test cluster for a developer might have meant turning up three machines.\\nWith Kubernetes it is simple to have all developers share a single test cluster, aggre‐\\ngating their usage onto a much smaller set of machines. Reducing the overall number\\nof machines used in turn drives up the efficiency of each system: since more of the\\nresources (CPU, RAM, etc.) on each individual machine are used, the overall cost of\\neach container becomes much lower.\\nReducing the cost of development instances in your stack enables development prac‐\\ntices that might previously have been cost-prohibitive. For example, with your appli‐\\ncation deployed via Kubernetes it becomes conceivable to deploy and test every single\\ncommit contributed by every developer throughout your entire stack.\\nWhen the cost of each deployment is measured in terms of a small number of con‐\\ntainers, rather than multiple complete virtual machines (VMs), the cost you incur for\\nsuch testing is dramatically lower. Returning to the original value of Kubernetes, this\\nincreased testing also increases velocity, since you have strong signals as to the relia‐\\nbility of your code as well as the granularity of detail required to quickly identify\\nwhere a problem may have been introduced.\\nSummary\\nKubernetes was built to radically change the way that applications are built and\\ndeployed in the cloud. Fundamentally, it was designed to give developers more veloc‐\\nity, efficiency, and agility. We hope the preceding sections have given you an idea of\\nwhy you should deploy your applications using Kubernetes. Now that you are con‐\\nvinced of that, the following chapters will teach you how to deploy your application.\\nSummary | 11\\n\\nCHAPTER 2\\nCreating and Running Containers\\nKubernetes is a platform for creating, deploying, and managing distributed applica‐\\ntions. These applications come in many different shapes and sizes, but ultimately,\\nthey are all comprised of one or more programs that run on individual machines.\\nThese programs accept input, manipulate data, and then return the results. Before we\\ncan even consider building a distributed system, we must first consider how to build\\nthe application container images that contain these programs and make up the pieces\\nof our distributed system.\\nApplication programs are typically comprised of a language runtime, libraries, and\\nyour source code. In many cases, your application relies on external shared libraries\\nsuch as libc and libssl. These external libraries are generally shipped as shared\\ncomponents in the OS that you have installed on a particular machine.\\nThis dependency on shared libraries causes problems when an application developed\\non a programmer’s laptop has a dependency on a shared library that isn’t available\\nwhen the program is rolled out to the production OS. Even when the development\\nand production environments share the exact same version of the OS, problems can\\noccur when developers forget to include dependent asset files inside a package that\\nthey deploy to production.\\nThe traditional methods of running multiple programs on a single machine require\\nthat all of these programs share the same versions of shared libraries on the system. If\\nthe different programs are developed by different teams or organizations, these\\nshared dependencies add needless complexity and coupling between these teams.\\nA program can only execute successfully if it can be reliably deployed onto the\\nmachine where it should run. Too often the state of the art for deployment involves\\nrunning imperative scripts, which inevitably have twisty and byzantine failure cases.\\n13\\nThis makes the task of rolling out a new version of all or parts of a distributed system\\na labor-intensive and difficult task.\\nIn Chapter 1, we argued strongly for the value of immutable images and infrastruc‐\\nture. This immutability is exactly what the container image provides. As we will see, it\\neasily solves all the problems of dependency management and encapsulation just\\ndescribed.\\nWhen working with applications it’s often helpful to package them in a way that\\nmakes it easy to share them with others. Docker, the default container runtime\\nengine, makes it easy to package an executable and push it to a remote registry where\\nit can later be pulled by others. At the time of writing, container registries are avail‐\\nable in all of the major public clouds, and services to build images in the cloud are\\nalso available in many of them. You can also run your own registry using open source\\nor commercial systems. These registries make it easy for users to manage and deploy\\nprivate images, while image-builder services provide easy integration with continuous\\ndelivery systems.\\nFor this chapter, and the remainder of the book, we are going to work with a simple\\nexample application that we built to help show this workflow in action. You can find\\nthe application on GitHub.\\nContainer images bundle a program and its dependencies into a single artifact under\\na root filesystem. The most popular container image format is the Docker image for‐\\nmat, which has been standardized by the Open Container Initiative to the OCI image\\nformat. Kubernetes supports both Docker- and OCI-compatible images via Docker\\nand other runtimes. Docker images also include additional metadata used by a con‐\\ntainer runtime to start a running application instance based on the contents of the\\ncontainer image.\\nThis chapter covers the following topics:\\n• How to package an application using the Docker image format\\n• How to start an application using the Docker container runtime\\nContainer Images\\nFor nearly everyone, their first interaction with any container technology is with a\\ncontainer image. A container image is a binary package that encapsulates all of the\\nfiles necessary to run a program inside of an OS container. Depending on how you\\nfirst experiment with containers, you will either build a container image from your\\nlocal filesystem or download a preexisting image from a container registry. In either\\ncase, once the container image is present on your computer, you can run that image\\nto produce a running application inside an OS container.\\n14 | Chapter 2: Creating and Running Containers\\nThe Docker Image Format\\nThe most popular and widespread container image format is the Docker image for‐\\nmat, which was developed by the Docker open source project for packaging, distrib‐\\nuting, and running containers using the docker command. Subsequently, work has\\nbegun by Docker, Inc., and others to standardize the container image format via the\\nOpen Container Initiative (OCI) project. While the OCI standard achieved a 1.0\\nrelease milestone in mid-2017, adoption of these standards is proceeding slowly. The\\nDocker image format continues to be the de facto standard, and is made up of a series\\nof filesystem layers. Each layer adds, removes, or modifies files from the preceding\\nlayer in the filesystem. This is an example of an overlay filesystem. The overlay system\\nis used both when packaging up the image and when the image is actually being used.\\nDuring runtime, there are a variety of different concrete implementations of such file‐\\nsystems, including aufs, overlay, and overlay2.\\nContainer Layering\\nThe phrases “Docker image format” and “container images” may be a bit confusing.\\nThe image isn’t a single file but rather a specification for a manifest file that points to\\nother files. The manifest and associated files are often treated by users as a unit. The\\nlevel of indirection allows for more efficient storage and transmittal. Associated with\\nthis format is an API for uploading and downloading images to an image registry.\\nContainer images are constructed with a series of filesystem layers, where each layer\\ninherits and modifies the layers that came before it. To help explain this in detail, let’s\\nbuild some containers. Note that for correctness the ordering of the layers should be\\nbottom up, but for ease of understanding we take the opposite approach:\\n.\\n└── container A: a base operating system only, such as Debian\\n └── container B: build upon #A, by adding Ruby v2.1.10\\n └── container C: build upon #A, by adding Golang v1.6\\nAt this point we have three containers: A, B, and C. B and C are forked from A and\\nshare nothing besides the base container’s files. Taking it further, we can build on top\\nof B by adding Rails (version 4.2.6). We may also want to support a legacy application\\nthat requires an older version of Rails (e.g., version 3.2.x). We can build a container\\nimage to support that application based on B also, planning to someday migrate the\\napp to version 4:\\n. (continuing from above)\\n└── container B: build upon #A, by adding Ruby v2.1.10\\n └── container D: build upon #B, by adding Rails v4.2.6\\n └── container E: build upon #B, by adding Rails v3.2.x\\nContainer Images | 15\\nConceptually, each container image layer builds upon a previous one. Each parent\\nreference is a pointer. While the example here is a simple set of containers, other realworld containers can be part of a larger extensive directed acyclic graph.\\nContainer images are typically combined with a container configuration file, which\\nprovides instructions on how to set up the container environment and execute an\\napplication entry point. The container configuration often includes information on\\nhow to set up networking, namespace isolation, resource constraints (cgroups), and\\nwhat syscall restrictions should be placed on a running container instance. The\\ncontainer root filesystem and configuration file are typically bundled using the\\nDocker image format.\\nContainers fall into two main categories:\\n• System containers\\n• Application containers\\nSystem containers seek to mimic virtual machines and often run a full boot process.\\nThey often include a set of system services typically found in a VM, such as ssh, cron,\\nand syslog. When Docker was new, these types of containers were much more com‐\\nmon. Over time, they have come to be seen as poor practice and application contain‐\\ners have gained favor.\\nApplication containers differ from system containers in that they commonly run a\\nsingle program. While running a single program per container might seem like an\\nunnecessary constraint, it provides the perfect level of granularity for composing scal‐\\nable applications and is a design philosophy that is leveraged heavily by Pods. We will\\nexamine how Pods work in detail in Chapter 5.\\nBuilding Application Images with Docker\\nIn general, container orchestration systems like Kubernetes are focused on building\\nand deploying distributed systems made up of application containers. Consequently,\\nwe will focus on application containers for the remainder of this chapter.\\nDocker\\x80les\\nA Dockerfile can be used to automate the creation of a Docker container image.\\nLet’s start by building an application image for a simple Node.js program. This exam‐\\nple would be very similar for many other dynamic languages, like Python or Ruby.\\n16 | Chapter 2: Creating and Running Containers\\nThe simplest of npm/Node/Express apps has two files: package.json (Example 2-1)\\nand server.js (Example 2-2). Put these in a directory and then run npm install\\nexpress --save to establish a dependency on Express and install it.\\nExample 2-1. package.json\\n{\\n \"name\": \"simple-node\",\\n \"version\": \"1.0.0\",\\n \"description\": \"A sample simple application for Kubernetes Up & Running\",\\n \"main\": \"server.js\",\\n \"scripts\": {\\n \"start\": \"node server.js\"\\n },\\n \"author\": \"\"\\n}\\nExample 2-2. server.js\\nvar express = require(\\'express\\');\\nvar app = express();\\napp.get(\\'/\\', function (req, res) {\\n res.send(\\'Hello World!\\');\\n});\\napp.listen(3000, function () {\\n console.log(\\'Listening on port 3000!\\');\\n console.log(\\' http://localhost:3000\\');\\n});\\nTo package this up as a Docker image we need to create two additional files: .docker‐\\nignore (Example 2-3) and the Dockerfile (Example 2-4). The Dockerfile is a recipe for\\nhow to build the container image, while .dockerignore defines the set of files that\\nshould be ignored when copying files into the image. A full description of the syntax\\nof the Dockerfile is available on the Docker website.\\nExample 2-3. .dockerignore\\nnode_modules\\nExample 2-4. Dockerfile\\n# Start from a Node.js 10 (LTS) image\\nFROM node:10\\n# Specify the directory inside the image in which all commands will run\\nWORKDIR /usr/src/app\\nBuilding Application Images with Docker | 17\\n# Copy package files and install dependencies\\nCOPY package*.json ./\\nRUN npm install\\n# Copy all of the app files into the image\\nCOPY . .\\n# The default command to run when starting the container\\nCMD [ \"npm\", \"start\" ]\\nEvery Dockerfile builds on other container images. This line specifies that we are\\nstarting from the node:10 image on the Docker Hub. This is a preconfigured\\nimage with Node.js 10.\\nThis line sets the work directory, in the container image, for all following\\ncommands.\\nThese two lines initialize the dependencies for Node.js. First we copy the package\\nfiles into the image. This will include package.json and package-lock.json. The RUN\\ncommand then runs the correct command in the container to install the neces‐\\nsary dependencies.\\nNow we copy the rest of the program files into the image. This will include every‐\\nthing except node_modules, as that is excluded via the .dockerignore file.\\nFinally, we specify the command that should be run when the container is run.\\nRun the following command to create the simple-node Docker image:\\n$ docker build -t simple-node .\\nWhen you want to run this image, you can do it with the following command. You\\ncan navigate to http://localhost:3000 to access the program running in the container:\\n$ docker run --rm -p 3000:3000 simple-node\\nAt this point our simple-node image lives in the local Docker registry where the\\nimage was built and is only accessible to a single machine. The true power of Docker\\ncomes from the ability to share images across thousands of machines and the broader\\nDocker community.\\nOptimizing Image Sizes\\nThere are several gotchas that come when people begin to experiment with container\\nimages that lead to overly large images. The first thing to remember is that files that\\nare removed by subsequent layers in the system are actually still present in the\\nimages; they’re just inaccessible. Consider the following situation:\\n18 | Chapter 2: Creating and Running Containers\\n.\\n└── layer A: contains a large file named \\'BigFile\\'\\n └── layer B: removes \\'BigFile\\'\\n └── layer C: builds on B by adding a static binary\\nYou might think that BigFile is no longer present in this image. After all, when you\\nrun the image, it is no longer accessible. But in fact it is still present in layer A, which\\nmeans that whenever you push or pull the image, BigFile is still transmitted through\\nthe network, even if you can no longer access it.\\nAnother pitfall that people fall into revolves around image caching and building.\\nRemember that each layer is an independent delta from the layer below it. Every time\\nyou change a layer, it changes every layer that comes after it. Changing the preceding\\nlayers means that they need to be rebuilt, repushed, and repulled to deploy your\\nimage to development.\\nTo understand this more fully, consider two images:\\n.\\n└── layer A: contains a base OS\\n └── layer B: adds source code server.js\\n └── layer C: installs the \\'node\\' package\\nversus:\\n.\\n└── layer A: contains a base OS\\n └── layer B: installs the \\'node\\' package\\n └── layer C: adds source code server.js\\nIt seems obvious that both of these images will behave identically, and indeed the first\\ntime they are pulled they do. However, consider what happens when server.js changes.\\nIn one case, it is only the change that needs to be pulled or pushed, but in the other\\ncase, both server.js and the layer providing the node package need to be pulled and\\npushed, since the node layer is dependent on the server.js layer. In general, you want\\nto order your layers from least likely to change to most likely to change in order to\\noptimize the image size for pushing and pulling. This is why, in Example 2-4, we copy\\nthe package*.json files and install dependencies before copying the rest of the pro‐\\ngram files. A developer is going to update and change the program files much more\\noften than the dependencies.\\nImage Security\\nWhen it comes to security, there are no shortcuts. When building images that will\\nultimately run in a production Kubernetes cluster, be sure to follow best practices for\\npackaging and distributing applications. For example, don’t build containers with\\npasswords baked in—and this includes not just in the final layer, but any layers in the\\nimage. One of the counterintuitive problems introduced by container layers is that\\ndeleting a file in one layer doesn’t delete that file from preceding layers. It still takes\\nBuilding Application Images with Docker | 19\\nup space, and it can be accessed by anyone with the right tools—an enterprising\\nattacker can simply create an image that only consists of the layers that contain the\\npassword.\\nSecrets and images should never be mixed. If you do so, you will be hacked, and you\\nwill bring shame to your entire company or department. We all want to be on TV\\nsomeday, but there are better ways to go about that.\\nMultistage Image Builds\\nOne of the most common ways to accidentally build large images is to do the actual\\nprogram compilation as part of the construction of the application container image.\\nCompiling code as part of the image build feels natural, and it is the easiest way to\\nbuild a container image from your program. The trouble with doing this is that it\\nleaves all of the unnecessary development tools, which are usually quite large, lying\\naround inside of your image and slowing down your deployments.\\nTo resolve this problem, Docker introduced multistage builds. With multistage builds,\\nrather than producing a single image, a Docker file can actually produce multiple\\nimages. Each image is considered a stage. Artifacts can be copied from preceding\\nstages to the current stage.\\nTo illustrate this concretely, we will look at how to build our example application,\\nkuard. This is a somewhat complicated application that involves a React.js frontend\\n(with its own build process) that then gets embedded into a Go program. The Go\\nprogram runs a backend API server that the React.js frontend interacts with.\\nA simple Dockerfile might look like this:\\nFROM golang:1.11-alpine\\n# Install Node and NPM\\nRUN apk update && apk upgrade && apk add --no-cache git nodejs bash npm\\n# Get dependencies for Go part of build\\nRUN go get -u github.com/jteeuwen/go-bindata/...\\nRUN go get github.com/tools/godep\\nWORKDIR /go/src/github.com/kubernetes-up-and-running/kuard\\n# Copy all sources in\\nCOPY . .\\n# This is a set of variables that the build script expects\\nENV VERBOSE=0\\nENV PKG=github.com/kubernetes-up-and-running/kuard\\nENV ARCH=amd64\\nENV VERSION=test\\n20 | Chapter 2: Creating and Running Containers\\n# Do the build. This script is part of incoming sources.\\nRUN build/build.sh\\nCMD [ \"/go/bin/kuard\" ]\\nThis Dockerfile produces a container image containing a static executable, but it also\\ncontains all of the Go development tools and the tools to build the React.js frontend\\nand the source code for the application, neither of which are needed by the final\\napplication. The image, across all layers, adds up to over 500 MB.\\nTo see how we would do this with multistage builds, examine this multistage Docker‐\\nfile:\\n# STAGE 1: Build\\nFROM golang:1.11-alpine AS build\\n# Install Node and NPM\\nRUN apk update && apk upgrade && apk add --no-cache git nodejs bash npm\\n# Get dependencies for Go part of build\\nRUN go get -u github.com/jteeuwen/go-bindata/...\\nRUN go get github.com/tools/godep\\nWORKDIR /go/src/github.com/kubernetes-up-and-running/kuard\\n# Copy all sources in\\nCOPY . .\\n# This is a set of variables that the build script expects\\nENV VERBOSE=0\\nENV PKG=github.com/kubernetes-up-and-running/kuard\\nENV ARCH=amd64\\nENV VERSION=test\\n# Do the build. Script is part of incoming sources.\\nRUN build/build.sh\\n# STAGE 2: Deployment\\nFROM alpine\\nUSER nobody:nobody\\nCOPY --from=build /go/bin/kuard /kuard\\nCMD [ \"/kuard\" ]\\nThis Dockerfile produces two images. The first is the build image, which contains the\\nGo compiler, React.js toolchain, and source code for the program. The second is the\\ndeployment image, which simply contains the compiled binary. Building a container\\nimage using multistage builds can reduce your final container image size by hundreds\\nof megabytes and thus dramatically speed up your deployment times, since generally,\\nMultistage Image Builds | 21\\ndeployment latency is gated on network performance. The final image produced\\nfrom this Dockerfile is somewhere around 20 MB.\\nYou can build and run this image with the following commands:\\n$ docker build -t kuard .\\n$ docker run --rm -p 8080:8080 kuard\\nStoring Images in a Remote Registry\\nWhat good is a container image if it’s only available on a single machine?\\nKubernetes relies on the fact that images described in a Pod manifest are available\\nacross every machine in the cluster. One option for getting this image to all machines\\nin the cluster would be to export the kuard image and import it on each of them. We\\ncan’t think of anything more tedious than managing Docker images this way. The\\nprocess of manually importing and exporting Docker images has human error writ‐\\nten all over it. Just say no!\\nThe standard within the Docker community is to store Docker images in a remote\\nregistry. There are tons of options when it comes to Docker registries, and what you\\nchoose will be largely based on your needs in terms of security and collaboration\\nfeatures.\\nGenerally speaking, the first choice you need to make regarding a registry is whether\\nto use a private or a public registry. Public registries allow anyone to download\\nimages stored in the registry, while private registries require authentication to down‐\\nload images. In choosing public versus private, it’s helpful to consider your use case.\\nPublic registries are great for sharing images with the world, because they allow for\\neasy, unauthenticated use of the container images. You can easily distribute your soft‐\\nware as a container image and have confidence that users everywhere will have the\\nexact same experience.\\nIn contrast, a private registry is best for storing applications that are private to your\\nservice and that you don’t want the world to use.\\nRegardless, to push an image, you need to authenticate to the registry. You can gener‐\\nally do this with the docker login command, though there are some differences for\\ncertain registries. In the examples here we are pushing to the Google Cloud Platform\\nregistry, called the Google Container Registry (GCR); other clouds, including Azure\\nand Amazon Web Services (AWS), also have hosted container registries. For new\\nusers hosting publicly readable images, the Docker Hub is a great place to start.\\nOnce you are logged in, you can tag the kuard image by prepending the target\\nDocker registry. You can also append another identifier that is usually used for the\\nversion or variant of that image, separated by a colon (:):\\n22 | Chapter 2: Creating and Running Containers\\n$ docker tag kuard gcr.io/kuar-demo/kuard-amd64:blue\\nThen you can push the kuard image:\\n$ docker push gcr.io/kuar-demo/kuard-amd64:blue\\nNow that the kuard image is available on a remote registry, it’s time to deploy it using\\nDocker. Because we pushed it to the public Docker registry, it will be available every‐\\nwhere without authentication.\\nThe Docker Container Runtime\\nKubernetes provides an API for describing an application deployment, but relies on a\\ncontainer runtime to set up an application container using the container-specific\\nAPIs native to the target OS. On a Linux system that means configuring cgroups and\\nnamespaces. The interface to this container runtime is defined by the Container Run‐\\ntime Interface (CRI) standard. The CRI API is implemented by a number of different\\nprograms, including the containerd-cri built by Docker and the cri-o implementa‐\\ntion contributed by Red Hat.\\nRunning Containers with Docker\\nThough generally in Kubernetes containers are launched by a daemon on each node\\ncalled the kubelet, it’s easier to get started with containers using the Docker\\ncommand-line tool. The Docker CLI tool can be used to deploy containers. To deploy\\na container from the gcr.io/kuar-demo/kuard-amd64:blue image, run the following\\ncommand:\\n$ docker run -d --name kuard \\\\\\n --publish 8080:8080 \\\\\\n gcr.io/kuar-demo/kuard-amd64:blue\\nThis command starts the kuard container and maps ports 8080 on your local\\nmachine to 8080 in the container. The --publish option can be shortened to -p. This\\nforwarding is necessary because each container gets its own IP address, so listening\\non localhost inside the container doesn’t cause you to listen on your machine.\\nWithout the port forwarding, connections will be inaccessible to your machine. The\\n-d option specifies that this should run in the background (daemon), while --name\\nkuard gives the container a friendly name.\\nExploring the kuard Application\\nkuard exposes a simple web interface, which you can load by pointing your browser\\nat http://localhost:8080 or via the command line:\\n$ curl http://localhost:8080\\nThe Docker Container Runtime | 23\\nkuard also exposes a number of interesting functions that we will explore later on in\\nthis book.\\nLimiting Resource Usage\\nDocker provides the ability to limit the amount of resources used by applications by\\nexposing the underlying cgroup technology provided by the Linux kernel. These\\ncapabilities are likewise used by Kubernetes to limit the resources used by each Pod.\\nLimiting memory resources\\nOne of the key benefits to running applications within a container is the ability to\\nrestrict resource utilization. This allows multiple applications to coexist on the same\\nhardware and ensures fair usage.\\nTo limit kuard to 200 MB of memory and 1 GB of swap space, use the --memory and\\n--memory-swap flags with the docker run command.\\nStop and remove the current kuard container:\\n$ docker stop kuard\\n$ docker rm kuard\\nThen start another kuard container using the appropriate flags to limit memory\\nusage:\\n$ docker run -d --name kuard \\\\\\n --publish 8080:8080 \\\\\\n --memory 200m \\\\\\n --memory-swap 1G \\\\\\n gcr.io/kuar-demo/kuard-amd64:blue\\nIf the program in the container uses too much memory, it will be terminated.\\nLimiting CPU resources\\nAnother critical resource on a machine is the CPU. Restrict CPU utilization using the\\n--cpu-shares flag with the docker run command:\\n$ docker run -d --name kuard \\\\\\n --publish 8080:8080 \\\\\\n --memory 200m \\\\\\n --memory-swap 1G \\\\\\n --cpu-shares 1024 \\\\\\n gcr.io/kuar-demo/kuard-amd64:blue\\nCleanup\\nOnce you are done building an image, you can delete it with the docker rmi\\ncommand:\\n24 | Chapter 2: Creating and Running Containers\\ndocker rmi <tag-name>\\nor:\\ndocker rmi <image-id>\\nImages can either be deleted via their tag name (e.g., gcr.io/kuar-demo/kuardamd64:blue) or via their image ID. As with all ID values in the docker tool, the image\\nID can be shortened as long as it remains unique. Generally only three or four char‐\\nacters of the ID are necessary.\\nIt’s important to note that unless you explicitly delete an image it will live on your\\nsystem forever, even if you build a new image with an identical name. Building this\\nnew image simply moves the tag to the new image; it doesn’t delete or replace the old\\nimage.\\nConsequently, as you iterate while you are creating a new image, you will often create\\nmany, many different images that end up taking up unnecessary space on your\\ncomputer.\\nTo see the images currently on your machine, you can use the docker images com‐\\nmand. You can then delete tags you are no longer using.\\nDocker provides a tool called docker system prune for doing general cleanup. This\\nwill remove all stopped containers, all untagged images, and all unused image layers\\ncached as part of the build process. Use it carefully.\\nA slightly more sophisticated approach is to set up a cron job to run an image\\ngarbage collector. For example, the docker-gc tool is a commonly used image\\ngarbage collector that can easily run as a recurring cron job, once per day or once per\\nhour, depending on how many images you are creating.\\nSummary\\nApplication containers provide a clean abstraction for applications, and when pack‐\\naged in the Docker image format, applications become easy to build, deploy, and dis‐\\ntribute. Containers also provide isolation between applications running on the same\\nmachine, which helps avoid dependency conflicts.\\nIn future chapters we’ll see how the ability to mount external directories means we\\ncan run not only stateless applications in a container, but also applications like mysql\\nand others that generate lots of data.\\nSummary | 25\\n\\nCHAPTER 3\\nDeploying a Kubernetes Cluster\\nNow that you have successfully built an application container, the next step is to learn\\nhow to transform it into a complete, reliable, scalable distributed system. To do that,\\nyou need a working Kubernetes cluster. At this point, there are cloud-based Kuber‐\\nnetes services in most public clouds that make it easy to create a cluster with a few\\ncommand-line instructions. We highly recommend this approach if you are just get‐\\nting started with Kubernetes. Even if you are ultimately planning on running Kuber‐\\nnetes on bare metal, it’s a good way to quickly get started with Kubernetes, learn\\nabout Kubernetes itself, and then learn how to install it on physical machines. Fur‐\\nthermore, managing a Kubernetes cluster is a complicated task in itself, and for most\\npeople it makes sense to defer this management to the cloud—especially when in\\nmost clouds the management service is free.\\nOf course, using a cloud-based solution requires paying for those cloud-based resour‐\\nces as well as having an active network connection to the cloud. For these reasons,\\nlocal development can be more attractive, and in that case the minikube tool provides\\nan easy-to-use way to get a local Kubernetes cluster up running in a VM on your local\\nlaptop or desktop. Though this is a nice option, minikube only creates a single-node\\ncluster, which doesn’t quite demonstrate all of the aspects of a complete Kubernetes\\ncluster. For that reason, we recommend people start with a cloud-based solution,\\nunless it really doesn’t work for their situation. A more recent alternative is to run a\\nDocker-in-Docker cluster, which can spin up a multi-node cluster on a single\\nmachine. This project is still in beta, though, so keep in mind that you may encounter\\nunexpected issues.\\nIf you truly insist on starting on bare metal, Appendix A at the end of this book gives\\ninstructions for building a cluster from a collection of Raspberry Pi single-board\\ncomputers. These instructions use the kubeadm tool and can be adapted to other\\nmachines beyond Raspberry Pis.\\n27\\nInstalling Kubernetes on a Public Cloud Provider\\nThis chapter covers installing Kubernetes on the three major cloud providers: Ama‐\\nzon Web Services, Microsoft Azure, and the Google Cloud Platform.\\nIf you choose to use a cloud provider to manage Kubernetes, you only need to install\\none of these options; once you have a cluster configured and ready to go you can skip\\nto “The Kubernetes Client” on page 31, unless you would prefer to install Kubernetes\\nelsewhere.\\nGoogle Kubernetes Engine\\nThe Google Cloud Platform offers a hosted Kubernetes-as-a-Service called Google\\nKubernetes Engine (GKE). To get started with GKE, you need a Google Cloud Plat‐\\nform account with billing enabled and the gcloud tool installed.\\nOnce you have gcloud installed, first set a default zone:\\n$ gcloud config set compute/zone us-west1-a\\nThen you can create a cluster:\\n$ gcloud container clusters create kuar-cluster\\nThis will take a few minutes. When the cluster is ready you can get credentials for the\\ncluster using:\\n$ gcloud auth application-default login\\nIf you run into trouble, you can find the complete instructions for creating a GKE\\ncluster in the Google Cloud Platform documentation.\\nInstalling Kubernetes with Azure Kubernetes Service\\nMicrosoft Azure offers a hosted Kubernetes-as-a-Service as part of the Azure Con‐\\ntainer Service. The easiest way to get started with Azure Container Service is to use\\nthe built-in Azure Cloud Shell in the Azure portal. You can activate the shell by click‐\\ning the shell icon in the upper-right toolbar:\\nThe shell has the az tool automatically installed and configured to work with your\\nAzure environment.\\nAlternatively, you can install the az command-line interface (CLI) on your local\\nmachine.\\nWhen you have the shell up and working, you can run:\\n28 | Chapter 3: Deploying a Kubernetes Cluster\\n$ az group create --name=kuar --location=westus\\nOnce the resource group is created, you can create a cluster using:\\n$ az aks create --resource-group=kuar --name=kuar-cluster\\nThis will take a few minutes. Once the cluster is created, you can get credentials for\\nthe cluster with:\\n$ az aks get-credentials --resource-group=kuar --name=kuar-cluster\\nIf you don’t already have the kubectl tool installed, you can install it using:\\n$ az aks install-cli\\nYou can find complete instructions for installing Kubernetes on Azure in the Azure\\ndocumentation.\\nInstalling Kubernetes on Amazon Web Services\\nAmazon offers a managed Kubernetes service called Elastic Kubernetes Service\\n(EKS). The easiest way to create an EKS cluster is via the open source eksctl\\ncommand-line tool..\\nOnce you have eksctl installed and in your path, you can run the following com‐\\nmand to create a cluster:\\n$ eksctl create cluster --name kuar-cluster ...\\nFor more details on installation options (such as node size and more), view the help\\nusing this command:\\n$ eksctl create cluster --help\\nThe cluster installation includes the right configuration for the kubectl commandline tool. If you don’t already have kubectl installed, you can follow the instructions\\nin the documentation.\\nInstalling Kubernetes Locally Using minikube\\nIf you need a local development experience, or you don’t want to pay for cloud\\nresources, you can install a simple single-node cluster using minikube.\\nAlternatively, if you have already installed Docker Desktop, it comes bundled with a\\nsingle-machine installation of Kubernetes.\\nWhile minikube (or Docker Desktop) is a good simulation of a Kubernetes cluster, it’s\\nreally intended for local development, learning, and experimentation. Because it only\\nruns in a VM on a single node, it doesn’t provide the reliability of a distributed\\nKubernetes cluster.\\nInstalling Kubernetes Locally Using minikube | 29\\nIn addition, certain features described in this book require integration with a cloud\\nprovider. These features are either not available or work in a limited way with\\nminikube.\\nYou need to have a hypervisor installed on your machine to use\\nminikube. For Linux and macOS, this is generally virtualbox. On\\nWindows, the Hyper-V hypervisor is the default option. Make sure\\nyou install the hypervisor before using minikube.\\nYou can find the minikube tool on GitHub. There are binaries for Linux, macOS, and\\nWindows that you can download. Once you have the minikube tool installed, you can\\ncreate a local cluster using:\\n$ minikube start\\nThis will create a local VM, provision Kubernetes, and create a local kubectl configu‐\\nration that points to that cluster.\\nWhen you are done with your cluster, you can stop the VM with:\\n$ minikube stop\\nIf you want to remove the cluster, you can run:\\n$ minikube delete\\nRunning Kubernetes in Docker\\nA different approach to running a Kubernetes cluster has been developed more\\nrecently, which uses Docker containers to simulate multiple Kubernetes nodes instead\\nof running everything in a virtual machine. The kind project provides a great experi‐\\nence for launching and managing test clusters in Docker. (kind stands for Kubernetes\\nIN Docker.) kind is still a work in progress (pre 1.0), but is widely used by those\\nbuilding Kubernetes for fast and easy testing.\\nInstallation instructions for your platform can be found at the kind site. Once you get\\nit installed, creating a cluster is as easy as:\\n$ kind create cluster --wait 5m \\\\\\n$ export KUBECONFIG=\"$(kind get kubeconfig-path)\"\\n$ kubectl cluster-info\\n$ kind delete cluster\\n30 | Chapter 3: Deploying a Kubernetes Cluster\\nRunning Kubernetes on Raspberry Pi\\nIf you want to experiment with a realistic Kubernetes cluster but don’t want to pay a\\nlot, a very nice Kubernetes cluster can be built on top of Raspberry Pi computers for a\\nrelatively small cost. The details of building such a cluster are out of scope for this\\nchapter, but they are given in Appendix A at the end of this book.\\nThe Kubernetes Client\\nThe official Kubernetes client is kubectl: a command-line tool for interacting with\\nthe Kubernetes API. kubectl can be used to manage most Kubernetes objects, such as\\nPods, ReplicaSets, and Services. kubectl can also be used to explore and verify the\\noverall health of the cluster.\\nWe’ll use the kubectl tool to explore the cluster you just created.\\nChecking Cluster Status\\nThe first thing you can do is check the version of the cluster that you are running:\\n$ kubectl version\\nThis will display two different versions: the version of the local kubectl tool, as well\\nas the version of the Kubernetes API server.\\nDon’t worry if these versions are different. The Kubernetes tools\\nare backward- and forward-compatible with different versions of\\nthe Kubernetes API, so long as you stay within two minor versions\\nfor both the tools and the cluster and don’t try to use newer fea‐\\ntures on an older cluster. Kubernetes follows the semantic version‐\\ning specification, where the minor version is the middle number\\n(e.g., the 5 in 1.5.2).\\nNow that we’ve established that you can communicate with your Kubernetes cluster,\\nwe’ll explore the cluster in more depth.\\nFirst, you can get a simple diagnostic for the cluster. This is a good way to verify that\\nyour cluster is generally healthy:\\n$ kubectl get componentstatuses\\nThe output should look like this:\\nNAME STATUS MESSAGE ERROR\\nscheduler Healthy ok\\ncontroller-manager Healthy ok\\netcd-0 Healthy {\"health\": \"true\"}\\nRunning Kubernetes on Raspberry Pi | 31\\nAs Kubernetes changes and improves over time, the output of the\\nkubectl command sometimes changes. Don’t worry if the output\\ndoesn’t look exactly identical to what is shown in the examples in\\nthis book.\\nYou can see here the components that make up the Kubernetes cluster. The\\ncontroller-manager is responsible for running various controllers that regulate\\nbehavior in the cluster; for example, ensuring that all of the replicas of a service are\\navailable and healthy. The scheduler is responsible for placing different Pods onto\\ndifferent nodes in the cluster. Finally, the etcd server is the storage for the cluster\\nwhere all of the API objects are stored.\\nListing Kubernetes Worker Nodes\\nNext, you can list out all of the nodes in your cluster:\\n$ kubectl get nodes\\nNAME STATUS AGE VERSION\\nkubernetes Ready,master 45d v1.12.1\\nnode-1 Ready 45d v1.12.1\\nnode-2 Ready 45d v1.12.1\\nnode-3 Ready 45d v1.12.1\\nYou can see this is a four-node cluster that’s been up for 45 days. In Kubernetes, nodes\\nare separated into master nodes that contain containers like the API server, schedu‐\\nler, etc., which manage the cluster, and worker nodes where your containers will run.\\nKubernetes won’t generally schedule work onto master nodes to ensure that user\\nworkloads don’t harm the overall operation of the cluster.\\nYou can use the kubectl describe command to get more information about a spe‐\\ncific node, such as node-1:\\n$ kubectl describe nodes node-1\\nFirst, you see basic information about the node:\\nName: node-1\\nRole:\\nLabels: beta.kubernetes.io/arch=arm\\n beta.kubernetes.io/os=linux\\n kubernetes.io/hostname=node-1\\nYou can see that this node is running the Linux OS and is running on an ARM pro‐\\ncessor.\\nNext, you see information about the operation of node-1 itself:\\n32 | Chapter 3: Deploying a Kubernetes Cluster\\nConditions:\\n Type Status LastHeartbeatTime Reason Message\\n ---- ------ ----------------- ------ -------\\n OutOfDisk False Sun, 05 Feb 2017… KubeletHasSufficientDisk kubelet…\\n MemoryPressure False Sun, 05 Feb 2017… KubeletHasSufficientMemory kubelet…\\n DiskPressure False Sun, 05 Feb 2017… KubeletHasNoDiskPressure kubelet…\\n Ready True Sun, 05 Feb 2017… KubeletReady kubelet…\\nThese statuses show that the node has sufficient disk and memory space and is\\nreporting that it is healthy to the Kubernetes master. Next, there is information about\\nthe capacity of the machine:\\nCapacity:\\n alpha.kubernetes.io/nvidia-gpu: 0\\n cpu: 4\\n memory: 882636Ki\\n pods: 110\\nAllocatable:\\n alpha.kubernetes.io/nvidia-gpu: 0\\n cpu: 4\\n memory: 882636Ki\\n pods: 110\\nThen there is information about the software on the node, including the version of\\nDocker that is running, the versions of Kubernetes and the Linux kernel, and more:\\nSystem Info:\\n Machine ID: 9122895d0d494e3f97dda1e8f969c85c\\n System UUID: A7DBF2CE-DB1E-E34A-969A-3355C36A2149\\n Boot ID: ba53d5ee-27d2-4b6a-8f19-e5f702993ec6\\n Kernel Version: 4.15.0-1037-azure\\n OS Image: Ubuntu 16.04.5 LTS\\n Operating System: linux\\n Architecture: amd64\\n Container Runtime Version: docker://3.0.4\\n Kubelet Version: v1.12.6\\n Kube-Proxy Version: v1.12.6\\nPodCIDR: 10.244.1.0/24\\nFinally, there is information about the Pods that are currently running on this node:\\nNon-terminated Pods: (3 in total)\\n Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits\\n --------- ---- ------------ ---------- --------------- -------------\\n kube-system kube-dns... 260m (6%) 0 (0%) 140Mi (16%) 220Mi (25%)\\n kube-system kube-fla... 0 (0%) 0 (0%) 0 (0%) 0 (0%)\\n kube-system kube-pro... 0 (0%) 0 (0%) 0 (0%) 0 (0%)\\nAllocated resources:\\n (Total limits may be over 100 percent, i.e., overcommitted.\\n CPU Requests CPU Limits Memory Requests Memory Limits\\n ------------ ---------- --------------- -------------\\n 260m (6%) 0 (0%) 140Mi (16%) 220Mi (25%)\\nNo events.\\nThe Kubernetes Client | 33\\n1 As you’ll learn in the next chapter, a namespace in Kubernetes is an entity for organizing Kubernetes resour‐\\nces. You can think of it like a folder in a filesystem.\\nFrom this output you can see the Pods on the node (e.g., the kube-dns Pod that sup‐\\nplies DNS services for the cluster), the CPU and memory that each Pod is requesting\\nfrom the node, as well as the total resources requested. It’s worth noting here that\\nKubernetes tracks both the requests and upper limits for resources for each Pod that\\nruns on a machine. The difference between requests and limits is described in detail\\nin Chapter 5, but in a nutshell, resources requested by a Pod are guaranteed to be\\npresent on the node, while a Pod’s limit is the maximum amount of a given resource\\nthat a Pod can consume. A Pod’s limit can be higher than its request, in which case\\nthe extra resources are supplied on a best-effort basis. They are not guaranteed to be\\npresent on the node.\\nCluster Components\\nOne of the interesting aspects of Kubernetes is that many of the components that\\nmake up the Kubernetes cluster are actually deployed using Kubernetes itself. We’ll\\ntake a look at a few of these. These components use a number of the concepts that\\nwe’ll introduce in later chapters. All of these components run in the kube-system\\nnamespace.1\\nKubernetes Proxy\\nThe Kubernetes proxy is responsible for routing network traffic to load-balanced\\nservices in the Kubernetes cluster. To do its job, the proxy must be present on every\\nnode in the cluster. Kubernetes has an API object named DaemonSet, which you will\\nlearn about later in the book, that is used in many clusters to accomplish this. If your\\ncluster runs the Kubernetes proxy with a DaemonSet, you can see the proxies by\\nrunning:\\n$ kubectl get daemonSets --namespace=kube-system kube-proxy\\nNAME DESIRED CURRENT READY NODE-SELECTOR AGE\\nkube-proxy 4 4 4 <none> 45d\\nDepending on how your cluster is set up, the DaemonSet for the kube-proxy may be\\nnamed something else, or its possible that it won’t use a DaemonSet at all. Regardless,\\nthe kube-proxy container should be running on all nodes in a cluster.\\nKubernetes DNS\\nKubernetes also runs a DNS server, which provides naming and discovery for the\\nservices that are defined in the cluster. This DNS server also runs as a replicated ser‐\\nvice on the cluster. Depending on the size of your cluster, you may see one or more\\n34 | Chapter 3: Deploying a Kubernetes Cluster\\nDNS servers running in your cluster. The DNS service is run as a Kubernetes deploy‐\\nment, which manages these replicas:\\n$ kubectl get deployments --namespace=kube-system core-dns\\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE\\ncore-dns 1 1 1 1 45d\\nThere is also a Kubernetes service that performs load balancing for the DNS server:\\n$ kubectl get services --namespace=kube-system core-dns\\nNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE\\ncore-dns 10.96.0.10 <none> 53/UDP,53/TCP 45d\\nThis shows that the DNS service for the cluster has the address 10.96.0.10. If you log\\nin to a container in the cluster, you’ll see that this has been populated into the /etc/\\nresolv.conf file for the container.\\nWith Kubernetes 1.12, Kubernetes transitioned from the kube-dns\\nDNS server to the core-dns DNS server. Because of this, if you are\\nrunning an older Kubernetes cluster, you may see kube-dns\\ninstead.\\nKubernetes UI\\nThe final Kubernetes component is a GUI. The UI is run as a single replica, but it is\\nstill managed by a Kubernetes deployment for reliability and upgrades. You can see\\nthis UI server using:\\n$ kubectl get deployments --namespace=kube-system kubernetes-dashboard\\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE\\nkubernetes-dashboard 1 1 1 1 45d\\nThe dashboard also has a service that performs load balancing for the dashboard:\\n$ kubectl get services --namespace=kube-system kubernetes-dashboard\\nNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE\\nkubernetes-dashboard 10.99.104.174 <nodes> 80:32551/TCP 45d\\nYou can use kubectl proxy to access this UI. Launch the Kubernetes proxy using:\\n$ kubectl proxy\\nThis starts up a server running on localhost:8001. If you visit http://localhost:\\n8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/\\nproxy/ in your web browser, you should see the Kubernetes web UI. You can use this\\ninterface to explore your cluster, as well as create new containers. The full details of\\nthis interface are outside of the scope of this book, and it is changing rapidly as the\\ndashboard is improved.\\nCluster Components | 35\\nSome providers don’t install the Kubernetes dashboard by default, so don’t worry if\\nyou don’t see it in your cluster. Documentation on how to install the dashboard for\\nthese clusters is available at https://kubernetes.io/docs/tasks/access-application-cluster/\\nweb-ui-dashboard/.\\nSummary\\nHopefully at this point you have a Kubernetes cluster (or three) up and running and\\nyou’ve used a few commands to explore the cluster you have created. Next, we’ll\\nspend some more time exploring the command-line interface to that Kubernetes\\ncluster and teach you how to master the kubectl tool. Throughout the rest of the\\nbook, you’ll be using kubectl and your test cluster to explore the various objects in\\nthe Kubernetes API.\\n36 | Chapter 3: Deploying a Kubernetes Cluster\\nCHAPTER 4\\nCommon kubectl Commands\\nThe kubectl command-line utility is a powerful tool, and in the following chapters\\nyou will use it to create objects and interact with the Kubernetes API. Before that,\\nhowever, it makes sense to go over the basic kubectl commands that apply to all\\nKubernetes objects.\\nNamespaces\\nKubernetes uses namespaces to organize objects in the cluster. You can think of each\\nnamespace as a folder that holds a set of objects. By default, the kubectl commandline tool interacts with the default namespace. If you want to use a different name‐\\nspace, you can pass kubectl the --namespace flag. For example,\\nkubectl --namespace=mystuff references objects in the mystuff namespace. If you\\nwant to interact with all namespaces—for example, to list all Pods in your cluster—\\nyou can pass the --all-namespaces flag.\\nContexts\\nIf you want to change the default namespace more permanently, you can use a con‐\\ntext. This gets recorded in a kubectl configuration file, usually located at\\n$HOME/.kube/config. This configuration file also stores how to both find and authen‐\\nticate to your cluster. For example, you can create a context with a different default\\nnamespace for your kubectl commands using:\\n$ kubectl config set-context my-context --namespace=mystuff\\nThis creates a new context, but it doesn’t actually start using it yet. To use this newly\\ncreated context, you can run:\\n$ kubectl config use-context my-context\\n37\\nContexts can also be used to manage different clusters or different users for authenti‐\\ncating to those clusters using the --users or --clusters flags with the set-context\\ncommand.\\nViewing Kubernetes API Objects\\nEverything contained in Kubernetes is represented by a RESTful resource. Through‐\\nout this book, we refer to these resources as Kubernetes objects. Each Kubernetes\\nobject exists at a unique HTTP path; for example, https://your-k8s.com/api/v1/name‐\\nspaces/default/pods/my-pod leads to the representation of a Pod in the default name‐\\nspace named my-pod. The kubectl command makes HTTP requests to these URLs to\\naccess the Kubernetes objects that reside at these paths.\\nThe most basic command for viewing Kubernetes objects via kubectl is get. If you\\nrun kubectl get <resource-name> you will get a listing of all resources in the cur‐\\nrent namespace. If you want to get a specific resource, you can use kubectl get\\n<resource-name> <obj-name>.\\nBy default, kubectl uses a human-readable printer for viewing the responses from\\nthe API server, but this human-readable printer removes many of the details of the\\nobjects to fit each object on one terminal line. One way to get slightly more informa‐\\ntion is to add the -o wide flag, which gives more details, on a longer line. If you want\\nto view the complete object, you can also view the objects as raw JSON or YAML\\nusing the -o json or -o yaml flags, respectively.\\nA common option for manipulating the output of kubectl is to remove the headers,\\nwhich is often useful when combining kubectl with Unix pipes (e.g., kubectl ... |\\nawk ...). If you specify the --no-headers flag, kubectl will skip the headers at the\\ntop of the human-readable table.\\nAnother common task is extracting specific fields from the object. kubectl uses the\\nJSONPath query language to select fields in the returned object. The complete details\\nof JSONPath are beyond the scope of this chapter, but as an example, this command\\nwill extract and print the IP address of the specified Pod:\\n$ kubectl get pods my-pod -o jsonpath --template={.status.podIP}\\nIf you are interested in more detailed information about a particular object, use the\\ndescribe command:\\n$ kubectl describe <resource-name> <obj-name>\\nThis will provide a rich multiline human-readable description of the object as well as\\nany other relevant, related objects and events in the Kubernetes cluster.\\n38 | Chapter 4: Common kubectl Commands\\nCreating, Updating, and Destroying Kubernetes Objects\\nObjects in the Kubernetes API are represented as JSON or YAML files. These files are\\neither returned by the server in response to a query or posted to the server as part of\\nan API request. You can use these YAML or JSON files to create, update, or delete\\nobjects on the Kubernetes server.\\nLet’s assume that you have a simple object stored in obj.yaml. You can use kubectl to\\ncreate this object in Kubernetes by running:\\n$ kubectl apply -f obj.yaml\\nNotice that you don’t need to specify the resource type of the object; it’s obtained\\nfrom the object file itself.\\nSimilarly, after you make changes to the object, you can use the apply command\\nagain to update the object:\\n$ kubectl apply -f obj.yaml\\nThe apply tool will only modify objects that are different from the current objects in\\nthe cluster. If the objects you are creating already exist in the cluster, it will simply exit\\nsuccessfully without making any changes. This makes it useful for loops where you\\nwant to ensure the state of the cluster matches the state of the filesystem. You can\\nrepeatedly use apply to reconcile state.\\nIf you want to see what the apply command will do without actually making the\\nchanges, you can use the --dry-run flag to print the objects to the terminal without\\nactually sending them to the server.\\nIf you feel like making interactive edits instead of editing a local\\nfile, you can instead use the edit command, which will download\\nthe latest object state and then launch an editor that contains the\\ndefinition:\\n$ kubectl edit <resource-name> <obj-name>\\nAfter you save the file, it will be automatically uploaded back to the\\nKubernetes cluster.\\nThe apply command also records the history of previous configurations in an anno‐\\ntation within the object. You can manipulate these records with the edit-lastapplied, set-last-applied, and view-last-applied commands. For example:\\n$ kubectl apply -f myobj.yaml view-last-applied\\nwill show you the last state that was applied to the object.\\nCreating, Updating, and Destroying Kubernetes Objects | 39\\nWhen you want to delete an object, you can simply run:\\n$ kubectl delete -f obj.yaml\\nIt is important to note that kubectl will not prompt you to confirm the deletion.\\nOnce you issue the command, the object will be deleted.\\nLikewise, you can delete an object using the resource type and name:\\n$ kubectl delete <resource-name> <obj-name>\\nLabeling and Annotating Objects\\nLabels and annotations are tags for your objects. We’ll discuss the differences in\\nChapter 6, but for now, you can update the labels and annotations on any Kubernetes\\nobject using the annotate and label commands. For example, to add the color=red\\nlabel to a Pod named bar, you can run:\\n$ kubectl label pods bar color=red\\nThe syntax for annotations is identical.\\nBy default, label and annotate will not let you overwrite an existing label. To do this,\\nyou need to add the --overwrite flag.\\nIf you want to remove a label, you can use the <label-name>- syntax:\\n$ kubectl label pods bar colorThis will remove the color label from the Pod named bar.\\nDebugging Commands\\nkubectl also makes a number of commands available for debugging your containers.\\nYou can use the following to see the logs for a running container:\\n$ kubectl logs <pod-name>\\nIf you have multiple containers in your Pod, you can choose the container to view\\nusing the -c flag.\\nBy default, kubectl logs lists the current logs and exits. If you instead want to con‐\\ntinuously stream the logs back to the terminal without exiting, you can add the -f\\n(follow) command-line flag.\\nYou can also use the exec command to execute a command in a running container:\\n$ kubectl exec -it <pod-name> -- bash\\nThis will provide you with an interactive shell inside the running container so that\\nyou can perform more debugging.\\n40 | Chapter 4: Common kubectl Commands\\nIf you don’t have bash or some other terminal available within your container, you\\ncan always attach to the running process:\\n$ kubectl attach -it <pod-name>\\nThis will attach to the running process. It is similar to kubectl logs but will allow\\nyou to send input to the running process, assuming that process is set up to read from\\nstandard input.\\nYou can also copy files to and from a container using the cp command:\\n$ kubectl cp <pod-name>:</path/to/remote/file> </path/to/local/file>\\nThis will copy a file from a running container to your local machine. You can also\\nspecify directories, or reverse the syntax to copy a file from your local machine back\\nout into the container.\\nIf you want to access your Pod via the network, you can use the port-forward com‐\\nmand to forward network traffic from the local machine to the Pod. This enables you\\nto securely tunnel network traffic through to containers that might not be exposed\\nanywhere on the public network. For example, the following command:\\n$ kubectl port-forward <pod-name> 8080:80\\nopens up a connection that forwards traffic from the local machine on port 8080 to\\nthe remote container on port 80.\\nYou can also use the port-forward command with services by\\nspecifying services/<service-name> instead of <pod-name>, but\\nnote that if you do port-forward to a service, the requests will only\\never be forwarded to a single Pod in that service. They will not go\\nthrough the service load balancer.\\nFinally, if you are interested in how your cluster is using resources, you can use the\\ntop command to see the list of resources in use by either nodes or Pods. This\\ncommand:\\nkubectl top nodes\\nwill display the total CPU and memory in use by the nodes in terms of both absolute\\nunits (e.g., cores) and percentage of available resources (e.g., total number of cores).\\nSimilarly, this command:\\nkubectl top pods\\nwill show all Pods and their resource usage. By default it only displays Pods in the\\ncurrent namespace, but you can add the --all-namespaces flag to see resource usage\\nby all Pods in the cluster.\\nDebugging Commands | 41\\nCommand Autocompletion\\nkubectl supports integration with your shell to enable tab completion for both com‐\\nmands and resources. Depending on your environment, you may need to install the\\nbash-completion package before you activate command autocompletion. You can do\\nthis using the appropriate package manager:\\n# macOS\\nbrew install bash-completion\\n# CentOS/Red Hat\\nyum install bash-completion\\n# Debian/Ubuntu\\napt-get install bash-completion\\nWhen installing on macOS, make sure to follow the instructions from brew about\\nhow to activate tab completion using your ${HOME}/.bash_profile.\\nOnce bash-completion is installed, you can temporarily activate it for your terminal\\nusing:\\nsource <(kubectl completion bash)\\nTo make this automatic for every terminal, you can add it to your ${HOME}/.bashrc\\nfile:\\necho \"source <(kubectl completion bash)\" >> ${HOME}/.bashrc\\nIf you use zsh you can find similar instructions online.\\nAlternative Ways of Viewing Your Cluster\\nIn addition to kubectl, there are other tools for interacting with your Kubernetes\\ncluster.\\nFor example, there are plug-ins for several editors that integrate Kubernetes and the\\neditor environment, including:\\n• Visual Studio Code\\n• IntelliJ\\n• Eclipse\\nAdditionally, there is an open source mobile application that allows you to access\\nyour cluster from your phone.\\n42 | Chapter 4: Common kubectl Commands\\nSummary\\nkubectl is a powerful tool for managing your applications in your Kubernetes cluster.\\nThis chapter has illustrated many of the common uses for the tool, but kubectl has a\\ngreat deal of built-in help available. You can start viewing this help with:\\n$ kubectl help\\nor:\\n$ kubectl help <command-name>\\nSummary | 43\\n\\nCHAPTER 5\\nPods\\nIn earlier chapters we discussed how you might go about containerizing your applica‐\\ntion, but in real-world deployments of containerized applications you will often want\\nto colocate multiple applications into a single atomic unit, scheduled onto a single\\nmachine.\\nA canonical example of such a deployment is illustrated in Figure 5-1, which consists\\nof a container serving web requests and a container synchronizing the filesystem with\\na remote Git repository.\\nFigure 5-1. An example Pod with two containers and a shared filesystem\\nAt first, it might seem tempting to wrap up both the web server and the Git syn‐\\nchronizer into a single container. After closer inspection, however, the reasons for the\\nseparation become clear. First, the two different containers have significantly different\\nrequirements in terms of resource usage. Take, for example, memory. Because the\\nweb server is serving user requests, we want to ensure that it is always available and\\nresponsive. On the other hand, the Git synchronizer isn’t really user-facing and has a\\n“best effort” quality of service.\\n45\\nSuppose that our Git synchronizer has a memory leak. We need to ensure that the Git\\nsynchronizer cannot use up memory that we want to use for our web server, since\\nthis can affect web server performance or even crash the server.\\nThis sort of resource isolation is exactly the sort of thing that containers are designed\\nto accomplish. By separating the two applications into two separate containers, we\\ncan ensure reliable web server operation.\\nOf course, the two containers are quite symbiotic; it makes no sense to schedule the\\nweb server on one machine and the Git synchronizer on another. Consequently,\\nKubernetes groups multiple containers into a single atomic unit called a Pod. (The\\nname goes with the whale theme of Docker containers, since a Pod is also a group of\\nwhales.)\\nThough the concept of such sidecars seemed controversial or con‐\\nfusing when it was first introduced in Kubernetes, it has subse‐\\nquently been adopted by a variety of different applications to\\ndeploy their infrastructure. For example, several Service Mesh\\nimplementations use sidecars to inject network management into\\nan application’s Pod.\\nPods in Kubernetes\\nA Pod represents a collection of application containers and volumes running in the\\nsame execution environment. Pods, not containers, are the smallest deployable arti‐\\nfact in a Kubernetes cluster. This means all of the containers in a Pod always land on\\nthe same machine.\\nEach container within a Pod runs in its own cgroup, but they share a number of\\nLinux namespaces.\\nApplications running in the same Pod share the same IP address and port space (net‐\\nwork namespace), have the same hostname (UTS namespace), and can communicate\\nusing native interprocess communication channels over System V IPC or POSIX\\nmessage queues (IPC namespace). However, applications in different Pods are iso‐\\nlated from each other; they have different IP addresses, different hostnames, and\\nmore. Containers in different Pods running on the same node might as well be on\\ndifferent servers.\\nThinking with Pods\\nOne of the most common questions that occurs in the adoption of Kubernetes is\\n“What should I put in a Pod?”\\n46 | Chapter 5: Pods\\nSometimes people see Pods and think, “Aha! A WordPress container and a MySQL\\ndatabase container should be in the same Pod.” However, this kind of Pod is actually\\nan example of an anti-pattern for Pod construction. There are two reasons for this.\\nFirst, WordPress and its database are not truly symbiotic. If the WordPress container\\nand the database container land on different machines, they still can work together\\nquite effectively, since they communicate over a network connection. Secondly, you\\ndon’t necessarily want to scale WordPress and the database as a unit. WordPress itself\\nis mostly stateless, and thus you may want to scale your WordPress frontends in\\nresponse to frontend load by creating more WordPress Pods. Scaling a MySQL data‐\\nbase is much trickier, and you would be much more likely to increase the resources\\ndedicated to a single MySQL Pod. If you group the WordPress and MySQL containers\\ntogether in a single Pod, you are forced to use the same scaling strategy for both con‐\\ntainers, which doesn’t fit well.\\nIn general, the right question to ask yourself when designing Pods is, “Will these con‐\\ntainers work correctly if they land on different machines?” If the answer is “no,” a Pod\\nis the correct grouping for the containers. If the answer is “yes,” multiple Pods is\\nprobably the correct solution. In the example at the beginning of this chapter, the two\\ncontainers interact via a local filesystem. It would be impossible for them to operate\\ncorrectly if the containers were scheduled on different machines.\\nIn the remaining sections of this chapter, we will describe how to create, introspect,\\nmanage, and delete Pods in Kubernetes.\\nThe Pod Manifest\\nPods are described in a Pod manifest. The Pod manifest is just a text-file representa‐\\ntion of the Kubernetes API object. Kubernetes strongly believes in declarative configu‐\\nration. Declarative configuration means that you write down the desired state of the\\nworld in a configuration and then submit that configuration to a service that takes\\nactions to ensure the desired state becomes the actual state.\\nDeclarative configuration is different from imperative configura‐\\ntion, where you simply take a series of actions (e.g., apt-get\\ninstall foo) to modify the world. Years of production experience\\nhave taught us that maintaining a written record of the system’s\\ndesired state leads to a more manageable, reliable system. Declara‐\\ntive configuration enables numerous advantages, including code\\nreview for configurations as well as documenting the current state\\nof the world for distributed teams. Additionally, it is the basis for\\nall of the self-healing behaviors in Kubernetes that keep applica‐\\ntions running without user action.\\nThe Pod Manifest | 47\\nThe Kubernetes API server accepts and processes Pod manifests before storing them\\nin persistent storage (etcd). The scheduler also uses the Kubernetes API to find Pods\\nthat haven’t been scheduled to a node. The scheduler then places the Pods onto nodes\\ndepending on the resources and other constraints expressed in the Pod manifests.\\nMultiple Pods can be placed on the same machine as long as there are sufficient\\nresources. However, scheduling multiple replicas of the same application onto the\\nsame machine is worse for reliability, since the machine is a single failure domain.\\nConsequently, the Kubernetes scheduler tries to ensure that Pods from the same\\napplication are distributed onto different machines for reliability in the presence of\\nsuch failures. Once scheduled to a node, Pods don’t move and must be explicitly\\ndestroyed and rescheduled.\\nMultiple instances of a Pod can be deployed by repeating the workflow described\\nhere. However, ReplicaSets (Chapter 9) are better suited for running multiple instan‐\\nces of a Pod. (It turns out they’re also better at running a single Pod, but we’ll get into\\nthat later.)\\nCreating a Pod\\nThe simplest way to create a Pod is via the imperative kubectl run command. For\\nexample, to run our same kuard server, use:\\n$ kubectl run kuard --generator=run-pod/v1 \\\\\\n --image=gcr.io/kuar-demo/kuard-amd64:blue\\nYou can see the status of this Pod by running:\\n$ kubectl get pods\\nYou may initially see the container as Pending, but eventually you will see it transition\\nto Running, which means that the Pod and its containers have been successfully\\ncreated.\\nFor now, you can delete this Pod by running:\\n$ kubectl delete pods/kuard\\nWe will now move on to writing a complete Pod manifest by hand.\\nCreating a Pod Manifest\\nPod manifests can be written using YAML or JSON, but YAML is generally preferred\\nbecause it is slightly more human-editable and has the ability to add comments. Pod\\nmanifests (and other Kubernetes API objects) should really be treated in the same\\nway as source code, and things like comments help explain the Pod to new team\\nmembers who are looking at them for the first time.\\n48 | Chapter 5: Pods\\nPod manifests include a couple of key fields and attributes: namely a metadata sec‐\\ntion for describing the Pod and its labels, a spec section for describing volumes, and a\\nlist of containers that will run in the Pod.\\nIn Chapter 2 we deployed kuard using the following Docker command:\\n$ docker run -d --name kuard \\\\\\n --publish 8080:8080 \\\\\\n gcr.io/kuar-demo/kuard-amd64:blue\\nA similar result can be achieved by instead writing Example 5-1 to a file named\\nkuard-pod.yaml and then using kubectl commands to load that manifest to\\nKubernetes.\\nExample 5-1. kuard-pod.yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n name: kuard\\nspec:\\n containers:\\n - image: gcr.io/kuar-demo/kuard-amd64:blue\\n name: kuard\\n ports:\\n - containerPort: 8080\\n name: http\\n protocol: TCP\\nRunning Pods\\nIn the previous section we created a Pod manifest that can be used to start a Pod run‐\\nning kuard. Use the kubectl apply command to launch a single instance of kuard:\\n$ kubectl apply -f kuard-pod.yaml\\nThe Pod manifest will be submitted to the Kubernetes API server. The Kubernetes\\nsystem will then schedule that Pod to run on a healthy node in the cluster, where it\\nwill be monitored by the kubelet daemon process. Don’t worry if you don’t under‐\\nstand all the moving parts of Kubernetes right now; we’ll get into more details\\nthroughout the book.\\nListing Pods\\nNow that we have a Pod running, let’s go find out some more about it. Using the\\nkubectl command-line tool, we can list all Pods running in the cluster. For now, this\\nshould only be the single Pod that we created in the previous step:\\nRunning Pods | 49\\n$ kubectl get pods\\nNAME READY STATUS RESTARTS AGE\\nkuard 1/1 Running 0 44s\\nYou can see the name of the Pod (kuard) that we gave it in the previous YAML file. In\\naddition to the number of ready containers (1/1), the output also shows the status,\\nthe number of times the Pod was restarted, as well as the age of the Pod.\\nIf you ran this command immediately after the Pod was created, you might see:\\nNAME READY STATUS RESTARTS AGE\\nkuard 0/1 Pending 0 1s\\nThe Pending state indicates that the Pod has been submitted but hasn’t been sched‐\\nuled yet.\\nIf a more significant error occurs (e.g., an attempt to create a Pod with a container\\nimage that doesn’t exist), it will also be listed in the status field.\\nBy default, the kubectl command-line tool tries to be concise in\\nthe information it reports, but you can get more information via\\ncommand-line flags. Adding -o wide to any kubectl command\\nwill print out slightly more information (while still trying to keep\\nthe information to a single line). Adding -o json or -o yaml will\\nprint out the complete objects in JSON or YAML, respectively.\\nPod Details\\nSometimes, the single-line view is insufficient because it is too terse. Additionally,\\nKubernetes maintains numerous events about Pods that are present in the event\\nstream, not attached to the Pod object.\\nTo find out more information about a Pod (or any Kubernetes object) you can use the\\nkubectl describe command. For example, to describe the Pod we previously cre‐\\nated, you can run:\\n$ kubectl describe pods kuard\\nThis outputs a bunch of information about the Pod in different sections. At the top is\\nbasic information about the Pod:\\nName: kuard\\nNamespace: default\\nNode: node1/10.0.15.185\\nStart Time: Sun, 02 Jul 2017 15:00:38 -0700\\nLabels: <none>\\nAnnotations: <none>\\nStatus: Running\\nIP: 192.168.199.238\\nControllers: <none>\\n50 | Chapter 5: Pods\\nThen there is information about the containers running in the Pod:\\nContainers:\\n kuard:\\n Container ID: docker://055095…\\n Image: gcr.io/kuar-demo/kuard-amd64:blue\\n Image ID: docker-pullable://gcr.io/kuar-demo/kuard-amd64@sha256:a580…\\n Port: 8080/TCP\\n State: Running\\n Started: Sun, 02 Jul 2017 15:00:41 -0700\\n Ready: True\\n Restart Count: 0\\n Environment: <none>\\n Mounts:\\n /var/run/secrets/kubernetes.io/serviceaccount from default-token-cg5f5 (ro)\\nFinally, there are events related to the Pod, such as when it was scheduled, when its\\nimage was pulled, and if/when it had to be restarted because of failing health checks:\\nEvents:\\n Seen From SubObjectPath Type Reason Message\\n ---- ---- ------------- -------- ------ -------\\n 50s default-scheduler Normal Scheduled Success…\\n 49s kubelet, node1 spec.containers{kuard} Normal Pulling pulling…\\n 47s kubelet, node1 spec.containers{kuard} Normal Pulled Success…\\n 47s kubelet, node1 spec.containers{kuard} Normal Created Created…\\n 47s kubelet, node1 spec.containers{kuard} Normal Started Started…\\nDeleting a Pod\\nWhen it is time to delete a Pod, you can delete it either by name:\\n$ kubectl delete pods/kuard\\nor using the same file that you used to create it:\\n$ kubectl delete -f kuard-pod.yaml\\nWhen a Pod is deleted, it is not immediately killed. Instead, if you run kubectl get\\npods you will see that the Pod is in the Terminating state. All Pods have a termina‐\\ntion grace period. By default, this is 30 seconds. When a Pod is transitioned to\\nTerminating it no longer receives new requests. In a serving scenario, the grace\\nperiod is important for reliability because it allows the Pod to finish any active\\nrequests that it may be in the middle of processing before it is terminated.\\nIt’s important to note that when you delete a Pod, any data stored in the containers\\nassociated with that Pod will be deleted as well. If you want to persist data across mul‐\\ntiple instances of a Pod, you need to use PersistentVolumes, described at the end of\\nthis chapter.\\nRunning Pods | 51\\nAccessing Your Pod\\nNow that your Pod is running, you’re going to want to access it for a variety of rea‐\\nsons. You may want to load the web service that is running in the Pod. You may want\\nto view its logs to debug a problem that you are seeing, or even execute other com‐\\nmands in the context of the Pod to help debug. The following sections detail various\\nways that you can interact with the code and data running inside your Pod.\\nUsing Port Forwarding\\nLater in the book, we’ll show how to expose a service to the world or other containers\\nusing load balancers—but oftentimes you simply want to access a specific Pod, even if\\nit’s not serving traffic on the internet.\\nTo achieve this, you can use the port-forwarding support built into the Kubernetes\\nAPI and command-line tools.\\nWhen you run:\\n$ kubectl port-forward kuard 8080:8080\\na secure tunnel is created from your local machine, through the Kubernetes master, to\\nthe instance of the Pod running on one of the worker nodes.\\nAs long as the port-forward command is still running, you can access the Pod (in\\nthis case the kuard web interface) at http://localhost:8080.\\nGetting More Info with Logs\\nWhen your application needs debugging, it’s helpful to be able to dig deeper than\\ndescribe to understand what the application is doing. Kubernetes provides two com‐\\nmands for debugging running containers. The kubectl logs command downloads\\nthe current logs from the running instance:\\n$ kubectl logs kuard\\nAdding the -f flag will cause you to continuously stream logs.\\nThe kubectl logs command always tries to get logs from the currently running con‐\\ntainer. Adding the --previous flag will get logs from a previous instance of the con‐\\ntainer. This is useful, for example, if your containers are continuously restarting due\\nto a problem at container startup.\\n52 | Chapter 5: Pods\\nWhile using kubectl logs is useful for one-off debugging of con‐\\ntainers in production environments, it’s generally useful to use a log\\naggregation service. There are several open source log aggregation\\ntools, like fluentd and elasticsearch, as well as numerous cloud\\nlogging providers. Log aggregation services provide greater\\ncapacity for storing a longer duration of logs, as well as rich log\\nsearching and filtering capabilities. Finally, they often provide the\\nability to aggregate logs from multiple Pods into a single view.\\nRunning Commands in Your Container with exec\\nSometimes logs are insufficient, and to truly determine what’s going on you need to\\nexecute commands in the context of the container itself. To do this you can use:\\n$ kubectl exec kuard date\\nYou can also get an interactive session by adding the -it flags:\\n$ kubectl exec -it kuard ash\\nCopying Files to and from Containers\\nAt times you may need to copy files from a remote container to a local machine for\\nmore in-depth exploration. For example, you can use a tool like Wireshark to visual‐\\nize tcpdump packet captures. Suppose you had a file called /captures/capture3.txt\\ninside a container in your Pod. You could securely copy that file to your local\\nmachine by running:\\n$ kubectl cp <pod-name>:/captures/capture3.txt ./capture3.txt\\nOther times you may need to copy files from your local machine into a container.\\nLet’s say you want to copy $HOME/config.txt to a remote container. In this case, you\\ncan run:\\n$ kubectl cp $HOME/config.txt <pod-name>:/config.txt\\nGenerally speaking, copying files into a container is an anti-pattern. You really should\\ntreat the contents of a container as immutable. But occasionally it’s the most immedi‐\\nate way to stop the bleeding and restore your service to health, since it is quicker than\\nbuilding, pushing, and rolling out a new image. Once the bleeding is stopped, how‐\\never, it is critically important that you immediately go and do the image build and\\nrollout, or you are guaranteed to forget the local change that you made to your con‐\\ntainer and overwrite it in the subsequent regularly scheduled rollout.\\nAccessing Your Pod | 53\\nHealth Checks\\nWhen you run your application as a container in Kubernetes, it is automatically kept\\nalive for you using a process health check. This health check simply ensures that the\\nmain process of your application is always running. If it isn’t, Kubernetes restarts it.\\nHowever, in most cases, a simple process check is insufficient. For example, if your\\nprocess has deadlocked and is unable to serve requests, a process health check will\\nstill believe that your application is healthy since its process is still running.\\nTo address this, Kubernetes introduced health checks for application liveness.\\nLiveness health checks run application-specific logic (e.g., loading a web page) to ver‐\\nify that the application is not just still running, but is functioning properly. Since\\nthese liveness health checks are application-specific, you have to define them in your\\nPod manifest.\\nLiveness Probe\\nOnce the kuard process is up and running, we need a way to confirm that it is\\nactually healthy and shouldn’t be restarted. Liveness probes are defined per container,\\nwhich means each container inside a Pod is health-checked separately. In\\nExample 5-2, we add a liveness probe to our kuard container, which runs an HTTP\\nrequest against the /healthy path on our container.\\nExample 5-2. kuard-pod-health.yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n name: kuard\\nspec:\\n containers:\\n - image: gcr.io/kuar-demo/kuard-amd64:blue\\n name: kuard\\n livenessProbe:\\n httpGet:\\n path: /healthy\\n port: 8080\\n initialDelaySeconds: 5\\n timeoutSeconds: 1\\n periodSeconds: 10\\n failureThreshold: 3\\n ports:\\n - containerPort: 8080\\n name: http\\n protocol: TCP\\n54 | Chapter 5: Pods\\nThe preceding Pod manifest uses an httpGet probe to perform an HTTP GET request\\nagainst the /healthy endpoint on port 8080 of the kuard container. The probe sets an\\ninitialDelaySeconds of 5, and thus will not be called until 5 seconds after all the\\ncontainers in the Pod are created. The probe must respond within the 1-second time‐\\nout, and the HTTP status code must be equal to or greater than 200 and less than 400\\nto be considered successful. Kubernetes will call the probe every 10 seconds. If more\\nthan three consecutive probes fail, the container will fail and restart.\\nYou can see this in action by looking at the kuard status page. Create a Pod using this\\nmanifest and then port-forward to that Pod:\\n$ kubectl apply -f kuard-pod-health.yaml\\n$ kubectl port-forward kuard 8080:8080\\nPoint your browser to http://localhost:8080. Click the “Liveness Probe” tab. You\\nshould see a table that lists all of the probes that this instance of kuard has received. If\\nyou click the “Fail” link on that page, kuard will start to fail health checks. Wait long\\nenough and Kubernetes will restart the container. At that point the display will reset\\nand start over again. Details of the restart can be found with kubectl describe pods\\nkuard. The “Events” section will have text similar to the following:\\nKilling container with id docker://2ac946...:pod \"kuard_default(9ee84...)\"\\ncontainer \"kuard\" is unhealthy, it will be killed and re-created.\\nWhile the default response to a failed liveness check is to restart the\\nPod, the actual behavior is governed by the Pod’s restartPolicy.\\nThere are three options for the restart policy: Always (the default),\\nOnFailure (restart only on liveness failure or nonzero process exit\\ncode), or Never.\\nReadiness Probe\\nOf course, liveness isn’t the only kind of health check we want to perform. Kubernetes\\nmakes a distinction between liveness and readiness. Liveness determines if an applica‐\\ntion is running properly. Containers that fail liveness checks are restarted. Readiness\\ndescribes when a container is ready to serve user requests. Containers that fail readi‐\\nness checks are removed from service load balancers. Readiness probes are config‐\\nured similarly to liveness probes. We explore Kubernetes services in detail in\\nChapter 7.\\nCombining the readiness and liveness probes helps ensure only healthy containers\\nare running within the cluster.\\nHealth Checks | 55\\nTypes of Health Checks\\nIn addition to HTTP checks, Kubernetes also supports tcpSocket health checks that\\nopen a TCP socket; if the connection is successful, the probe succeeds. This style of\\nprobe is useful for non-HTTP applications; for example, databases or other non–\\nHTTP-based APIs.\\nFinally, Kubernetes allows exec probes. These execute a script or program in the con‐\\ntext of the container. Following typical convention, if this script returns a zero exit\\ncode, the probe succeeds; otherwise, it fails. exec scripts are often useful for custom\\napplication validation logic that doesn’t fit neatly into an HTTP call.\\nResource Management\\nMost people move into containers and orchestrators like Kubernetes because of the\\nradical improvements in image packaging and reliable deployment they provide. In\\naddition to application-oriented primitives that simplify distributed system develop‐\\nment, equally important is the ability to increase the overall utilization of the com‐\\npute nodes that make up the cluster. The basic cost of operating a machine, either\\nvirtual or physical, is basically constant regardless of whether it is idle or fully loaded.\\nConsequently, ensuring that these machines are maximally active increases the effi‐\\nciency of every dollar spent on infrastructure.\\nGenerally speaking, we measure this efficiency with the utilization metric. Utilization\\nis defined as the amount of a resource actively being used divided by the amount of a\\nresource that has been purchased. For example, if you purchase a one-core machine,\\nand your application uses one-tenth of a core, then your utilization is 10%.\\nWith scheduling systems like Kubernetes managing resource packing, you can drive\\nyour utilization to greater than 50%.\\nTo achieve this, you have to tell Kubernetes about the resources your application\\nrequires, so that Kubernetes can find the optimal packing of containers onto pur‐\\nchased machines.\\nKubernetes allows users to specify two different resource metrics. Resource requests\\nspecify the minimum amount of a resource required to run the application. Resource\\nlimits specify the maximum amount of a resource that an application can consume.\\nBoth of these resource definitions are described in greater detail in the following\\nsections.\\nResource Requests: Minimum Required Resources\\nWith Kubernetes, a Pod requests the resources required to run its containers. Kuber‐\\nnetes guarantees that these resources are available to the Pod. The most commonly\\n56 | Chapter 5: Pods\\nrequested resources are CPU and memory, but Kubernetes has support for other\\nresource types as well, such as GPUs and more.\\nFor example, to request that the kuard container lands on a machine with half a CPU\\nfree and gets 128 MB of memory allocated to it, we define the Pod as shown in\\nExample 5-3.\\nExample 5-3. kuard-pod-resreq.yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n name: kuard\\nspec:\\n containers:\\n - image: gcr.io/kuar-demo/kuard-amd64:blue\\n name: kuard\\n resources:\\n requests:\\n cpu: \"500m\"\\n memory: \"128Mi\"\\n ports:\\n - containerPort: 8080\\n name: http\\n protocol: TCP\\nResources are requested per container, not per Pod. The total\\nresources requested by the Pod is the sum of all resources reques‐\\nted by all containers in the Pod. The reason for this is that in many\\ncases the different containers have very different CPU require‐\\nments. For example, in the web server and data synchronizer Pod,\\nthe web server is user-facing and likely needs a great deal of CPU,\\nwhile the data synchronizer can make do with very little.\\nRequest limit details\\nRequests are used when scheduling Pods to nodes. The Kubernetes scheduler will\\nensure that the sum of all requests of all Pods on a node does not exceed the capacity\\nof the node. Therefore, a Pod is guaranteed to have at least the requested resources\\nwhen running on the node. Importantly, “request” specifies a minimum. It does not\\nspecify a maximum cap on the resources a Pod may use. To explore what this means,\\nlet’s look at an example.\\nImagine that we have container whose code attempts to use all available CPU cores.\\nSuppose that we create a Pod with this container that requests 0.5 CPU. Kubernetes\\nschedules this Pod onto a machine with a total of 2 CPU cores.\\nResource Management | 57\\nAs long as it is the only Pod on the machine, it will consume all 2.0 of the available\\ncores, despite only requesting 0.5 CPU.\\nIf a second Pod with the same container and the same request of 0.5 CPU lands on\\nthe machine, then each Pod will receive 1.0 cores.\\nIf a third identical Pod is scheduled, each Pod will receive 0.66 cores. Finally, if a\\nfourth identical Pod is scheduled, each Pod will receive the 0.5 core it requested, and\\nthe node will be at capacity.\\nCPU requests are implemented using the cpu-shares functionality in the Linux\\nkernel.\\nMemory requests are handled similarly to CPU, but there is an\\nimportant difference. If a container is over its memory request, the\\nOS can’t just remove memory from the process, because it’s been\\nallocated. Consequently, when the system runs out of memory, the\\nkubelet terminates containers whose memory usage is greater\\nthan their requested memory. These containers are automatically\\nrestarted, but with less available memory on the machine for the\\ncontainer to consume.\\nSince resource requests guarantee resource availability to a Pod, they are critical to\\nensuring that containers have sufficient resources in high-load situations.\\nCapping Resource Usage with Limits\\nIn addition to setting the resources required by a Pod, which establishes the mini‐\\nmum resources available to the Pod, you can also set a maximum on a Pod’s resource\\nusage via resource limits.\\nIn our previous example we created a kuard Pod that requested a minimum of 0.5 of\\na core and 128 MB of memory. In the Pod manifest in Example 5-4, we extend this\\nconfiguration to add a limit of 1.0 CPU and 256 MB of memory.\\nExample 5-4. kuard-pod-reslim.yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n name: kuard\\nspec:\\n containers:\\n - image: gcr.io/kuar-demo/kuard-amd64:blue\\n name: kuard\\n resources:\\n requests:\\n58 | Chapter 5: Pods\\n cpu: \"500m\"\\n memory: \"128Mi\"\\n limits:\\n cpu: \"1000m\"\\n memory: \"256Mi\"\\n ports:\\n - containerPort: 8080\\n name: http\\n protocol: TCP\\nWhen you establish limits on a container, the kernel is configured to ensure that con‐\\nsumption cannot exceed these limits. A container with a CPU limit of 0.5 cores will\\nonly ever get 0.5 cores, even if the CPU is otherwise idle. A container with a memory\\nlimit of 256 MB will not be allowed additional memory (e.g., malloc will fail) if its\\nmemory usage exceeds 256 MB.\\nPersisting Data with Volumes\\nWhen a Pod is deleted or a container restarts, any and all data in the container’s file‐\\nsystem is also deleted. This is often a good thing, since you don’t want to leave around\\ncruft that happened to be written by your stateless web application. In other cases,\\nhaving access to persistent disk storage is an important part of a healthy application.\\nKubernetes models such persistent storage.\\nUsing Volumes with Pods\\nTo add a volume to a Pod manifest, there are two new stanzas to add to our configu‐\\nration. The first is a new spec.volumes section. This array defines all of the volumes\\nthat may be accessed by containers in the Pod manifest. It’s important to note that not\\nall containers are required to mount all volumes defined in the Pod. The second addi‐\\ntion is the volumeMounts array in the container definition. This array defines the vol‐\\numes that are mounted into a particular container, and the path where each volume\\nshould be mounted. Note that two different containers in a Pod can mount the same\\nvolume at different mount paths.\\nThe manifest in Example 5-5 defines a single new volume named kuard-data, which\\nthe kuard container mounts to the /data path.\\nExample 5-5. kuard-pod-vol.yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n name: kuard\\nspec:\\n volumes:\\nPersisting Data with Volumes | 59\\n - name: \"kuard-data\"\\n hostPath:\\n path: \"/var/lib/kuard\"\\n containers:\\n - image: gcr.io/kuar-demo/kuard-amd64:blue\\n name: kuard\\n volumeMounts:\\n - mountPath: \"/data\"\\n name: \"kuard-data\"\\n ports:\\n - containerPort: 8080\\n name: http\\n protocol: TCP\\nDifferent Ways of Using Volumes with Pods\\nThere are a variety of ways you can use data in your application. The following are a\\nfew, and the recommended patterns for Kubernetes.\\nCommunication/synchronization\\nIn the first example of a Pod, we saw how two containers used a shared volume to\\nserve a site while keeping it synchronized to a remote Git location. To achieve this,\\nthe Pod uses an emptyDir volume. Such a volume is scoped to the Pod’s lifespan, but\\nit can be shared between two containers, forming the basis for communication\\nbetween our Git sync and web serving containers.\\nCache\\nAn application may use a volume that is valuable for performance, but not required\\nfor correct operation of the application. For example, perhaps the application keeps\\nprerendered thumbnails of larger images. Of course, they can be reconstructed from\\nthe original images, but that makes serving the thumbnails more expensive. You want\\nsuch a cache to survive a container restart due to a health-check failure, and thus\\nemptyDir works well for the cache use case as well.\\nPersistent data\\nSometimes you will use a volume for truly persistent data—data that is independent\\nof the lifespan of a particular Pod, and should move between nodes in the cluster if a\\nnode fails or a Pod moves to a different machine for some reason. To achieve this,\\nKubernetes supports a wide variety of remote network storage volumes, including\\nwidely supported protocols like NFS and iSCSI as well as cloud provider network\\nstorage like Amazon’s Elastic Block Store, Azure’s Files and Disk Storage, as well as\\nGoogle’s Persistent Disk.\\n60 | Chapter 5: Pods\\nMounting the host \\x80lesystem\\nOther applications don’t actually need a persistent volume, but they do need some\\naccess to the underlying host filesystem. For example, they may need access to\\nthe /dev filesystem in order to perform raw block-level access to a device on the sys‐\\ntem. For these cases, Kubernetes supports the hostPath volume, which can mount\\narbitrary locations on the worker node into the container.\\nThe previous example uses the hostPath volume type. The volume created is /var/lib/\\nkuard on the host.\\nPersisting Data Using Remote Disks\\nOftentimes, you want the data a Pod is using to stay with the Pod, even if it is restar‐\\nted on a different host machine.\\nTo achieve this, you can mount a remote network storage volume into your Pod.\\nWhen using network-based storage, Kubernetes automatically mounts and unmounts\\nthe appropriate storage whenever a Pod using that volume is scheduled onto a partic‐\\nular machine.\\nThere are numerous methods for mounting volumes over the network. Kubernetes\\nincludes support for standard protocols such as NFS and iSCSI as well as cloud pro‐\\nvider–based storage APIs for the major cloud providers (both public and private). In\\nmany cases, the cloud providers will also create the disk for you if it doesn’t already\\nexist.\\nHere is an example of using an NFS server:\\n...\\n# Rest of pod definition above here\\nvolumes:\\n - name: \"kuard-data\"\\n nfs:\\n server: my.nfs.server.local\\n path: \"/exports\"\\nPersistent volumes are a deep topic that has many different details: in particular, the\\nmanner in which persistent volumes, persistent volume claims, and dynamic volume\\nprovisioning work together. There is a more in-depth examination of the subject in\\nChapter 15.\\nPutting It All Together\\nMany applications are stateful, and as such we must preserve any data and ensure\\naccess to the underlying storage volume regardless of what machine the application\\nruns on. As we saw earlier, this can be achieved using a persistent volume backed by\\nnetwork-attached storage. We also want to ensure that a healthy instance of the\\nPutting It All Together | 61\\napplication is running at all times, which means we want to make sure the container\\nrunning kuard is ready before we expose it to clients.\\nThrough a combination of persistent volumes, readiness and liveness probes, and\\nresource restrictions, Kubernetes provides everything needed to run stateful applica‐\\ntions reliably. Example 5-6 pulls this all together into one manifest.\\nExample 5-6. kuard-pod-full.yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n name: kuard\\nspec:\\n volumes:\\n - name: \"kuard-data\"\\n nfs:\\n server: my.nfs.server.local\\n path: \"/exports\"\\n containers:\\n - image: gcr.io/kuar-demo/kuard-amd64:blue\\n name: kuard\\n ports:\\n - containerPort: 8080\\n name: http\\n protocol: TCP\\n resources:\\n requests:\\n cpu: \"500m\"\\n memory: \"128Mi\"\\n limits:\\n cpu: \"1000m\"\\n memory: \"256Mi\"\\n volumeMounts:\\n - mountPath: \"/data\"\\n name: \"kuard-data\"\\n livenessProbe:\\n httpGet:\\n path: /healthy\\n port: 8080\\n initialDelaySeconds: 5\\n timeoutSeconds: 1\\n periodSeconds: 10\\n failureThreshold: 3\\n readinessProbe:\\n httpGet:\\n path: /ready\\n port: 8080\\n initialDelaySeconds: 30\\n timeoutSeconds: 1\\n62 | Chapter 5: Pods\\n periodSeconds: 10\\n failureThreshold: 3\\nSummary\\nPods represent the atomic unit of work in a Kubernetes cluster. Pods are comprised of\\none or more containers working together symbiotically. To create a Pod, you write a\\nPod manifest and submit it to the Kubernetes API server by using the command-line\\ntool or (less frequently) by making HTTP and JSON calls to the server directly.\\nOnce you’ve submitted the manifest to the API server, the Kubernetes scheduler finds\\na machine where the Pod can fit and schedules the Pod to that machine. Once sched‐\\nuled, the kubelet daemon on that machine is responsible for creating the containers\\nthat correspond to the Pod, as well as performing any health checks defined in the\\nPod manifest.\\nOnce a Pod is scheduled to a node, no rescheduling occurs if that node fails. Addi‐\\ntionally, to create multiple replicas of the same Pod you have to create and name them\\nmanually. In a later chapter we introduce the ReplicaSet object and show how you can\\nautomate the creation of multiple identical Pods and ensure that they are recreated in\\nthe event of a node machine failure.\\nSummary | 63\\n\\nCHAPTER 6\\nLabels and Annotations\\nKubernetes was made to grow with you as your application scales in both size and\\ncomplexity. With this in mind, labels and annotations were added as foundational\\nconcepts. Labels and annotations let you work in sets of things that map to how you\\nthink about your application. You can organize, mark, and cross-index all of your\\nresources to represent the groups that make the most sense for your application.\\nLabels are key/value pairs that can be attached to Kubernetes objects such as Pods and\\nReplicaSets. They can be arbitrary, and are useful for attaching identifying informa‐\\ntion to Kubernetes objects. Labels provide the foundation for grouping objects.\\nAnnotations, on the other hand, provide a storage mechanism that resembles labels:\\nannotations are key/value pairs designed to hold nonidentifying information that can\\nbe leveraged by tools and libraries.\\nLabels\\nLabels provide identifying metadata for objects. These are fundamental qualities of\\nthe object that will be used for grouping, viewing, and operating.\\n65\\nThe motivations for labels grew out of Google’s experience in run‐\\nning large and complex applications. There were a couple of les‐\\nsons that emerged from this experience. See the great site reliability\\nbook Site Reliability Engineering by Betsy Beyer et al. (O’Reilly) for\\nsome deeper background on how Google approaches production\\nsystems.\\nThe first lesson is that production abhors a singleton. When\\ndeploying software, users will often start with a single instance.\\nHowever, as the application matures, these singletons often multi‐\\nply and become sets of objects. With this in mind, Kubernetes uses\\nlabels to deal with sets of objects instead of single instances.\\nThe second lesson is that any hierarchy imposed by the system will\\nfall short for many users. In addition, user groupings and hierar‐\\nchies are subject to change over time. For instance, a user may start\\nout with the idea that all apps are made up of many services. How‐\\never, over time, a service may be shared across multiple apps.\\nKubernetes labels are flexible enough to adapt to these situations\\nand more.\\nLabels have simple syntax. They are key/value pairs, where both the key and value are\\nrepresented by strings. Label keys can be broken down into two parts: an optional\\nprefix and a name, separated by a slash. The prefix, if specified, must be a DNS sub‐\\ndomain with a 253-character limit. The key name is required and must be shorter\\nthan 63 characters. Names must also start and end with an alphanumeric character\\nand permit the use of dashes (-), underscores (_), and dots (.) between characters.\\nLabel values are strings with a maximum length of 63 characters. The contents of the\\nlabel values follow the same rules as for label keys.\\nTable 6-1 shows some valid label keys and values.\\nTable 6-1. Label examples\\nKey Value\\nacme.com/app-version 1.0.0\\nappVersion 1.0.0\\napp.version 1.0.0\\nkubernetes.io/cluster-service true\\nWhen domain names are used in labels and annotations they are expected to be\\naligned to that particular entity in some way. For example, a project might define a\\ncanonical set of labels used to identify the various stages of application deployment\\n(e.g., staging, canary, production).\\n66 | Chapter 6: Labels and Annotations\\nOr a cloud provider might define provider-specific annotations that extend Kuber‐\\nnetes objects to activate features specific to their service.\\nApplying Labels\\nHere we create a few deployments (a way to create an array of Pods) with some inter‐\\nesting labels. We’ll take two apps (called alpaca and bandicoot) and have two envi‐\\nronments for each. We will also have two different versions:\\n1. First, create the alpaca-prod deployment and set the ver, app, and env labels:\\n$ kubectl run alpaca-prod \\\\\\n --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\\\n --replicas=2 \\\\\\n --labels=\"ver=1,app=alpaca,env=prod\"\\n2. Next, create the alpaca-test deployment and set the ver, app, and env labels\\nwith the appropriate values:\\n$ kubectl run alpaca-test \\\\\\n --image=gcr.io/kuar-demo/kuard-amd64:green \\\\\\n --replicas=1 \\\\\\n --labels=\"ver=2,app=alpaca,env=test\"\\n3. Finally, create two deployments for bandicoot. Here we name the environments\\nprod and staging:\\n$ kubectl run bandicoot-prod \\\\\\n --image=gcr.io/kuar-demo/kuard-amd64:green \\\\\\n --replicas=2 \\\\\\n --labels=\"ver=2,app=bandicoot,env=prod\"\\n$ kubectl run bandicoot-staging \\\\\\n --image=gcr.io/kuar-demo/kuard-amd64:green \\\\\\n --replicas=1 \\\\\\n --labels=\"ver=2,app=bandicoot,env=staging\"\\nAt this point you should have four deployments—alpaca-prod, alpaca-test,\\nbandicoot-prod, and bandicoot-staging:\\n$ kubectl get deployments --show-labels\\nNAME ... LABELS\\nalpaca-prod ... app=alpaca,env=prod,ver=1\\nalpaca-test ... app=alpaca,env=test,ver=2\\nbandicoot-prod ... app=bandicoot,env=prod,ver=2\\nbandicoot-staging ... app=bandicoot,env=staging,ver=2\\nWe can visualize this as a Venn diagram based on the labels (Figure 6-1).\\nLabels | 67\\nFigure 6-1. Visualization of labels applied to our deployments\\nModifying Labels\\nLabels can also be applied (or updated) on objects after they are created:\\n$ kubectl label deployments alpaca-test \"canary=true\"\\nThere is a caveat to be aware of here. In this example, the kubectl\\nlabel command will only change the label on the deployment\\nitself; it won’t affect the objects (ReplicaSets and Pods) the deploy‐\\nment creates. To change those, you’ll need to change the template\\nembedded in the deployment (see Chapter 10).\\nYou can also use the -L option to kubectl get to show a label value as a column:\\n$ kubectl get deployments -L canary\\nNAME DESIRED CURRENT ... CANARY\\nalpaca-prod 2 2 ... <none>\\nalpaca-test 1 1 ... true\\nbandicoot-prod 2 2 ... <none>\\nbandicoot-staging 1 1 ... <none>\\nYou can remove a label by applying a dash suffix:\\n$ kubectl label deployments alpaca-test \"canary-\"\\nLabel Selectors\\nLabel selectors are used to filter Kubernetes objects based on a set of labels. Selectors\\nuse a simple Boolean language. They are used both by end users (via tools like\\nkubectl) and by different types of objects (such as how a ReplicaSet relates to its\\nPods).\\n68 | Chapter 6: Labels and Annotations\\nEach deployment (via a ReplicaSet) creates a set of Pods using the labels specified in\\nthe template embedded in the deployment. This is configured by the kubectl run\\ncommand.\\nRunning the kubectl get pods command should return all the Pods currently run‐\\nning in the cluster. We should have a total of six kuard Pods across our three\\nenvironments:\\n$ kubectl get pods --show-labels\\nNAME ... LABELS\\nalpaca-prod-3408831585-4nzfb ... app=alpaca,env=prod,ver=1,...\\nalpaca-prod-3408831585-kga0a ... app=alpaca,env=prod,ver=1,...\\nalpaca-test-1004512375-3r1m5 ... app=alpaca,env=test,ver=2,...\\nbandicoot-prod-373860099-0t1gp ... app=bandicoot,env=prod,ver=2,...\\nbandicoot-prod-373860099-k2wcf ... app=bandicoot,env=prod,ver=2,...\\nbandicoot-staging-1839769971-3ndv ... app=bandicoot,env=staging,ver=2,...\\nYou may see a new label that we haven’t seen yet: pod-templatehash. This label is applied by the deployment so it can keep track of\\nwhich Pods were generated from which template versions. This\\nallows the deployment to manage updates in a clean way, as will be\\ncovered in depth in Chapter 10.\\nIf we only wanted to list Pods that had the ver label set to 2, we could use the\\n--selector flag:\\n$ kubectl get pods --selector=\"ver=2\"\\nNAME READY STATUS RESTARTS AGE\\nalpaca-test-1004512375-3r1m5 1/1 Running 0 3m\\nbandicoot-prod-373860099-0t1gp 1/1 Running 0 3m\\nbandicoot-prod-373860099-k2wcf 1/1 Running 0 3m\\nbandicoot-staging-1839769971-3ndv5 1/1 Running 0 3m\\nIf we specify two selectors separated by a comma, only the objects that satisfy both\\nwill be returned. This is a logical AND operation:\\n$ kubectl get pods --selector=\"app=bandicoot,ver=2\"\\nNAME READY STATUS RESTARTS AGE\\nbandicoot-prod-373860099-0t1gp 1/1 Running 0 4m\\nbandicoot-prod-373860099-k2wcf 1/1 Running 0 4m\\nbandicoot-staging-1839769971-3ndv5 1/1 Running 0 4m\\nWe can also ask if a label is one of a set of values. Here we ask for all Pods where the\\napp label is set to alpaca or bandicoot (which will be all six Pods):\\n$ kubectl get pods --selector=\"app in (alpaca,bandicoot)\"\\nLabels | 69\\nNAME READY STATUS RESTARTS AGE\\nalpaca-prod-3408831585-4nzfb 1/1 Running 0 6m\\nalpaca-prod-3408831585-kga0a 1/1 Running 0 6m\\nalpaca-test-1004512375-3r1m5 1/1 Running 0 6m\\nbandicoot-prod-373860099-0t1gp 1/1 Running 0 6m\\nbandicoot-prod-373860099-k2wcf 1/1 Running 0 6m\\nbandicoot-staging-1839769971-3ndv5 1/1 Running 0 6m\\nFinally, we can ask if a label is set at all. Here we are asking for all of the deployments\\nwith the canary label set to anything:\\n$ kubectl get deployments --selector=\"canary\"\\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE\\nalpaca-test 1 1 1 1 7m\\nThere are also “negative” versions of each of these, as shown in Table 6-2.\\nTable 6-2. Selector operators\\nOperator Description\\nkey=value key is set to value\\nkey!=value key is not set to value\\nkey in (value1, value2) key is one of value1 or value2\\nkey notin (value1, value2) key is not one of value1 or value2\\nkey key is set\\n!key key is not set\\nFor example, asking if a key, in this case canary, is not set can look like:\\n$ kubectl get deployments --selector=\\'!canary\\'\\nSimilarly, you can combine positive and negative selectors together as follows:\\n$ kubectl get pods -l \\'ver=2,!canary\\'\\nLabel Selectors in API Objects\\nWhen a Kubernetes object refers to a set of other Kubernetes objects, a label selector\\nis used. Instead of a simple string as described in the previous section, we use a\\nparsed structure.\\nFor historical reasons (Kubernetes doesn’t break API compatibility!), there are two\\nforms. Most objects support a newer, more powerful set of selector operators.\\nA selector of app=alpaca,ver in (1, 2) would be converted to this:\\nselector:\\n matchLabels:\\n app: alpaca\\n70 | Chapter 6: Labels and Annotations\\n matchExpressions:\\n - {key: ver, operator: In, values: [1, 2]}\\nCompact YAML syntax. This is an item in a list (matchExpressions) that is a\\nmap with three entries. The last entry (values) has a value that is a list with two\\nitems.\\nAll of the terms are evaluated as a logical AND. The only way to represent the !=\\noperator is to convert it to a NotIn expression with a single value.\\nThe older form of specifying selectors (used in ReplicationControllers and serv‐\\nices) only supports the = operator. This is a simple set of key/value pairs that must all\\nmatch a target object to be selected.\\nThe selector app=alpaca,ver=1 would be represented like this:\\nselector:\\n app: alpaca\\n ver: 1\\nLabels in the Kubernetes Architecture\\nIn addition to enabling users to organize their infrastructure, labels play a critical role\\nin linking various related Kubernetes objects. Kubernetes is a purposefully decoupled\\nsystem. There is no hierarchy and all components operate independently. However, in\\nmany cases objects need to relate to one another, and these relationships are defined\\nby labels and label selectors.\\nFor example, ReplicaSets, which create and maintain multiple replicas of a Pod, find\\nthe Pods that they are managing via a selector. Likewise, a service load balancer finds\\nthe Pods it should bring traffic to via a selector query. When a Pod is created, it can\\nuse a node selector to identify a particular set of nodes that it can be scheduled onto.\\nWhen people want to restrict network traffic in their cluster, they use NetworkPolicy\\nin conjunction with specific labels to identify Pods that should or should not be\\nallowed to communicate with each other. Labels are a powerful and ubiquitous glue\\nthat holds a Kubernetes application together. Though your application will likely start\\nout with a simple set of labels and queries, you should expect it to grow in size and\\nsophistication with time.\\nAnnotations\\nAnnotations provide a place to store additional metadata for Kubernetes objects with\\nthe sole purpose of assisting tools and libraries. They are a way for other programs\\ndriving Kubernetes via an API to store some opaque data with an object. Annotations\\ncan be used for the tool itself or to pass configuration information between external\\nsystems.\\nAnnotations | 71\\nWhile labels are used to identify and group objects, annotations are used to provide\\nextra information about where an object came from, how to use it, or policy around\\nthat object. There is overlap, and it is a matter of taste as to when to use an annotation\\nor a label. When in doubt, add information to an object as an annotation and pro‐\\nmote it to a label if you find yourself wanting to use it in a selector.\\nAnnotations are used to:\\n• Keep track of a “reason” for the latest update to an object.\\n• Communicate a specialized scheduling policy to a specialized scheduler.\\n• Extend data about the last tool to update the resource and how it was updated\\n(used for detecting changes by other tools and doing a smart merge).\\n• Attach build, release, or image information that isn’t appropriate for labels (may\\ninclude a Git hash, timestamp, PR number, etc.).\\n• Enable the Deployment object (Chapter 10) to keep track of ReplicaSets that it is\\nmanaging for rollouts.\\n• Provide extra data to enhance the visual quality or usability of a UI. For example,\\nobjects could include a link to an icon (or a base64-encoded version of an icon).\\n• Prototype alpha functionality in Kubernetes (instead of creating a first-class API\\nfield, the parameters for that functionality are encoded in an annotation).\\nAnnotations are used in various places in Kubernetes, with the primary use case\\nbeing rolling deployments. During rolling deployments, annotations are used to track\\nrollout status and provide the necessary information required to roll back a deploy‐\\nment to a previous state.\\nUsers should avoid using the Kubernetes API server as a general-purpose database.\\nAnnotations are good for small bits of data that are highly associated with a specific\\nresource. If you want to store data in Kubernetes but you don’t have an obvious object\\nto associate it with, consider storing that data in some other, more appropriate\\ndatabase.\\nDe\\x80ning Annotations\\nAnnotation keys use the same format as label keys. However, because they are often\\nused to communicate information between tools, the “namespace” part of the key is\\nmore important. Example keys include deployment.kubernetes.io/revision or\\nkubernetes.io/change-cause.\\nThe value component of an annotation is a free-form string field. While this allows\\nmaximum flexibility as users can store arbitrary data, because this is arbitrary text,\\nthere is no validation of any format. For example, it is not uncommon for a JSON\\ndocument to be encoded as a string and stored in an annotation. It is important to\\n72 | Chapter 6: Labels and Annotations\\nnote that the Kubernetes server has no knowledge of the required format of annota‐\\ntion values. If annotations are used to pass or store data, there is no guarantee the\\ndata is valid. This can make tracking down errors more difficult.\\nAnnotations are defined in the common metadata section in every Kubernetes\\nobject:\\n...\\nmetadata:\\n annotations:\\n example.com/icon-url: \"https://example.com/icon.png\"\\n...\\nAnnotations are very convenient and provide powerful loose cou‐\\npling. However, they should be used judiciously to avoid an unty‐\\nped mess of data.\\nCleanup\\nIt is easy to clean up all of the deployments that we started in this chapter:\\n$ kubectl delete deployments --all\\nIf you want to be more selective, you can use the --selector flag to choose which\\ndeployments to delete.\\nSummary\\nLabels are used to identify and optionally group objects in a Kubernetes cluster.\\nLabels are also used in selector queries to provide flexible runtime grouping of objects\\nsuch as Pods.\\nAnnotations provide object-scoped key/value storage of metadata that can be used by\\nautomation tooling and client libraries. Annotations can also be used to hold configu‐\\nration data for external tools such as third-party schedulers and monitoring tools.\\nLabels and annotations are vital to understanding how key components in a Kuber‐\\nnetes cluster work together to ensure the desired cluster state. Using labels and anno‐\\ntations properly unlocks the true power of Kubernetes’s flexibility and provides the\\nstarting point for building automation tools and deployment workflows.\\nCleanup | 73\\n\\nCHAPTER 7\\nService Discovery\\nKubernetes is a very dynamic system. The system is involved in placing Pods on\\nnodes, making sure they are up and running, and rescheduling them as needed.\\nThere are ways to automatically change the number of Pods based on load (such as\\nhorizontal Pod autoscaling [see “Autoscaling a ReplicaSet” on page 110]). The APIdriven nature of the system encourages others to create higher and higher levels of\\nautomation.\\nWhile the dynamic nature of Kubernetes makes it easy to run a lot of things, it creates\\nproblems when it comes to finding those things. Most of the traditional network\\ninfrastructure wasn’t built for the level of dynamism that Kubernetes presents.\\nWhat Is Service Discovery?\\nThe general name for this class of problems and solutions is service discovery. Servicediscovery tools help solve the problem of finding which processes are listening at\\nwhich addresses for which services. A good service-discovery system will enable users\\nto resolve this information quickly and reliably. A good system is also low-latency;\\nclients are updated soon after the information associated with a service changes.\\nFinally, a good service-discovery system can store a richer definition of what that ser‐\\nvice is. For example, perhaps there are multiple ports associated with the service.\\nThe Domain Name System (DNS) is the traditional system of service discovery on\\nthe internet. DNS is designed for relatively stable name resolution with wide and effi‐\\ncient caching. It is a great system for the internet but falls short in the dynamic world\\nof Kubernetes.\\nUnfortunately, many systems (for example, Java, by default) look up a name in DNS\\ndirectly and never re-resolve. This can lead to clients caching stale mappings and\\ntalking to the wrong IP. Even with short TTLs and well-behaved clients, there is a\\n75\\nnatural delay between when a name resolution changes and when the client notices.\\nThere are natural limits to the amount and type of information that can be returned\\nin a typical DNS query, too. Things start to break past 20–30 A records for a single\\nname. SRV records solve some problems, but are often very hard to use. Finally, the\\nway that clients handle multiple IPs in a DNS record is usually to take the first IP\\naddress and rely on the DNS server to randomize or round-robin the order of\\nrecords. This is no substitute for more purpose-built load balancing.\\nThe Service Object\\nReal service discovery in Kubernetes starts with a Service object.\\nA Service object is a way to create a named label selector. As we will see, the Service\\nobject does some other nice things for us, too.\\nJust as the kubectl run command is an easy way to create a Kubernetes deployment,\\nwe can use kubectl expose to create a service. Let’s create some deployments and\\nservices so we can see how they work:\\n$ kubectl run alpaca-prod \\\\\\n --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\\\n --replicas=3 \\\\\\n --port=8080 \\\\\\n --labels=\"ver=1,app=alpaca,env=prod\"\\n$ kubectl expose deployment alpaca-prod\\n$ kubectl run bandicoot-prod \\\\\\n --image=gcr.io/kuar-demo/kuard-amd64:green \\\\\\n --replicas=2 \\\\\\n --port=8080 \\\\\\n --labels=\"ver=2,app=bandicoot,env=prod\"\\n$ kubectl expose deployment bandicoot-prod\\n$ kubectl get services -o wide\\nNAME CLUSTER-IP ... PORT(S) ... SELECTOR\\nalpaca-prod 10.115.245.13 ... 8080/TCP ... app=alpaca,env=prod,ver=1\\nbandicoot-prod 10.115.242.3 ... 8080/TCP ... app=bandicoot,env=prod,ver=2\\nkubernetes 10.115.240.1 ... 443/TCP ... <none>\\nAfter running these commands, we have three services. The ones we just created are\\nalpaca-prod and bandicoot-prod. The kubernetes service is automatically created\\nfor you so that you can find and talk to the Kubernetes API from within the app.\\nIf we look at the SELECTOR column, we see that the alpaca-prod service simply gives a\\nname to a selector and specifies which ports to talk to for that service. The kubectl\\nexpose command will conveniently pull both the label selector and the relevant ports\\n(8080, in this case) from the deployment definition.\\n76 | Chapter 7: Service Discovery\\nFurthermore, that service is assigned a new type of virtual IP called a cluster IP. This\\nis a special IP address the system will load-balance across all of the Pods that are iden‐\\ntified by the selector.\\nTo interact with services, we are going to port forward to one of the alpaca Pods.\\nStart and leave this command running in a terminal window. You can see the port\\nforward working by accessing the alpaca Pod at http://localhost:48858:\\n$ ALPACA_POD=$(kubectl get pods -l app=alpaca \\\\\\n -o jsonpath=\\'{.items[0].metadata.name}\\')\\n$ kubectl port-forward $ALPACA_POD 48858:8080\\nService DNS\\nBecause the cluster IP is virtual, it is stable, and it is appropriate to give it a DNS\\naddress. All of the issues around clients caching DNS results no longer apply. Within\\na namespace, it is as easy as just using the service name to connect to one of the Pods\\nidentified by a service.\\nKubernetes provides a DNS service exposed to Pods running in the cluster. This\\nKubernetes DNS service was installed as a system component when the cluster was\\nfirst created. The DNS service is, itself, managed by Kubernetes and is a great exam‐\\nple of Kubernetes building on Kubernetes. The Kubernetes DNS service provides\\nDNS names for cluster IPs.\\nYou can try this out by expanding the “DNS Query” section on the kuard server sta‐\\ntus page. Query the A record for alpaca-prod. The output should look something\\nlike this:\\n;; opcode: QUERY, status: NOERROR, id: 12071\\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0\\n;; QUESTION SECTION:\\n;alpaca-prod.default.svc.cluster.local. IN A\\n;; ANSWER SECTION:\\nalpaca-prod.default.svc.cluster.local. 30 IN A 10.115.245.13\\nThe full DNS name here is alpaca-prod.default.svc.cluster.local.. Let’s break\\nthis down:\\nalpaca-prod\\nThe name of the service in question.\\ndefault\\nThe namespace that this service is in.\\nThe Service Object | 77\\nsvc\\nRecognizing that this is a service. This allows Kubernetes to expose other types of\\nthings as DNS in the future.\\ncluster.local.\\nThe base domain name for the cluster. This is the default and what you will see\\nfor most clusters. Administrators may change this to allow unique DNS names\\nacross multiple clusters.\\nWhen referring to a service in your own namespace you can just use the service name\\n(alpaca-prod). You can also refer to a service in another namespace with alpacaprod.default. And, of course, you can use the fully qualified service name (alpacaprod.default.svc.cluster.local.). Try each of these out in the “DNS Query”\\nsection of kuard.\\nReadiness Checks\\nOften, when an application first starts up it isn’t ready to handle requests. There is\\nusually some amount of initialization that can take anywhere from under a second to\\nseveral minutes. One nice thing the Service object does is track which of your Pods\\nare ready via a readiness check. Let’s modify our deployment to add a readiness check\\nthat is attached to a Pod, as we discussed in Chapter 5:\\n$ kubectl edit deployment/alpaca-prod\\nThis command will fetch the current version of the alpaca-prod deployment and\\nbring it up in an editor. After you save and quit your editor, it’ll then write the object\\nback to Kubernetes. This is a quick way to edit an object without saving it to a YAML\\nfile.\\nAdd the following section:\\nspec:\\n ...\\n template:\\n ...\\n spec:\\n containers:\\n ...\\n name: alpaca-prod\\n readinessProbe:\\n httpGet:\\n path: /ready\\n port: 8080\\n periodSeconds: 2\\n initialDelaySeconds: 0\\n failureThreshold: 3\\n successThreshold: 1\\n78 | Chapter 7: Service Discovery\\nThis sets up the Pods this deployment will create so that they will be checked for\\nreadiness via an HTTP GET to /ready on port 8080. This check is done every 2 sec‐\\nonds starting as soon as the Pod comes up. If three successive checks fail, then the\\nPod will be considered not ready. However, if only one check succeeds, the Pod will\\nagain be considered ready.\\nOnly ready Pods are sent traffic.\\nUpdating the deployment definition like this will delete and recreate the alpaca Pods.\\nAs such, we need to restart our port-forward command from earlier:\\n$ ALPACA_POD=$(kubectl get pods -l app=alpaca \\\\\\n -o jsonpath=\\'{.items[0].metadata.name}\\')\\n$ kubectl port-forward $ALPACA_POD 48858:8080\\nPoint your browser to http://localhost:48858 and you should see the debug page for\\nthat instance of kuard. Expand the “Readiness Probe” section. You should see this\\npage update every time there is a new readiness check from the system, which should\\nhappen every 2 seconds.\\nIn another terminal window, start a watch command on the endpoints for the\\nalpaca-prod service. Endpoints are a lower-level way of finding what a service is\\nsending traffic to and are covered later in this chapter. The --watch option here\\ncauses the kubectl command to hang around and output any updates. This is an easy\\nway to see how a Kubernetes object changes over time:\\n$ kubectl get endpoints alpaca-prod --watch\\nNow go back to your browser and hit the “Fail” link for the readiness check. You\\nshould see that the server is now returning 500s. After three of these, this server is\\nremoved from the list of endpoints for the service. Hit the “Succeed” link and notice\\nthat after a single readiness check the endpoint is added back.\\nThis readiness check is a way for an overloaded or sick server to signal to the system\\nthat it doesn’t want to receive traffic anymore. This is a great way to implement grace‐\\nful shutdown. The server can signal that it no longer wants traffic, wait until existing\\nconnections are closed, and then cleanly exit.\\nPress Ctrl-C to exit out of both the port-forward and watch commands in your\\nterminals.\\nLooking Beyond the Cluster\\nSo far, everything we’ve covered in this chapter has been about exposing services\\ninside of a cluster. Oftentimes, the IPs for Pods are only reachable from within the\\ncluster. At some point, we have to allow new traffic in!\\nLooking Beyond the Cluster | 79\\nThe most portable way to do this is to use a feature called NodePorts, which enhance\\na service even further. In addition to a cluster IP, the system picks a port (or the user\\ncan specify one), and every node in the cluster then forwards traffic to that port to\\nthe service.\\nWith this feature, if you can reach any node in the cluster you can contact a service.\\nYou use the NodePort without knowing where any of the Pods for that service are\\nrunning. This can be integrated with hardware or software load balancers to expose\\nthe service further.\\nTry this out by modifying the alpaca-prod service:\\n$ kubectl edit service alpaca-prod\\nChange the spec.type field to NodePort. You can also do this when creating the ser‐\\nvice via kubectl expose by specifying --type=NodePort. The system will assign a\\nnew NodePort:\\n$ kubectl describe service alpaca-prod\\nName: alpaca-prod\\nNamespace: default\\nLabels: app=alpaca\\n env=prod\\n ver=1\\nAnnotations: <none>\\nSelector: app=alpaca,env=prod,ver=1\\nType: NodePort\\nIP: 10.115.245.13\\nPort: <unset> 8080/TCP\\nNodePort: <unset> 32711/TCP\\nEndpoints: 10.112.1.66:8080,10.112.2.104:8080,10.112.2.105:8080\\nSession Affinity: None\\nNo events.\\nHere we see that the system assigned port 32711 to this service. Now we can hit any\\nof our cluster nodes on that port to access the service. If you are sitting on the same\\nnetwork, you can access it directly. If your cluster is in the cloud someplace, you can\\nuse SSH tunneling with something like this:\\n$ ssh <node> -L 8080:localhost:32711\\nNow if you point your browser to http://localhost:8080 you will be connected to that\\nservice. Each request that you send to the service will be randomly directed to one of\\nthe Pods that implements the service. Reload the page a few times and you will see\\nthat you are randomly assigned to different Pods.\\nWhen you are done, exit out of the SSH session.\\n80 | Chapter 7: Service Discovery\\nCloud Integration\\nFinally, if you have support from the cloud that you are running on (and your cluster\\nis configured to take advantage of it), you can use the LoadBalancer type. This builds\\non the NodePort type by additionally configuring the cloud to create a new load bal‐\\nancer and direct it at nodes in your cluster.\\nEdit the alpaca-prod service again (kubectl edit service alpaca-prod) and\\nchange spec.type to LoadBalancer.\\nIf you do a kubectl get services right away you’ll see that the EXTERNAL-IP col‐\\numn for alpaca-prod now says <pending>. Wait a bit and you should see a public\\naddress assigned by your cloud. You can look in the console for your cloud account\\nand see the configuration work that Kubernetes did for you:\\n$ kubectl describe service alpaca-prod\\nName: alpaca-prod\\nNamespace: default\\nLabels: app=alpaca\\n env=prod\\n ver=1\\nSelector: app=alpaca,env=prod,ver=1\\nType: LoadBalancer\\nIP: 10.115.245.13\\nLoadBalancer Ingress: 104.196.248.204\\nPort: <unset> 8080/TCP\\nNodePort: <unset> 32711/TCP\\nEndpoints: 10.112.1.66:8080,10.112.2.104:8080,10.112.2.105:8080\\nSession Affinity: None\\nEvents:\\n FirstSeen ... Reason Message\\n --------- ... ------ -------\\n 3m ... Type NodePort -> LoadBalancer\\n 3m ... CreatingLoadBalancer Creating load balancer\\n 2m ... CreatedLoadBalancer Created load balancer\\nHere we see that we have an address of 104.196.248.204 now assigned to the alpacaprod service. Open up your browser and try!\\nThis example is from a cluster launched and managed on the Goo‐\\ngle Cloud Platform via GKE. However, the way a load balancer is\\nconfigured is specific to a cloud. In addition, some clouds have\\nDNS-based load balancers (e.g., AWS ELB). In this case you’ll see a\\nhostname here instead of an IP. Also, depending on the cloud pro‐\\nvider, it may still take a little while for the load balancer to be fully\\noperational.\\nCloud Integration | 81\\nCreating a cloud-based load balancer can take some time. Don’t be surprised if it\\ntakes a few minutes on most cloud providers.\\nAdvanced Details\\nKubernetes is built to be an extensible system. As such, there are layers that allow for\\nmore advanced integrations. Understanding the details of how a sophisticated con‐\\ncept like services is implemented may help you troubleshoot or create more advanced\\nintegrations. This section goes a bit below the surface.\\nEndpoints\\nSome applications (and the system itself) want to be able to use services without\\nusing a cluster IP. This is done with another type of object called an Endpoints object.\\nFor every Service object, Kubernetes creates a buddy Endpoints object that contains\\nthe IP addresses for that service:\\n$ kubectl describe endpoints alpaca-prod\\nName: alpaca-prod\\nNamespace: default\\nLabels: app=alpaca\\n env=prod\\n ver=1\\nSubsets:\\n Addresses: 10.112.1.54,10.112.2.84,10.112.2.85\\n NotReadyAddresses: <none>\\n Ports:\\n Name Port Protocol\\n ---- ---- --------\\n <unset> 8080 TCP\\nNo events.\\nTo use a service, an advanced application can talk to the Kubernetes API directly to\\nlook up endpoints and call them. The Kubernetes API even has the capability to\\n“watch” objects and be notified as soon as they change. In this way, a client can react\\nimmediately as soon as the IPs associated with a service change.\\nLet’s demonstrate this. In a terminal window, start the following command and leave\\nit running:\\n$ kubectl get endpoints alpaca-prod --watch\\nIt will output the current state of the endpoint and then “hang”:\\nNAME ENDPOINTS AGE\\nalpaca-prod 10.112.1.54:8080,10.112.2.84:8080,10.112.2.85:8080 1m\\n82 | Chapter 7: Service Discovery\\nNow open up another terminal window and delete and recreate the deployment back‐\\ning alpaca-prod:\\n$ kubectl delete deployment alpaca-prod\\n$ kubectl run alpaca-prod \\\\\\n --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\\\n --replicas=3 \\\\\\n --port=8080 \\\\\\n --labels=\"ver=1,app=alpaca,env=prod\"\\nIf you look back at the output from the watched endpoint, you will see that as you\\ndeleted and re-created these Pods, the output of the command reflected the most upto-date set of IP addresses associated with the service. Your output will look some‐\\nthing like this:\\nNAME ENDPOINTS AGE\\nalpaca-prod 10.112.1.54:8080,10.112.2.84:8080,10.112.2.85:8080 1m\\nalpaca-prod 10.112.1.54:8080,10.112.2.84:8080 1m\\nalpaca-prod <none> 1m\\nalpaca-prod 10.112.2.90:8080 1m\\nalpaca-prod 10.112.1.57:8080,10.112.2.90:8080 1m\\nalpaca-prod 10.112.0.28:8080,10.112.1.57:8080,10.112.2.90:8080 1m\\nThe Endpoints object is great if you are writing new code that is built to run on\\nKubernetes from the start. But most projects aren’t in this position! Most existing sys‐\\ntems are built to work with regular old IP addresses that don’t change that often.\\nManual Service Discovery\\nKubernetes services are built on top of label selectors over Pods. That means that you\\ncan use the Kubernetes API to do rudimentary service discovery without using a Ser‐\\nvice object at all! Let’s demonstrate.\\nWith kubectl (and via the API) we can easily see what IPs are assigned to each Pod in\\nour example deployments:\\n$ kubectl get pods -o wide --show-labels\\nNAME ... IP ... LABELS\\nalpaca-prod-12334-87f8h ... 10.112.1.54 ... app=alpaca,env=prod,ver=1\\nalpaca-prod-12334-jssmh ... 10.112.2.84 ... app=alpaca,env=prod,ver=1\\nalpaca-prod-12334-tjp56 ... 10.112.2.85 ... app=alpaca,env=prod,ver=1\\nbandicoot-prod-5678-sbxzl ... 10.112.1.55 ... app=bandicoot,env=prod,ver=2\\nbandicoot-prod-5678-x0dh8 ... 10.112.2.86 ... app=bandicoot,env=prod,ver=2\\nThis is great, but what if you have a ton of Pods? You’ll probably want to filter this\\nbased on the labels applied as part of the deployment. Let’s do that for just the alpaca\\napp:\\nAdvanced Details | 83\\n$ kubectl get pods -o wide --selector=app=alpaca,env=prod\\nNAME ... IP ...\\nalpaca-prod-3408831585-bpzdz ... 10.112.1.54 ...\\nalpaca-prod-3408831585-kncwt ... 10.112.2.84 ...\\nalpaca-prod-3408831585-l9fsq ... 10.112.2.85 ...\\nAt this point you have the basics of service discovery! You can always use labels to\\nidentify the set of Pods you are interested in, get all of the Pods for those labels, and\\ndig out the IP address. But keeping the correct set of labels to use in sync can be\\ntricky. This is why the Service object was created.\\nkube-proxy and Cluster IPs\\nCluster IPs are stable virtual IPs that load-balance traffic across all of the endpoints in\\na service. This magic is performed by a component running on every node in the\\ncluster called the kube-proxy (Figure 7-1).\\nFigure 7-1. Configuring and using a cluster IP\\nIn Figure 7-1, the kube-proxy watches for new services in the cluster via the API\\nserver. It then programs a set of iptables rules in the kernel of that host to rewrite\\nthe destinations of packets so they are directed at one of the endpoints for that ser‐\\nvice. If the set of endpoints for a service changes (due to Pods coming and going or\\ndue to a failed readiness check), the set of iptables rules is rewritten.\\nThe cluster IP itself is usually assigned by the API server as the service is created.\\nHowever, when creating the service, the user can specify a specific cluster IP. Once\\nset, the cluster IP cannot be modified without deleting and recreating the Service\\nobject.\\n84 | Chapter 7: Service Discovery\\nThe Kubernetes service address range is configured using the\\n--service-cluster-ip-range flag on the kube-apiserver\\nbinary. The service address range should not overlap with the IP\\nsubnets and ranges assigned to each Docker bridge or Kubernetes\\nnode.\\nIn addition, any explicit cluster IP requested must come from that\\nrange and not already be in use.\\nCluster IP Environment Variables\\nWhile most users should be using the DNS services to find cluster IPs, there are some\\nolder mechanisms that may still be in use. One of these is injecting a set of environ‐\\nment variables into Pods as they start up.\\nTo see this in action, let’s look at the console for the bandicoot instance of kuard.\\nEnter the following commands in your terminal:\\n$ BANDICOOT_POD=$(kubectl get pods -l app=bandicoot \\\\\\n -o jsonpath=\\'{.items[0].metadata.name}\\')\\n$ kubectl port-forward $BANDICOOT_POD 48858:8080\\nNow point your browser to http://localhost:48858 to see the status page for this server.\\nExpand the “Server Env” section and note the set of environment variables for the\\nalpaca service. The status page should show a table similar to Table 7-1.\\nTable 7-1. Service environment variables\\nKey Value\\nALPACA_PROD_PORT tcp://10.115.245.13:8080\\nALPACA_PROD_PORT_8080_TCP tcp://10.115.245.13:8080\\nALPACA_PROD_PORT_8080_TCP_ADDR 10.115.245.13\\nALPACA_PROD_PORT_8080_TCP_PORT 8080\\nALPACA_PROD_PORT_8080_TCP_PROTO tcp\\nALPACA_PROD_SERVICE_HOST 10.115.245.13\\nALPACA_PROD_SERVICE_PORT 8080\\nThe two main environment variables to use are ALPACA_PROD_SERVICE_HOST and\\nALPACA_PROD_SERVICE_PORT. The other environment variables are created to be com‐\\npatible with (now deprecated) Docker link variables.\\nA problem with the environment variable approach is that it requires resources to be\\ncreated in a specific order. The services must be created before the Pods that reference\\nthem. This can introduce quite a bit of complexity when deploying a set of services\\nthat make up a larger application. In addition, using just environment variables seems\\nstrange to many users. For this reason, DNS is probably a better option.\\nAdvanced Details | 85\\nConnecting with Other Environments\\nWhile it is great to have service discovery within your own cluster, many real-world\\napplications actually require that you integrate more cloud-native applications\\ndeployed in Kubernetes with applications deployed to more legacy environments.\\nAdditionally, you may need to integrate a Kubernetes cluster in the cloud with infra‐\\nstructure that has been deployed on-premise.\\nThis is an area of Kubernetes that is still undergoing a fair amount of exploration and\\ndevelopment of solutions. When you are connecting Kubernetes to legacy resources\\noutside of the cluster, you can use selector-less services to declare a Kubernetes ser‐\\nvice with a manually assigned IP address that is outside of the cluster. That way,\\nKubernetes service discovery via DNS works as expected, but the network traffic itself\\nflows to an external resource.\\nConnecting external resources to Kubernetes services is somewhat trickier. If your\\ncloud provider supports it, the easiest thing to do is to create an “internal” load bal‐\\nancer that lives in your virtual private network and can deliver traffic from a fixed IP\\naddress into the cluster. You can then use traditional DNS to make this IP address\\navailable to the external resource. Another option is to run the full kube-proxy on an\\nexternal resource and program that machine to use the DNS server in the Kubernetes\\ncluster. Such a setup is significantly more difficult to get right and should really only\\nbe used in on-premise environments. There are also a variety of open source projects\\n(for example, HashiCorp’s Consul) that can be used to manage connectivity between\\nin-cluster and out-of-cluster resources.\\nCleanup\\nRun the following command to clean up all of the objects created in this chapter:\\n$ kubectl delete services,deployments -l app\\nSummary\\nKubernetes is a dynamic system that challenges traditional methods of naming and\\nconnecting services over the network. The Service object provides a flexible and pow‐\\nerful way to expose services both within the cluster and beyond. With the techniques\\ncovered here you can connect services to each other and expose them outside the\\ncluster.\\n86 | Chapter 7: Service Discovery\\nWhile using the dynamic service discovery mechanisms in Kubernetes introduces\\nsome new concepts and may, at first, seem complex, understanding and adapting\\nthese techniques is key to unlocking the power of Kubernetes. Once your application\\ncan dynamically find services and react to the dynamic placement of those applica‐\\ntions, you are free to stop worrying about where things are running and when they\\nmove. It is a critical piece of the puzzle to start to think about services in a logical way\\nand let Kubernetes take care of the details of container placement.\\nSummary | 87\\n\\n1 The Open Systems Interconnection (OSI) model is a standard way to describe how different networking lay‐\\ners build on each other. TCP and UDP are considered to be Layer 4, while HTTP is Layer 7.\\nCHAPTER 8\\nHTTP Load Balancing with Ingress\\nA critical part of any application is getting network traffic to and from that applica‐\\ntion. As described in Chapter 7, Kubernetes has a set of capabilities to enable services\\nto be exposed outside of the cluster. For many users and simple use cases these capa‐\\nbilities are sufficient.\\nBut the Service object operates at Layer 4 (according to the OSI model1\\n). This means\\nthat it only forwards TCP and UDP connections and doesn’t look inside of those con‐\\nnections. Because of this, hosting many applications on a cluster uses many different\\nexposed services. In the case where these services are type: NodePort, you’ll have to\\nhave clients connect to a unique port per service. In the case where these services are\\ntype: LoadBalancer, you’ll be allocating (often expensive or scarce) cloud resources\\nfor each service. But for HTTP (Layer 7)-based services, we can do better.\\nWhen solving a similar problem in non-Kubernetes situations, users often turn to the\\nidea of “virtual hosting.” This is a mechanism to host many HTTP sites on a single IP\\naddress. Typically, the user uses a load balancer or reverse proxy to accept incoming\\nconnections on HTTP (80) and HTTPS (443) ports. That program then parses the\\nHTTP connection and, based on the Host header and the URL path that is requested,\\nproxies the HTTP call to some other program. In this way, that load balancer or\\nreverse proxy plays “traffic cop” for decoding and directing incoming connections to\\nthe right “upstream” server.\\nKubernetes calls its HTTP-based load-balancing system Ingress. Ingress is a\\nKubernetes-native way to implement the “virtual hosting” pattern we just discussed.\\nOne of the more complex aspects of the pattern is that the user has to manage the\\n89\\nload balancer configuration file. In a dynamic environment and as the set of virtual\\nhosts expands, this can be very complex. The Kubernetes Ingress system works to\\nsimplify this by (a) standardizing that configuration, (b) moving it to a standard\\nKubernetes object, and (c) merging multiple Ingress objects into a single config for\\nthe load balancer.\\nThe typical software base implementation looks something like what is depicted in\\nFigure 8-1. The Ingress controller is a software system exposed outside the cluster\\nusing a service of type: LoadBalancer. It then proxies requests to “upstream”\\nservers. The configuration for how it does this is the result of reading and monitoring\\nIngress objects.\\nFigure 8-1. The typical software Ingress controller configuration\\nIngress Spec Versus Ingress Controllers\\nWhile conceptually simple, at an implementation level Ingress is very different from\\npretty much every other regular resource object in Kubernetes. Specifically, it is split\\ninto a common resource specification and a controller implementation. There is no\\n“standard” Ingress controller that is built into Kubernetes, so the user must install one\\nof many optional implementations.\\nUsers can create and modify Ingress objects just like every other object. But, by\\ndefault, there is no code running to actually act on those objects. It is up to the users\\n(or the distribution they are using) to install and manage an outside controller. In this\\nway, the controller is pluggable.\\nThere are multiple reasons that Ingress ended up like this. First of all, there is no one\\nsingle HTTP load balancer that can universally be used. In addition to many software\\nload balancers (both open source and proprietary), there are also load-balancing\\ncapabilities provided by cloud providers (e.g., ELB on AWS), and hardware-based\\nload balancers. The second reason is that the Ingress object was added to Kubernetes\\nbefore any of the common extensibility capabilities were added (see Chapter 16). As\\nIngress progresses, it is likely that it will evolve to use these mechanisms.\\n90 | Chapter 8: HTTP Load Balancing with Ingress\\n2 Heptio was recently acquired by VMware, so there is a chance that this URL could change. However, GitHub\\nwill forward to the new destination.\\nInstalling Contour\\nWhile there are many available Ingress controllers, for the examples here we use an\\nIngress controller called Contour. This is a controller built to configure the open\\nsource (and CNCF project) load balancer called Envoy. Envoy is built to be dynami‐\\ncally configured via an API. The Contour Ingress controller takes care of translating\\nthe Ingress objects into something that Envoy can understand.\\nThe Contour project is hosted at https://github.com/heptio/contour.\\nIt was created by Heptio2\\n in collaboration with real-world custom‐\\ners and is used in production settings.\\nYou can install Contour with a simple one-line invocation:\\n$ kubectl apply -f https://j.hept.io/contour-deployment-rbac\\nNote that this requires execution by a user who has cluster-admin permissions.\\nThis one line works for most configurations. It creates a namespace called heptiocontour. Inside of that namespace it creates a deployment (with two replicas) and an\\nexternal-facing service of type: LoadBalancer. In addition, it sets up the correct per‐\\nmissions via a service account and installs a CustomResourceDefinition (see Chap‐\\nter 16) for some extended capabilities discussed in “The Future of Ingress” on page\\n100.\\nBecause it is a global install, you need to ensure that you have wide admin permis‐\\nsions on the cluster you are installing into.\\nAfter you install it, you can fetch the external address of Contour via:\\n$ kubectl get -n heptio-contour service contour -o wide\\nNAME CLUSTER-IP EXTERNAL-IP PORT(S) ...\\ncontour 10.106.53.14 a477...amazonaws.com 80:30274/TCP ...\\nLook at the EXTERNAL-IP column. This can be either an IP address (for GCP and\\nAzure) or a hostname (for AWS). Other clouds and environments may differ. If your\\nKubernetes cluster doesn’t support services of type: LoadBalancer, you’ll have to\\nchange the YAML for installing Contour to simply use type: NodePort and route\\ntraffic to machines on the cluster via a mechanism that works in your configuration.\\nIf you are using minikube, you probably won’t have anything listed for EXTERNAL-IP.\\nTo fix this, you need to open a separate terminal window and run minikube tunnel.\\nInstalling Contour | 91\\nThis configures networking routes such that you have unique IP addresses assigned\\nto every service of type: LoadBalancer.\\nCon\\x80guring DNS\\nTo make Ingress work well, you need to configure DNS entries to the external address\\nfor your load balancer. You can map multiple hostnames to a single external endpoint\\nand the Ingress controller will play traffic cop and direct incoming requests to the\\nappropriate upstream service based on that hostname.\\nFor this chapter, we assume that you have a domain called example.com. You need to\\nconfigure two DNS entries: alpaca.example.com and bandicoot.example.com. If\\nyou have an IP address for your external load balancer, you’ll want to create A\\nrecords. If you have a hostname, you’ll want to configure CNAME records.\\nCon\\x80guring a Local hosts File\\nIf you don’t have a domain or if you are using a local solution such as minikube, you\\ncan set up a local configuration by editing your /etc/hosts file to add an IP address.\\nYou need admin/root privileges on your workstation. The location of the file may dif‐\\nfer on your platform, and making it take effect may require extra steps. For example,\\non Windows the file is usually at C:\\\\Windows\\\\System32\\\\drivers\\\\etc\\\\hosts, and for\\nrecent versions of macOS you need to run sudo killall -HUP mDNSResponder after\\nchanging the file.\\nEdit the file to add a line like the following:\\n<ip-address> alpaca.example.com bandicoot.example.com\\nFor <ip-address>, fill in the external IP address for Contour. If all you have is a host‐\\nname (like from AWS), you can get an IP address (that may change in the future) by\\nexecuting host -t a <address>.\\nDon’t forget to undo these changes when you are done!\\nUsing Ingress\\nNow that we have an Ingress controller configured, let’s put it through its paces. First\\nwe’ll create a few upstream (also sometimes referred to as “backend”) services to play\\nwith by executing the following commands:\\n$ kubectl run be-default \\\\\\n --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\\\n --replicas=3 \\\\\\n --port=8080\\n$ kubectl expose deployment be-default\\n$ kubectl run alpaca \\\\\\n92 | Chapter 8: HTTP Load Balancing with Ingress\\n --image=gcr.io/kuar-demo/kuard-amd64:green \\\\\\n --replicas=3 \\\\\\n --port=8080\\n$ kubectl expose deployment alpaca\\n$ kubectl run bandicoot \\\\\\n --image=gcr.io/kuar-demo/kuard-amd64:purple \\\\\\n --replicas=3 \\\\\\n --port=8080\\n$ kubectl expose deployment bandicoot\\n$ kubectl get services -o wide\\nNAME CLUSTER-IP ... PORT(S) ... SELECTOR\\nalpaca-prod 10.115.245.13 ... 8080/TCP ... run=alpaca\\nbandicoot-prod 10.115.242.3 ... 8080/TCP ... run=bandicoot\\nbe-default 10.115.246.6 ... 8080/TCP ... run=be-default\\nkubernetes 10.115.240.1 ... 443/TCP ... <none>\\nSimplest Usage\\nThe simplest way to use Ingress is to have it just blindly pass everything that it sees\\nthrough to an upstream service. There is limited support for imperative commands to\\nwork with Ingress in kubectl, so we’ll start with a YAML file (see Example 8-1).\\nExample 8-1. simple-ingress.yaml\\napiVersion: extensions/v1beta1\\nkind: Ingress\\nmetadata:\\n name: simple-ingress\\nspec:\\n backend:\\n serviceName: alpaca\\n servicePort: 8080\\nCreate this Ingress with kubectl apply:\\n$ kubectl apply -f simple-ingress.yaml\\ningress.extensions/simple-ingress created\\nYou can verify that it was set up correctly using kubectl get and kubectl describe:\\n$ kubectl get ingress\\nNAME HOSTS ADDRESS PORTS AGE\\nsimple-ingress * 80 13m\\n$ kubectl describe ingress simple-ingress\\nName: simple-ingress\\nNamespace: default\\nAddress:\\nDefault backend: be-default:8080\\n(172.17.0.6:8080,172.17.0.7:8080,172.17.0.8:8080)\\nUsing Ingress | 93\\nRules:\\n Host Path Backends\\n ---- ---- --------\\n * * be-default:8080 (172.17.0.6:8080,172.17.0.7:8080,172.17.0.8:8080)\\nAnnotations:\\n ...\\nEvents: <none>\\nThis sets things up so that any HTTP request that hits the Ingress controller is for‐\\nwarded on to the alpaca service. You can now access the alpaca instance of kuard on\\nany of the raw IPs/CNAMEs of the service; in this case, either alpaca.example.com\\nor bandicoot.example.com.\\nThis doesn’t, at this point, add much value above a simple service of type: LoadBa\\nlancer. We experiment with more complex configurations in the following sections.\\nUsing Hostnames\\nThings start to get interesting when we start to direct traffic based on properties of\\nthe request. The most common example of this is to have the Ingress system look at\\nthe HTTP host header (which is set to the DNS domain in the original URL) and\\ndirect traffic based on that header. Let’s add another Ingress object for directing traffic\\nto the alpaca service for any traffic directed to alpaca.example.com (see\\nExample 8-2).\\nExample 8-2. host-ingress.yaml\\napiVersion: extensions/v1beta1\\nkind: Ingress\\nmetadata:\\n name: host-ingress\\nspec:\\n rules:\\n - host: alpaca.example.com\\n http:\\n paths:\\n - backend:\\n serviceName: alpaca\\n servicePort: 8080\\n94 | Chapter 8: HTTP Load Balancing with Ingress\\nCreate this Ingress with kubectl apply:\\n$ kubectl apply -f host-ingress.yaml\\ningress.extensions/host-ingress created\\nWe can verify that things are set up correctly as follows:\\n$ kubectl get ingress\\nNAME HOSTS ADDRESS PORTS AGE\\nhost-ingress alpaca.example.com 80 54s\\nsimple-ingress * 80 13m\\n$ kubectl describe ingress host-ingress\\nName: host-ingress\\nNamespace: default\\nAddress:\\nDefault backend: default-http-backend:80 (<none>)\\nRules:\\n Host Path Backends\\n ---- ---- --------\\n alpaca.example.com\\n / alpaca:8080 (<none>)\\nAnnotations:\\n ...\\nEvents: <none>\\nThere are a couple of things that are a bit confusing here. First, there is a reference to\\nthe default-http-backend. This is a convention that only some Ingress controllers\\nuse to handle requests that aren’t handled in any other way. These controllers send\\nthose requests to a service called default-http-backend in the kube-system name‐\\nspace. This convention is surfaced client-side in kubectl.\\nNext, there are no endpoints listed for the alpaca backend service. This is a bug in\\nkubectl that is fixed in Kubernetes v1.14.\\nRegardless, you should now be able to address the alpaca service via http://\\nalpaca.example.com. If instead you reach the service endpoint via other methods, you\\nshould get the default service.\\nUsing Paths\\nThe next interesting scenario is to direct traffic based on not just the hostname, but\\nalso the path in the HTTP request. We can do this easily by specifying a path in the\\npaths entry (see Example 8-3). In this example we direct everything coming into\\nhttp://bandicoot.example.com to the bandicoot service, but we also send http://bandi‐\\ncoot.example.com/a to the alpaca service. This type of scenario can be used to host\\nmultiple services on different paths of a single domain.\\nUsing Ingress | 95\\nExample 8-3. path-ingress.yaml\\napiVersion: extensions/v1beta1\\nkind: Ingress\\nmetadata:\\n name: path-ingress\\nspec:\\n rules:\\n - host: bandicoot.example.com\\n http:\\n paths:\\n - path: \"/\"\\n backend:\\n serviceName: bandicoot\\n servicePort: 8080\\n - path: \"/a/\"\\n backend:\\n serviceName: alpaca\\n servicePort: 8080\\nWhen there are multiple paths on the same host listed in the Ingress system, the\\nlongest prefix matches. So, in this example, traffic starting with /a/ is forwarded to\\nthe alpaca service, while all other traffic (starting with /) is directed to the bandicoot\\nservice.\\nAs requests get proxied to the upstream service, the path remains unmodified. That\\nmeans a request to bandicoot.example.com/a/ shows up to the upstream server that\\nis configured for that request hostname and path. The upstream service needs to be\\nready to serve traffic on that subpath. In this case, kuard has special code for testing,\\nwhere it responds on the root path (/) along with a predefined set of subpaths (/\\na/, /b/, and /c/).\\nCleaning Up\\nTo clean up, execute the following:\\n$ kubectl delete ingress host-ingress path-ingress simple-ingress\\n$ kubectl delete service alpaca bandicoot be-default\\n$ kubectl delete deployment alpaca bandicoot be-default\\nAdvanced Ingress Topics and Gotchas\\nThere are some other fancy features that are supported by Ingress. The level of sup‐\\nport for these features differs based on the Ingress controller implementation, and\\ntwo controllers may implement a feature in slightly different ways.\\nMany of the extended features are exposed via annotations on the Ingress object. Be\\ncareful, as these annotations can be hard to validate and are easy to get wrong. Many\\n96 | Chapter 8: HTTP Load Balancing with Ingress\\nof these annotations apply to the entire Ingress object and so can be more general\\nthan you might like. To scope the annotations down you can always split a single\\nIngress object into multiple Ingress objects. The Ingress controller should read them\\nand merge them together.\\nRunning Multiple Ingress Controllers\\nOftentimes, you may want to run multiple Ingress controllers on a single cluster. In\\nthat case, you specify which Ingress object is meant for which Ingress controller using\\nthe kubernetes.io/ingress.class annotation. The value should be a string that\\nspecifies which Ingress controller should look at this object. The Ingress controllers\\nthemselves, then, should be configured with that same string and should only respect\\nthose Ingress objects with the correct annotation.\\nIf the kubernetes.io/ingress.class annotation is missing, behavior is undefined. It\\nis likely that multiple controllers will fight to satisfy the Ingress and write the status\\nfield of the Ingress objects.\\nMultiple Ingress Objects\\nIf you specify multiple Ingress objects, the Ingress controllers should read them all\\nand try to merge them into a coherent configuration. However, if you specify dupli‐\\ncate and conflicting configurations, the behavior is undefined. It is likely that differ‐\\nent Ingress controllers will behave differently. Even a single implementation may do\\ndifferent things depending on nonobvious factors.\\nIngress and Namespaces\\nIngress interacts with namespaces in some nonobvious ways.\\nFirst, due to an abundance of security caution, an Ingress object can only refer to an\\nupstream service in the same namespace. This means that you can’t use an Ingress\\nobject to point a subpath to a service in another namespace.\\nHowever, multiple Ingress objects in different namespaces can specify subpaths for\\nthe same host. These Ingress objects are then merged together to come up with the\\nfinal config for the Ingress controller.\\nThis cross-namespace behavior means that it is necessary that Ingress be coordinated\\nglobally across the cluster. If not coordinated carefully, an Ingress object in one\\nnamespace could cause problems (and undefined behavior) in other namespaces.\\nTypically there are no restrictions built into the Ingress controller around what name‐\\nspaces are allowed to specify what hostnames and paths. Advanced users may try to\\nenforce a policy for this using a custom admission controller. There are also\\nAdvanced Ingress Topics and Gotchas | 97\\nevolutions of Ingress described in “The Future of Ingress” on page 100 that address\\nthis problem.\\nPath Rewriting\\nSome Ingress controller implementations support, optionally, doing path rewriting.\\nThis can be used to modify the path in the HTTP request as it gets proxied. This is\\nusually specified by an annotation on the Ingress object and applies to all requests\\nthat are specified by that object. For example, if we were using the NGINX Ingress\\ncontroller, we could specify an annotation of nginx.ingress.kubernetes.io/\\nrewrite-target: /. This can sometimes make upstream services work on a subpath\\neven if they weren’t built to do so.\\nThere are multiple implementations that not only implement path rewriting, but also\\nsupport regular expressions when specifying the path. For example, the NGINX con‐\\ntroller allows regular expressions to capture parts of the path and then use that cap‐\\ntured content when doing rewriting. How this is done (and what variant of regular\\nexpressions is used) is implementation-specific.\\nPath rewriting isn’t a silver bullet, though, and can often lead to bugs. Many web\\napplications assume that they can link within themselves using absolute paths. In that\\ncase, the app in question may be hosted on /subpath but have requests show up to it\\non /. It may then send a user to /app-path. There is then the question of whether that\\nis an “internal” link for the app (in which case it should instead be /subpath/apppath) or a link to some other app. For this reason, it is probably best to avoid sub‐\\npaths if you can help it for any complicated applications.\\nServing TLS\\nWhen serving websites, it is becoming increasingly necessary to do so securely using\\nTLS and HTTPS. Ingress supports this (as do most Ingress controllers).\\nFirst, users need to specify a secret with their TLS certificate and keys—something\\nlike what is outlined in Example 8-4. You can also create a secret imperatively with\\nkubectl create secret tls <secret-name> --cert <certificate-pem-file> --\\nkey <private-key-pem-file>.\\nExample 8-4. tls-secret.yaml\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n creationTimestamp: null\\n name: tls-secret-name\\ntype: kubernetes.io/tls\\ndata:\\n98 | Chapter 8: HTTP Load Balancing with Ingress\\n tls.crt: <base64 encoded certificate>\\n tls.key: <base64 encoded private key>\\nOnce you have the certificate uploaded, you can reference it in an Ingress object. This\\nspecifies a list of certificates along with the hostnames that those certificates should\\nbe used for (see Example 8-5). Again, if multiple Ingress objects specify certificates\\nfor the same hostname, the behavior is undefined.\\nExample 8-5. tls-ingress.yaml\\napiVersion: extensions/v1beta1\\nkind: Ingress\\nmetadata:\\n name: tls-ingress\\nspec:\\n tls:\\n - hosts:\\n - alpaca.example.com\\n secretName: tls-secret-name\\n rules:\\n - host: alpaca.example.com\\n http:\\n paths:\\n - backend:\\n serviceName: alpaca\\n servicePort: 8080\\nUploading and managing TLS secrets can be difficult. In addition, certificates can\\noften come at a significant cost. To help solve this problem, there is a non-profit\\ncalled “Let’s Encrypt” running a free Certificate Authority that is API-driven. Since it\\nis API-driven, it is possible to set up a Kubernetes cluster that automatically fetches\\nand installs TLS certificates for you. It can be tricky to set up, but when working it’s\\nvery simple to use. The missing piece is an open source project called cert-manager\\ninitiated and supported by Jetstack, a UK startup. Visit its GitHub page for instruc‐\\ntions on installing and using cert-manager.\\nAlternate Ingress Implementations\\nThere are many different implementations of Ingress controllers, each building on\\nthe base Ingress object with unique features. It is a vibrant ecosystem.\\nFirst, each cloud provider has an Ingress implementation that exposes the specific\\ncloud-based L7 load balancer for that cloud. Instead of configuring a software load\\nbalancer running in a Pod, these controllers take Ingress objects and use them to con‐\\nfigure, via an API, the cloud-based load balancers. This reduces the load on the clus‐\\nter and management burden for the operators, but can often come at a cost.\\nAlternate Ingress Implementations | 99\\nThe most popular generic Ingress controller is probably the open source NGINX\\ningress controller. Be aware that there is also a commercial controller based on the\\nproprietary NGINX Plus. The open source controller essentially reads Ingress objects\\nand merges them into an NGINX configuration file. It then signals to the NGINX\\nprocess to restart with the new configuration (while responsibly serving existing inflight connections). The open NGINX controller has an enormous number of fea‐\\ntures and options exposed via annotations.\\nAmbassador and Gloo are two other Envoy-based Ingress controllers that are focused\\non being API gateways.\\nTraefik is a reverse proxy implemented in Go that also can function as an Ingress\\ncontroller. It has a set of features and dashboards that are very developer-friendly.\\nThis just scratches the surface. The Ingress ecosystem is very active and there are\\nmany new projects and commercial offerings that build on the humble Ingress object\\nin unique ways.\\nThe Future of Ingress\\nAs you have seen, the Ingress object provides a very useful abstraction for configur‐\\ning L7 load balancers—but it hasn’t scaled to all the features that users want and vari‐\\nous implementations are looking to offer.\\nMany of the features in Ingress are underdefined. Implementations can surface these\\nfeatures in different ways, reducing the portability of configurations between imple‐\\nmentations.\\nAnother problem is that it is easy to misconfigure Ingress. The way that multiple\\nobjects compose together opens the door for conflicts that are resolved differently by\\ndifferent implementations. In addition, the way that these are merged across name‐\\nspaces breaks the idea of namespace isolation.\\nIngress was also created before the idea of a Service Mesh (exemplified by projects\\nsuch as Istio and Linkerd) was well known. The intersection of Ingress and Service\\nMeshes is still being defined.\\nThere are a lot of great ideas floating around the community. For example, Istio\\nimplements the idea of a gateway that overlaps with Ingress in some ways. Contour\\nintroduces a new type called IngressRoute that is a better-defined and more explicit\\nversion of Ingress inspired by other network protocols like DNS. Defining the next\\nstep for Ingress is a hard problem. It involves creating a common language that\\napplies to most load balancers, while at the same time leaving room for innovation of\\nnew features and allowing different implementations to specialize in other directions.\\nThis is an active area of innovation in the Kubernetes community through the Net‐\\nwork Special Interest Group.\\n100 | Chapter 8: HTTP Load Balancing with Ingress\\nSummary\\nIngress is a unique system in Kubernetes. It is simply a schema, and the implementa‐\\ntions of a controller for that schema must be installed and managed separately. But it\\nis also a critical system for exposing services to users in a practical and cost-efficient\\nway. As Kubernetes continues to mature, expect to see Ingress become more and\\nmore relevant.\\nSummary | 101\\n\\nCHAPTER 9\\nReplicaSets\\nPreviously, we covered how to run individual containers as Pods. But these Pods are\\nessentially one-off singletons. More often than not, you want multiple replicas of a\\ncontainer running at a particular time. There are a variety of reasons for this type of\\nreplication:\\nRedundancy\\nMultiple running instances mean failure can be tolerated.\\nScale\\nMultiple running instances mean that more requests can be handled.\\nSharding\\nDifferent replicas can handle different parts of a computation in parallel.\\nOf course, you could manually create multiple copies of a Pod using multiple differ‐\\nent (though largely similar) Pod manifests, but doing so is both tedious and errorprone. Logically, a user managing a replicated set of Pods considers them as a single\\nentity to be defined and managed. This is precisely what a ReplicaSet is. A ReplicaSet\\nacts as a cluster-wide Pod manager, ensuring that the right types and number of Pods\\nare running at all times.\\nBecause ReplicaSets make it easy to create and manage replicated sets of Pods, they\\nare the building blocks used to describe common application deployment patterns\\nand provide the underpinnings of self-healing for our applications at the infrastruc‐\\nture level. Pods managed by ReplicaSets are automatically rescheduled under certain\\nfailure conditions, such as node failures and network partitions.\\nThe easiest way to think of a ReplicaSet is that it combines a cookie cutter and a\\ndesired number of cookies into a single API object. When we define a ReplicaSet, we\\ndefine a specification for the Pods we want to create (the “cookie cutter”), and a\\n103\\ndesired number of replicas. Additionally, we need to define a way of finding Pods that\\nthe ReplicaSet should control. The actual act of managing the replicated Pods is an\\nexample of a reconciliation loop. Such loops are fundamental to most of the design\\nand implementation of Kubernetes.\\nThe decision to embed the definition of a Pod inside a ReplicaSet\\n(and a deployment, and a job, and…) is one of the more interesting\\nones in Kubernetes. In retrospect, it probably would have been a\\nbetter decision to use a reference to the PodTemplate object rather\\nthan embedding it directly.\\nReconciliation Loops\\nThe central concept behind a reconciliation loop is the notion of desired state versus\\nobserved or current state. Desired state is the state you want. With a ReplicaSet, it is\\nthe desired number of replicas and the definition of the Pod to replicate. For example,\\n“the desired state is that there are three replicas of a Pod running the kuard server.”\\nIn contrast, the current state is the currently observed state of the system. For exam‐\\nple, “there are only two kuard Pods currently running.”\\nThe reconciliation loop is constantly running, observing the current state of the\\nworld and taking action to try to make the observed state match the desired state. For\\ninstance, with the previous examples, the reconciliation loop would create a new\\nkuard Pod in an effort to make the observed state match the desired state of three\\nreplicas.\\nThere are many benefits to the reconciliation loop approach to managing state. It is\\nan inherently goal-driven, self-healing system, yet it can often be easily expressed in a\\nfew lines of code.\\nAs a concrete example of this, note that the reconciliation loop for ReplicaSets is a\\nsingle loop, yet it handles user actions to scale up or scale down the ReplicaSet as well\\nas node failures or nodes rejoining the cluster after being absent.\\nWe’ll see numerous examples of reconciliation loops in action throughout the rest of\\nthe book.\\nRelating Pods and ReplicaSets\\nOne of the key themes that runs through Kubernetes is decoupling. In particular, it’s\\nimportant that all of the core concepts of Kubernetes are modular with respect to\\neach other and that they are swappable and replaceable with other components. In\\nthis spirit, the relationship between ReplicaSets and Pods is loosely coupled. Though\\nReplicaSets create and manage Pods, they do not own the Pods they create. Replica‐\\n104 | Chapter 9: ReplicaSets\\nSets use label queries to identify the set of Pods they should be managing. They then\\nuse the exact same Pod API that you used directly in Chapter 5 to create the Pods that\\nthey are managing. This notion of “coming in the front door” is another central\\ndesign concept in Kubernetes. In a similar decoupling, ReplicaSets that create multi‐\\nple Pods and the services that load-balance to those Pods are also totally separate,\\ndecoupled API objects. In addition to supporting modularity, the decoupling of Pods\\nand ReplicaSets enables several important behaviors, discussed in the following\\nsections.\\nAdopting Existing Containers\\nDespite the value placed on declarative configuration of software, there are times\\nwhen it is easier to build something up imperatively. In particular, early on you may\\nbe simply deploying a single Pod with a container image without a ReplicaSet manag‐\\ning it. But at some point you may want to expand your singleton container into a\\nreplicated service and create and manage an array of similar containers. You may\\nhave even defined a load balancer that is serving traffic to that single Pod. If Replica‐\\nSets owned the Pods they created, then the only way to start replicating your Pod\\nwould be to delete it and then relaunch it via a ReplicaSet. This might be disruptive,\\nas there would be a moment in time when there would be no copies of your container\\nrunning. However, because ReplicaSets are decoupled from the Pods they manage,\\nyou can simply create a ReplicaSet that will “adopt” the existing Pod, and scale out\\nadditional copies of those containers. In this way, you can seamlessly move from a\\nsingle imperative Pod to a replicated set of Pods managed by a ReplicaSet.\\nQuarantining Containers\\nOftentimes, when a server misbehaves, Pod-level health checks will automatically\\nrestart that Pod. But if your health checks are incomplete, a Pod can be misbehaving\\nbut still be part of the replicated set. In these situations, while it would work to simply\\nkill the Pod, that would leave your developers with only logs to debug the problem.\\nInstead, you can modify the set of labels on the sick Pod. Doing so will disassociate it\\nfrom the ReplicaSet (and service) so that you can debug the Pod. The ReplicaSet con‐\\ntroller will notice that a Pod is missing and create a new copy, but because the Pod is\\nstill running it is available to developers for interactive debugging, which is signifi‐\\ncantly more valuable than debugging from logs.\\nDesigning with ReplicaSets\\nReplicaSets are designed to represent a single, scalable microservice inside your\\narchitecture. The key characteristic of ReplicaSets is that every Pod that is created by\\nthe ReplicaSet controller is entirely homogeneous. Typically, these Pods are then\\nfronted by a Kubernetes service load balancer, which spreads traffic across the Pods\\nDesigning with ReplicaSets | 105\\nthat make up the service. Generally speaking, ReplicaSets are designed for stateless\\n(or nearly stateless) services. The elements created by the ReplicaSet are interchange‐\\nable; when a ReplicaSet is scaled down, an arbitrary Pod is selected for deletion. Your\\napplication’s behavior shouldn’t change because of such a scale-down operation.\\nReplicaSet Spec\\nLike all objects in Kubernetes, ReplicaSets are defined using a specification. All\\nReplicaSets must have a unique name (defined using the metadata.name field), a spec\\nsection that describes the number of Pods (replicas) that should be running clusterwide at any given time, and a Pod template that describes the Pod to be created when\\nthe defined number of replicas is not met. Example 9-1 shows a minimal ReplicaSet\\ndefinition.\\nExample 9-1. kuard-rs.yaml\\napiVersion: extensions/v1beta1\\nkind: ReplicaSet\\nmetadata:\\n name: kuard\\nspec:\\n replicas: 1\\n template:\\n metadata:\\n labels:\\n app: kuard\\n version: \"2\"\\n spec:\\n containers:\\n - name: kuard\\n image: \"gcr.io/kuar-demo/kuard-amd64:green\"\\nPod Templates\\nAs mentioned previously, when the number of Pods in the current state is less than\\nthe number of Pods in the desired state, the ReplicaSet controller will create new\\nPods. The Pods are created using a Pod template that is contained in the ReplicaSet\\nspecification. The Pods are created in exactly the same manner as when you created a\\nPod from a YAML file in previous chapters, but instead of using a file, the Kubernetes\\nReplicaSet controller creates and submits a Pod manifest based on the Pod template\\ndirectly to the API server.\\nThe following shows an example of a Pod template in a ReplicaSet:\\ntemplate:\\n metadata:\\n labels:\\n106 | Chapter 9: ReplicaSets\\n app: helloworld\\n version: v1\\n spec:\\n containers:\\n - name: helloworld\\n image: kelseyhightower/helloworld:v1\\n ports:\\n - containerPort: 80\\nLabels\\nIn any cluster of reasonable size, there are many different Pods running at any given\\ntime—so how does the ReplicaSet reconciliation loop discover the set of Pods for a\\nparticular ReplicaSet? ReplicaSets monitor cluster state using a set of Pod labels.\\nLabels are used to filter Pod listings and track Pods running within a cluster. When\\ninitially created, a ReplicaSet fetches a Pod listing from the Kubernetes API and filters\\nthe results by labels. Based on the number of Pods returned by the query, the Replica‐\\nSet deletes or creates Pods to meet the desired number of replicas. The labels used for\\nfiltering are defined in the ReplicaSet spec section and are the key to understanding\\nhow ReplicaSets work.\\nThe selector in the ReplicaSet spec should be a proper subset of the\\nlabels in the Pod template.\\nCreating a ReplicaSet\\nReplicaSets are created by submitting a ReplicaSet object to the Kubernetes API. In\\nthis section we will create a ReplicaSet using a configuration file and the\\nkubectl apply command.\\nThe ReplicaSet configuration file in Example 9-1 will ensure one copy of the gcr.io/\\nkuar-demo/kuard-amd64:green container is running at any given time.\\nUse the kubectl apply command to submit the kuard ReplicaSet to the Kubernetes\\nAPI:\\n$ kubectl apply -f kuard-rs.yaml\\nreplicaset \"kuard\" created\\nOnce the kuard ReplicaSet has been accepted, the ReplicaSet controller will detect\\nthat there are no kuard Pods running that match the desired state, and a new kuard\\nPod will be created based on the contents of the Pod template:\\nCreating a ReplicaSet | 107\\n$ kubectl get pods\\nNAME READY STATUS RESTARTS AGE\\nkuard-yvzgd 1/1 Running 0 11s\\nInspecting a ReplicaSet\\nAs with Pods and other Kubernetes API objects, if you are interested in further details\\nabout a ReplicaSet, the describe command will provide much more information\\nabout its state. Here is an example of using describe to obtain the details of the\\nReplicaSet we previously created:\\n$ kubectl describe rs kuard\\nName: kuard\\nNamespace: default\\nImage(s): kuard:1.9.15\\nSelector: app=kuard,version=2\\nLabels: app=kuard,version=2\\nReplicas: 1 current / 1 desired\\nPods Status: 1 Running / 0 Waiting / 0 Succeeded / 0 Failed\\nNo volumes.\\nYou can see the label selector for the ReplicaSet, as well as the state of all of the repli‐\\ncas managed by the ReplicaSet.\\nFinding a ReplicaSet from a Pod\\nSometimes you may wonder if a Pod is being managed by a ReplicaSet, and if it is,\\nwhich ReplicaSet.\\nTo enable this kind of discovery, the ReplicaSet controller adds an annotation to\\nevery Pod that it creates. The key for the annotation is kubernetes.io/created-by. If\\nyou run the following, look for the kubernetes.io/created-by entry in the annota‐\\ntions section:\\n$ kubectl get pods <pod-name> -o yaml\\nIf applicable, this will list the name of the ReplicaSet that is managing this Pod. Note\\nthat such annotations are best-effort; they are only created when the Pod is created by\\nthe ReplicaSet, and can be removed by a Kubernetes user at any time.\\nFinding a Set of Pods for a ReplicaSet\\nYou can also determine the set of Pods managed by a ReplicaSet. First, you can get the\\nset of labels using the kubectl describe command. In the previous example, the\\nlabel selector was app=kuard,version=2. To find the Pods that match this selector,\\nuse the --selector flag or the shorthand -l:\\n$ kubectl get pods -l app=kuard,version=2\\n108 | Chapter 9: ReplicaSets\\nThis is exactly the same query that the ReplicaSet executes to determine the current\\nnumber of Pods.\\nScaling ReplicaSets\\nReplicaSets are scaled up or down by updating the spec.replicas key on the\\nReplicaSet object stored in Kubernetes. When a ReplicaSet is scaled up, new Pods are\\nsubmitted to the Kubernetes API using the Pod template defined on the ReplicaSet.\\nImperative Scaling with kubectl scale\\nThe easiest way to achieve this is using the scale command in kubectl. For example,\\nto scale up to four replicas you could run:\\n$ kubectl scale replicasets kuard --replicas=4\\nWhile such imperative commands are useful for demonstrations and quick reactions\\nto emergency situations (e.g., in response to a sudden increase in load), it is impor‐\\ntant to also update any text-file configurations to match the number of replicas that\\nyou set via the imperative scale command. The reason for this becomes obvious\\nwhen you consider the following scenario:\\nAlice is on call, when suddenly there is a large increase in load on the service she is\\nmanaging. Alice uses the scale command to increase the number of servers respond‐\\ning to requests to 10, and the situation is resolved. However, Alice forgets to update the\\nReplicaSet configurations checked into source control. Several days later, Bob is pre‐\\nparing the weekly rollouts. Bob edits the ReplicaSet configurations stored in version\\ncontrol to use the new container image, but he doesn’t notice that the number of repli‐\\ncas in the file is currently 5, not the 10 that Alice set in response to the increased load.\\nBob proceeds with the rollout, which both updates the container image and reduces\\nthe number of replicas by half, causing an immediate overload or outage.\\nHopefully, this illustrates the need to ensure that any imperative changes are immedi‐\\nately followed by a declarative change in source control. Indeed, if the need is not\\nacute, we generally recommend only making declarative changes as described in the\\nfollowing section.\\nDeclaratively Scaling with kubectl apply\\nIn a declarative world, we make changes by editing the configuration file in version\\ncontrol and then applying those changes to our cluster. To scale the kuard ReplicaSet,\\nedit the kuard-rs.yaml configuration file and set the replicas count to 3:\\n...\\nspec:\\n replicas: 3\\n...\\nScaling ReplicaSets | 109\\nIn a multiuser setting, you would like to have a documented code review of this\\nchange and eventually check the changes into version control. Either way, you can\\nthen use the kubectl apply command to submit the updated kuard ReplicaSet to the\\nAPI server:\\n$ kubectl apply -f kuard-rs.yaml\\nreplicaset \"kuard\" configured\\nNow that the updated kuard ReplicaSet is in place, the ReplicaSet controller will\\ndetect that the number of desired Pods has changed and that it needs to take action to\\nrealize that desired state. If you used the imperative scale command in the previous\\nsection, the ReplicaSet controller will destroy one Pod to get the number to three.\\nOtherwise, it will submit two new Pods to the Kubernetes API using the Pod template\\ndefined on the kuard ReplicaSet. Regardless, use the kubectl get pods command to\\nlist the running kuard Pods. You should see output like the following:\\n$ kubectl get pods\\nNAME READY STATUS RESTARTS AGE\\nkuard-3a2sb 1/1 Running 0 26s\\nkuard-wuq9v 1/1 Running 0 26s\\nkuard-yvzgd 1/1 Running 0 2m\\nAutoscaling a ReplicaSet\\nWhile there will be times when you want to have explicit control over the number of\\nreplicas in a ReplicaSet, often you simply want to have “enough” replicas. The defini‐\\ntion varies depending on the needs of the containers in the ReplicaSet. For example,\\nwith a web server like NGINX, you may want to scale due to CPU usage. For an inmemory cache, you may want to scale with memory consumption. In some cases you\\nmay want to scale in response to custom application metrics. Kubernetes can handle\\nall of these scenarios via Horizontal Pod Autoscaling (HPA).\\nHPA requires the presence of the heapster Pod on your cluster.\\nheapster keeps track of metrics and provides an API for consum‐\\ning metrics that HPA uses when making scaling decisions. Most\\ninstallations of Kubernetes include heapster by default. You can\\nvalidate its presence by listing the Pods in the kube-system name‐\\nspace:\\n$ kubectl get pods --namespace=kube-system\\nYou should see a Pod named heapster somewhere in that list. If\\nyou do not see it, autoscaling will not work correctly.\\n“Horizontal Pod Autoscaling” is kind of a mouthful, and you might wonder why it is\\nnot simply called “autoscaling.” Kubernetes makes a distinction between horizontal\\nscaling, which involves creating additional replicas of a Pod, and vertical scaling,\\n110 | Chapter 9: ReplicaSets\\nwhich involves increasing the resources required for a particular Pod (e.g., increasing\\nthe CPU required for the Pod). Vertical scaling is not currently implemented in\\nKubernetes, but it is planned. Additionally, many solutions also enable cluster\\nautoscaling, where the number of machines in the cluster is scaled in response to\\nresource needs, but this solution is not covered here.\\nAutoscaling based on CPU\\nScaling based on CPU usage is the most common use case for Pod autoscaling. Gen‐\\nerally it is most useful for request-based systems that consume CPU proportionally to\\nthe number of requests they are receiving, while using a relatively static amount of\\nmemory.\\nTo scale a ReplicaSet, you can run a command like the following:\\n$ kubectl autoscale rs kuard --min=2 --max=5 --cpu-percent=80\\nThis command creates an autoscaler that scales between two and five replicas with a\\nCPU threshold of 80%. To view, modify, or delete this resource you can use the stan‐\\ndard kubectl commands and the horizontalpodautoscalers resource. horizontal\\npodautoscalers is quite a bit to type, but it can be shortened to hpa:\\n$ kubectl get hpa\\nBecause of the decoupled nature of Kubernetes, there is no direct\\nlink between the HPA and the ReplicaSet. While this is great for\\nmodularity and composition, it also enables some anti-patterns. In\\nparticular, it’s a bad idea to combine both autoscaling and impera‐\\ntive or declarative management of the number of replicas. If both\\nyou and an autoscaler are attempting to modify the number of rep‐\\nlicas, it’s highly likely that you will clash, resulting in unexpected\\nbehavior.\\nDeleting ReplicaSets\\nWhen a ReplicaSet is no longer required it can be deleted using the kubectl delete\\ncommand. By default, this also deletes the Pods that are managed by the ReplicaSet:\\n$ kubectl delete rs kuard\\nreplicaset \"kuard\" deleted\\nRunning the kubectl get pods command shows that all the kuard Pods created by\\nthe kuard ReplicaSet have also been deleted:\\n$ kubectl get pods\\nDeleting ReplicaSets | 111\\nIf you don’t want to delete the Pods that are being managed by the ReplicaSet, you\\ncan set the --cascade flag to false to ensure only the ReplicaSet object is deleted and\\nnot the Pods:\\n$ kubectl delete rs kuard --cascade=false\\nSummary\\nComposing Pods with ReplicaSets provides the foundation for building robust appli‐\\ncations with automatic failover, and makes deploying those applications a breeze by\\nenabling scalable and sane deployment patterns. ReplicaSets should be used for any\\nPod you care about, even if it is a single Pod! Some people even default to using Rep‐\\nlicaSets instead of Pods. A typical cluster will have many ReplicaSets, so apply liber‐\\nally to the affected area.\\n112 | Chapter 9: ReplicaSets\\nCHAPTER 10\\nDeployments\\nSo far, you have seen how to package your applications as containers, create replica‐\\nted sets of containers, and use Ingress controllers to load-balance traffic to your serv‐\\nices. All of these objects (Pods, ReplicaSets, and Services) are used to build a single\\ninstance of your application. However, they do little to help you manage the daily or\\nweekly cadence of releasing new versions of your application. Indeed, both Pods and\\nReplicaSets are expected to be tied to specific container images that don’t change.\\nThe Deployment object exists to manage the release of new versions. Deployments\\nrepresent deployed applications in a way that transcends any particular version.\\nAdditionally, deployments enable you to easily move from one version of your code\\nto the next. This “rollout” process is specifiable and careful. It waits for a userconfigurable amount of time between upgrading individual Pods. It also uses health\\nchecks to ensure that the new version of the application is operating correctly, and\\nstops the deployment if too many failures occur.\\nUsing deployments you can simply and reliably roll out new software versions\\nwithout downtime or errors. The actual mechanics of the software rollout performed\\nby a deployment is controlled by a deployment controller that runs in the Kubernetes\\ncluster itself. This means you can let a deployment proceed unattended and it will still\\noperate correctly and safely. This makes it easy to integrate deployments with numer‐\\nous continuous delivery tools and services. Further, running server-side makes it safe\\nto perform a rollout from places with poor or intermittent internet connectivity.\\nImagine rolling out a new version of your software from your phone while riding on\\nthe subway. Deployments make this possible and safe!\\n113\\nWhen Kubernetes was first released, one of the most popular dem‐\\nonstrations of its power was the “rolling update,” which showed\\nhow you could use a single command to seamlessly update a run‐\\nning application without any downtime and without losing\\nrequests. This original demo was based on the kubectl rollingupdate command, which is still available in the command-line\\ntool, although its functionality has largely been subsumed by the\\nDeployment object.\\nYour First Deployment\\nLike all objects in Kubernetes, a deployment can be represented as a declarative\\nYAML object that provides the details about what you want to run. In the following\\ncase, the deployment is requesting a single instance of the kuard application:\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n name: kuard\\nspec:\\n selector:\\n matchLabels:\\n run: kuard\\n replicas: 1\\n template:\\n metadata:\\n labels:\\n run: kuard\\n spec:\\n containers:\\n - name: kuard\\n image: gcr.io/kuar-demo/kuard-amd64:blue\\nSave this YAML file as kuard-deployment.yaml, then you can create it using:\\n$ kubectl create -f kuard-deployment.yaml\\nDeployment Internals\\nLet’s explore how deployments actually work. Just as we learned that ReplicaSets\\nmanage Pods, deployments manage ReplicaSets. As with all relationships in Kuber‐\\nnetes, this relationship is defined by labels and a label selector. You can see the label\\nselector by looking at the Deployment object:\\n$ kubectl get deployments kuard \\\\\\n -o jsonpath --template {.spec.selector.matchLabels}\\nmap[run:kuard]\\n114 | Chapter 10: Deployments\\nFrom this you can see that the deployment is managing a ReplicaSet with the label\\nrun=kuard. We can use this in a label selector query across ReplicaSets to find that\\nspecific ReplicaSet:\\n$ kubectl get replicasets --selector=run=kuard\\nNAME DESIRED CURRENT READY AGE\\nkuard-1128242161 1 1 1 13m\\nNow let’s see the relationship between a deployment and a ReplicaSet in action. We\\ncan resize the deployment using the imperative scale command:\\n$ kubectl scale deployments kuard --replicas=2\\ndeployment.extensions/kuard scaled\\nNow if we list that ReplicaSet again, we should see:\\n$ kubectl get replicasets --selector=run=kuard\\nNAME DESIRED CURRENT READY AGE\\nkuard-1128242161 2 2 2 13m\\nScaling the deployment has also scaled the ReplicaSet it controls.\\nNow let’s try the opposite, scaling the ReplicaSet:\\n$ kubectl scale replicasets kuard-1128242161 --replicas=1\\nreplicaset \"kuard-1128242161\" scaled\\nNow get that ReplicaSet again:\\n$ kubectl get replicasets --selector=run=kuard\\nNAME DESIRED CURRENT READY AGE\\nkuard-1128242161 2 2 2 13m\\nThat’s odd. Despite our scaling the ReplicaSet to one replica, it still has two replicas as\\nits desired state. What’s going on? Remember, Kubernetes is an online, self-healing\\nsystem. The top-level Deployment object is managing this ReplicaSet. When you\\nadjust the number of replicas to one, it no longer matches the desired state of the\\ndeployment, which has replicas set to 2. The deployment controller notices this and\\ntakes action to ensure the observed state matches the desired state, in this case read‐\\njusting the number of replicas back to two.\\nIf you ever want to manage that ReplicaSet directly, you need to delete the deploy‐\\nment (remember to set --cascade to false, or else it will delete the ReplicaSet and\\nPods as well!).\\nYour First Deployment | 115\\nCreating Deployments\\nOf course, as has been stated elsewhere, you should have a preference for declarative\\nmanagement of your Kubernetes configurations. This means maintaining the state of\\nyour deployments in YAML or JSON files on disk.\\nAs a starting point, download this deployment into a YAML file:\\n$ kubectl get deployments kuard --export -o yaml > kuard-deployment.yaml\\n$ kubectl replace -f kuard-deployment.yaml --save-config\\nIf you look in the file, you will see something like this:\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n annotations:\\n deployment.kubernetes.io/revision: \"1\"\\n creationTimestamp: null\\n generation: 1\\n labels:\\n run: kuard\\n name: kuard\\n selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/kuard\\nspec:\\n progressDeadlineSeconds: 2147483647\\n replicas: 2\\n revisionHistoryLimit: 10\\n selector:\\n matchLabels:\\n run: kuard\\n strategy:\\n rollingUpdate:\\n maxSurge: 1\\n maxUnavailable: 1\\n type: RollingUpdate\\n template:\\n metadata:\\n creationTimestamp: null\\n labels:\\n run: kuard\\n spec:\\n containers:\\n - image: gcr.io/kuar-demo/kuard-amd64:blue\\n imagePullPolicy: IfNotPresent\\n name: kuard\\n resources: {}\\n terminationMessagePath: /dev/termination-log\\n terminationMessagePolicy: File\\n dnsPolicy: ClusterFirst\\n restartPolicy: Always\\n schedulerName: default-scheduler\\n116 | Chapter 10: Deployments\\n securityContext: {}\\n terminationGracePeriodSeconds: 30\\nstatus: {}\\nA lot of read-only and default fields were removed in the preceding\\nlisting for brevity. You also need to run kubectl replace --saveconfig. This adds an annotation so that, when applying changes in\\nthe future, kubectl will know what the last applied configuration\\nwas for smarter merging of configs. If you always use kubectl\\napply, this step is only required after the first time you create a\\ndeployment using kubectl create -f.\\nThe deployment spec has a very similar structure to the ReplicaSet spec. There is a\\nPod template, which contains a number of containers that are created for each replica\\nmanaged by the deployment. In addition to the Pod specification, there is also a\\nstrategy object:\\n...\\n strategy:\\n rollingUpdate:\\n maxSurge: 1\\n maxUnavailable: 1\\n type: RollingUpdate\\n...\\nThe strategy object dictates the different ways in which a rollout of new software\\ncan proceed. There are two different strategies supported by deployments: Recreate\\nand RollingUpdate.\\nThese are discussed in detail later in this chapter.\\nManaging Deployments\\nAs with all Kubernetes objects, you can get detailed information about your deploy‐\\nment via the kubectl describe command:\\n$ kubectl describe deployments kuard\\nName: kuard\\nNamespace: default\\nCreationTimestamp: Tue, 16 Apr 2019 21:43:25 -0700\\nLabels: run=kuard\\nAnnotations: deployment.kubernetes.io/revision: 1\\nSelector: run=kuard\\nReplicas: 2 desired | 2 updated | 2 total | 2 available | 0 ...\\nStrategyType: RollingUpdate\\nMinReadySeconds: 0\\nRollingUpdateStrategy: 1 max unavailable, 1 max surge\\nManaging Deployments | 117\\nPod Template:\\n Labels: run=kuard\\n Containers:\\n kuard:\\n Image: gcr.io/kuar-demo/kuard-amd64:blue\\n Port: <none>\\n Host Port: <none>\\n Environment: <none>\\n Mounts: <none>\\n Volumes: <none>\\nConditions:\\n Type Status Reason\\n ---- ------ ------\\n Available True MinimumReplicasAvailable\\nOldReplicaSets: <none>\\nNewReplicaSet: kuard-6d69d9fc5c (2/2 replicas created)\\nEvents:\\n Type Reason Age From Message\\n ---- ------ ---- ---- -------\\n Normal ScalingReplicaSet 4m6s deployment-con... ...\\n Normal ScalingReplicaSet 113s (x2 over 3m20s) deployment-con... ...\\nIn the output of describe there is a great deal of important information.\\nTwo of the most important pieces of information in the output are OldReplicaSets\\nand NewReplicaSet. These fields point to the ReplicaSet objects this deployment is\\ncurrently managing. If a deployment is in the middle of a rollout, both fields will be\\nset to a value. If a rollout is complete, OldReplicaSets will be set to <none>.\\nIn addition to the describe command, there is also the kubectl rollout command\\nfor deployments. We will go into this command in more detail later on, but for now,\\nknow that you can use kubectl rollout history to obtain the history of rollouts\\nassociated with a particular deployment. If you have a current deployment in pro‐\\ngress, you can use kubectl rollout status to obtain the current status of a rollout.\\nUpdating Deployments\\nDeployments are declarative objects that describe a deployed application. The two\\nmost common operations on a deployment are scaling and application updates.\\nScaling a Deployment\\nAlthough we previously showed how you could imperatively scale a deployment\\nusing the kubectl scale command, the best practice is to manage your deployments\\ndeclaratively via the YAML files, and then use those files to update your deployment.\\nTo scale up a deployment, you would edit your YAML file to increase the number of\\nreplicas:\\n118 | Chapter 10: Deployments\\n...\\nspec:\\n replicas: 3\\n...\\nOnce you have saved and committed this change, you can update the deployment\\nusing the kubectl apply command:\\n$ kubectl apply -f kuard-deployment.yaml\\nThis will update the desired state of the deployment, causing it to increase the size of\\nthe ReplicaSet it manages, and eventually create a new Pod managed by the deploy‐\\nment:\\n$ kubectl get deployments kuard\\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE\\nkuard 3 3 3 3 4m\\nUpdating a Container Image\\nThe other common use case for updating a deployment is to roll out a new version of\\nthe software running in one or more containers. To do this, you should likewise edit\\nthe deployment YAML file, though in this case you are updating the container image,\\nrather than the number of replicas:\\n...\\n containers:\\n - image: gcr.io/kuar-demo/kuard-amd64:green\\n imagePullPolicy: Always\\n...\\nWe are also going to put an annotation in the template for the deployment to record\\nsome information about the update:\\n...\\nspec:\\n ...\\n template:\\n metadata:\\n annotations:\\n kubernetes.io/change-cause: \"Update to green kuard\"\\n...\\nMake sure you add this annotation to the template and not the\\ndeployment itself, since the kubectl apply command uses this\\nfield in the Deployment object. Also, do not update the changecause annotation when doing simple scaling operations. A modifi‐\\ncation of change-cause is a significant change to the template and\\nwill trigger a new rollout.\\nUpdating Deployments | 119\\nAgain, you can use kubectl apply to update the deployment:\\n$ kubectl apply -f kuard-deployment.yaml\\nAfter you update the deployment it will trigger a rollout, which you can then monitor\\nvia the kubectl rollout command:\\n$ kubectl rollout status deployments kuard\\ndeployment kuard successfully rolled out\\nYou can see the old and new ReplicaSets managed by the deployment along with the\\nimages being used. Both the old and new ReplicaSets are kept around in case you\\nwant to roll back:\\n$ kubectl get replicasets -o wide\\nNAME DESIRED CURRENT READY ... IMAGE(S) ...\\nkuard-1128242161 0 0 0 ... gcr.io/kuar-demo/ ...\\nkuard-1128635377 3 3 3 ... gcr.io/kuar-demo/ ...\\nIf you are in the middle of a rollout and you want to temporarily pause it for some\\nreason (e.g., if you start seeing weird behavior in your system and you want to inves‐\\ntigate), you can use the pause command:\\n$ kubectl rollout pause deployments kuard\\ndeployment \"kuard\" paused\\nIf, after investigation, you believe the rollout can safely proceed, you can use the\\nresume command to start up where you left off:\\n$ kubectl rollout resume deployments kuard\\ndeployment \"kuard\" resumed\\nRollout History\\nKubernetes deployments maintain a history of rollouts, which can be useful both for\\nunderstanding the previous state of the deployment and to roll back to a specific\\nversion.\\nYou can see the deployment history by running:\\n$ kubectl rollout history deployment kuard\\ndeployment.extensions/kuard\\nREVISION CHANGE-CAUSE\\n1 <none>\\n2 Update to green kuard\\nThe revision history is given in oldest to newest order. A unique revision number is\\nincremented for each new rollout. So far we have two: the initial deployment, and the\\nupdate of the image to kuard:1.9.10.\\n120 | Chapter 10: Deployments\\nIf you are interested in more details about a particular revision, you can add the\\n--revision flag to view details about that specific revision:\\n$ kubectl rollout history deployment kuard --revision=2\\ndeployment.extensions/kuard with revision #2\\nPod Template:\\n Labels: pod-template-hash=54b74ddcd4\\n run=kuard\\n Annotations: kubernetes.io/change-cause: Update to green kuard\\n Containers:\\n kuard:\\n Image: gcr.io/kuar-demo/kuard-amd64:green\\n Port: <none>\\n Host Port: <none>\\n Environment: <none>\\n Mounts: <none>\\n Volumes: <none>\\nLet’s do one more update for this example. Update the kuard version back to blue by\\nmodifying the container version number and updating the change-cause annotation.\\nApply it with kubectl apply. Our history should now have three entries:\\n$ kubectl rollout history deployment kuard\\ndeployment.extensions/kuard\\nREVISION CHANGE-CAUSE\\n1 <none>\\n2 Update to green kuard\\n3 Update to blue kuard\\nLet’s say there is an issue with the latest release and you want to roll back while you\\ninvestigate. You can simply undo the last rollout:\\n$ kubectl rollout undo deployments kuard\\ndeployment \"kuard\" rolled back\\nThe undo command works regardless of the stage of the rollout. You can undo both\\npartially completed and fully completed rollouts. An undo of a rollout is actually sim‐\\nply a rollout in reverse (e.g., from v2 to v1, instead of from v1 to v2), and all of the\\nsame policies that control the rollout strategy apply to the undo strategy as well. You\\ncan see the Deployment object simply adjusts the desired replica counts in the man‐\\naged ReplicaSets:\\n$ kubectl get replicasets -o wide\\nNAME DESIRED CURRENT READY ... IMAGE(S) ...\\nkuard-1128242161 0 0 0 ... gcr.io/kuar-demo/ ...\\nkuard-1570155864 0 0 0 ... gcr.io/kuar-demo/ ...\\nkuard-2738859366 3 3 3 ... gcr.io/kuar-demo/ ...\\nUpdating Deployments | 121\\nWhen using declarative files to control your production systems,\\nyou want to, as much as possible, ensure that the checked-in mani‐\\nfests match what is actually running in your cluster. When you do a\\nkubectl rollout undo you are updating the production state in a\\nway that isn’t reflected in your source control.\\nAn alternative (and perhaps preferred) way to undo a rollout is to\\nrevert your YAML file and kubectl apply the previous version. In\\nthis way, your “change tracked configuration” more closely tracks\\nwhat is really running in your cluster.\\nLet’s look at our deployment history again:\\n$ kubectl rollout history deployment kuard\\ndeployment.extensions/kuard\\nREVISION CHANGE-CAUSE\\n1 <none>\\n3 Update to blue kuard\\n4 Update to green kuard\\nRevision 2 is missing! It turns out that when you roll back to a previous revision, the\\ndeployment simply reuses the template and renumbers it so that it is the latest revi‐\\nsion. What was revision 2 before is now reordered into revision 4.\\nWe previously saw that you can use the kubectl rollout undo command to roll\\nback to a previous version of a deployment. Additionally, you can roll back to a spe‐\\ncific revision in the history using the --to-revision flag:\\n$ kubectl rollout undo deployments kuard --to-revision=3\\ndeployment \"kuard\" rolled back\\n$ kubectl rollout history deployment kuard\\ndeployment.extensions/kuard\\nREVISION CHANGE-CAUSE\\n1 <none>\\n4 Update to green kuard\\n5 Update to blue kuard\\nAgain, the undo took revision 3, applied it, and renumbered it as revision 5.\\nSpecifying a revision of 0 is a shorthand way of specifying the previous revision. In\\nthis way, kubectl rollout undo is equivalent to kubectl rollout undo --torevision=0.\\nBy default, the complete revision history of a deployment is kept attached to the\\nDeployment object itself. Over time (e.g., years) this history can grow fairly large, so\\nit is recommended that if you have deployments that you expect to keep around for a\\nlong time you set a maximum history size for the deployment revision history, to\\nlimit the total size of the Deployment object. For example, if you do a daily update\\n122 | Chapter 10: Deployments\\nyou may limit your revision history to 14, to keep a maximum of 2 weeks’ worth of\\nrevisions (if you don’t expect to need to roll back beyond 2 weeks).\\nTo accomplish this, use the revisionHistoryLimit property in the deployment\\nspecification:\\n...\\nspec:\\n # We do daily rollouts, limit the revision history to two weeks of\\n # releases as we don\\'t expect to roll back beyond that.\\n revisionHistoryLimit: 14\\n...\\nDeployment Strategies\\nWhen it comes time to change the version of software implementing your service, a\\nKubernetes deployment supports two different rollout strategies:\\n• Recreate\\n• RollingUpdate\\nRecreate Strategy\\nThe Recreate strategy is the simpler of the two rollout strategies. It simply updates\\nthe ReplicaSet it manages to use the new image and terminates all of the Pods associ‐\\nated with the deployment. The ReplicaSet notices that it no longer has any replicas,\\nand re-creates all Pods using the new image. Once the Pods are re-created, they are\\nrunning the new version.\\nWhile this strategy is fast and simple, it has one major drawback—it is potentially\\ncatastrophic, and will almost certainly result in some site downtime. Because of this,\\nthe Recreate strategy should only be used for test deployments where a service is not\\nuser-facing and a small amount of downtime is acceptable.\\nRollingUpdate Strategy\\nThe RollingUpdate strategy is the generally preferable strategy for any user-facing\\nservice. While it is slower than Recreate, it is also significantly more sophisticated\\nand robust. Using RollingUpdate, you can roll out a new version of your service\\nwhile it is still receiving user traffic, without any downtime.\\nAs you might infer from the name, the RollingUpdate strategy works by updating a\\nfew Pods at a time, moving incrementally until all of the Pods are running the new\\nversion of your software.\\nDeployment Strategies | 123\\nManaging multiple versions of your service\\nImportantly, this means that for a period of time, both the new and the old version of\\nyour service will be receiving requests and serving traffic. This has important impli‐\\ncations for how you build your software. Namely, it is critically important that each\\nversion of your software, and all of its clients, is capable of talking interchangeably\\nwith both a slightly older and a slightly newer version of your software.\\nAs an example of why this is important, consider the following scenario:\\nYou are in the middle of rolling out your frontend software; half of your servers are\\nrunning version 1 and half are running version 2. A user makes an initial request to\\nyour service and downloads a client-side JavaScript library that implements your UI.\\nThis request is serviced by a version 1 server and thus the user receives the version 1\\nclient library. This client library runs in the user’s browser and makes subsequent API\\nrequests to your service. These API requests happen to be routed to a version 2 server;\\nthus, version 1 of your JavaScript client library is talking to version 2 of your API\\nserver. If you haven’t ensured compatibility between these versions, your application\\nwon’t function correctly.\\nAt first, this might seem like an extra burden. But in truth, you always had this prob‐\\nlem; you may just not have noticed. Concretely, a user can make a request at time t\\njust before you initiate an update. This request is serviced by a version 1 server. At\\nt_1 you update your service to version 2. At t_2 the version 1 client code running on\\nthe user’s browser runs and hits an API endpoint being operated by a version 2\\nserver. No matter how you update your software, you have to maintain backward and\\nforward compatibility for reliable updates. The nature of the RollingUpdate strategy\\nsimply makes it more clear and explicit that this is something to think about.\\nNote that this doesn’t just apply to JavaScript clients—the same thing is true of client\\nlibraries that are compiled into other services that make calls to your service. Just\\nbecause you updated doesn’t mean they have updated their client libraries. This sort\\nof backward compatibility is critical to decoupling your service from systems that\\ndepend on your service. If you don’t formalize your APIs and decouple yourself, you\\nare forced to carefully manage your rollouts with all of the other systems that call into\\nyour service. This kind of tight coupling makes it extremely hard to produce the nec‐\\nessary agility to be able to push out new software every week, let alone every hour or\\nevery day. In the decoupled architecture shown in Figure 10-1, the frontend is iso‐\\nlated from the backend via an API contract and a load balancer, whereas in the cou‐\\npled architecture, a thick client compiled into the frontend is used to connect directly\\nto the backends.\\n124 | Chapter 10: Deployments\\nFigure 10-1. Diagrams of both decoupled (left) and coupled (right) application\\narchitectures\\nCon\\x80guring a rolling update\\nRollingUpdate is a fairly generic strategy; it can be used to update a variety of appli‐\\ncations in a variety of settings. Consequently, the rolling update itself is quite configu‐\\nrable; you can tune its behavior to suit your particular needs. There are two\\nparameters you can use to tune the rolling update behavior: maxUnavailable and\\nmaxSurge.\\nThe maxUnavailable parameter sets the maximum number of Pods that can be\\nunavailable during a rolling update. It can either be set to an absolute number (e.g., 3,\\nmeaning a maximum of three Pods can be unavailable) or to a percentage (e.g., 20%,\\nmeaning a maximum of 20% of the desired number of replicas can be unavailable).\\nGenerally speaking, using a percentage is a good approach for most services, since the\\nvalue is correctly applicable regardless of the desired number of replicas in the\\ndeployment. However, there are times when you may want to use an absolute number\\n(e.g., limiting the maximum unavailable Pods to one).\\nAt its core, the maxUnavailable parameter helps tune how quickly a rolling update\\nproceeds. For example, if you set maxUnavailable to 50%, then the rolling update will\\nimmediately scale the old ReplicaSet down to 50% of its original size. If you have four\\nreplicas, it will scale it down to two replicas. The rolling update will then replace the\\nremoved Pods by scaling the new ReplicaSet up to two replicas, for a total of four rep‐\\nlicas (two old, two new). It will then scale the old ReplicaSet down to zero replicas, for\\na total size of two new replicas. Finally, it will scale the new ReplicaSet up to four rep‐\\nlicas, completing the rollout. Thus, with maxUnavailable set to 50%, our rollout com‐\\npletes in four steps, but with only 50% of our service capacity at times.\\nDeployment Strategies | 125\\nConsider what happens if we instead set maxUnavailable to 25%. In this situation,\\neach step is only performed with a single replica at a time and thus it takes twice as\\nmany steps for the rollout to complete, but availability only drops to a minimum of\\n75% during the rollout. This illustrates how maxUnavailable allows us to trade roll‐\\nout speed for availability.\\nThe observant among you will note that the Recreate strategy is\\nidentical to the RollingUpdate strategy with maxUnavailable set\\nto 100%.\\nUsing reduced capacity to achieve a successful rollout is useful either when your ser‐\\nvice has cyclical traffic patterns (e.g., much less traffic at night) or when you have\\nlimited resources, so scaling to larger than the current maximum number of replicas\\nisn’t possible.\\nHowever, there are situations where you don’t want to fall below 100% capacity, but\\nyou are willing to temporarily use additional resources in order to perform a rollout.\\nIn these situations, you can set the maxUnavailable parameter to 0%, and instead con‐\\ntrol the rollout using the maxSurge parameter. Like maxUnavailable, maxSurge can be\\nspecified either as a specific number or a percentage.\\nThe maxSurge parameter controls how many extra resources can be created to ach‐\\nieve a rollout. To illustrate how this works, imagine we have a service with 10 replicas.\\nWe set maxUnavailable to 0 and maxSurge to 20%. The first thing the rollout will do is\\nscale the new ReplicaSet up to 2 replicas, for a total of 12 (120%) in the service. It will\\nthen scale the old ReplicaSet down to 8 replicas, for a total of 10 (8 old, 2 new) in the\\nservice. This process proceeds until the rollout is complete. At any time, the capacity\\nof the service is guaranteed to be at least 100% and the maximum extra resources\\nused for the rollout are limited to an additional 20% of all resources.\\nSetting maxSurge to 100% is equivalent to a blue/green deployment.\\nThe deployment controller first scales the new version up to 100%\\nof the old version. Once the new version is healthy, it immediately\\nscales the old version down to 0%.\\nSlowing Rollouts to Ensure Service Health\\nThe purpose of a staged rollout is to ensure that the rollout results in a healthy, stable\\nservice running the new software version. To do this, the deployment controller\\nalways waits until a Pod reports that it is ready before moving on to updating the next\\nPod.\\n126 | Chapter 10: Deployments\\nThe deployment controller examines the Pod’s status as determined\\nby its readiness checks. Readiness checks are part of the Pod’s\\nhealth probes, and they are described in detail in Chapter 5. If you\\nwant to use deployments to reliably roll out your software, you\\nhave to specify readiness health checks for the containers in your\\nPod. Without these checks, the deployment controller is running\\nblind.\\nSometimes, however, simply noticing that a Pod has become ready doesn’t give you\\nsufficient confidence that the Pod actually is behaving correctly. Some error condi‐\\ntions only occur after a period of time. For example, you could have a serious mem‐\\nory leak that takes a few minutes to show up, or you could have a bug that is only\\ntriggered by 1% of all requests. In most real-world scenarios, you want to wait a\\nperiod of time to have high confidence that the new version is operating correctly\\nbefore you move on to updating the next Pod.\\nFor deployments, this time to wait is defined by the minReadySeconds parameter:\\n...\\nspec:\\n minReadySeconds: 60\\n...\\nSetting minReadySeconds to 60 indicates that the deployment must wait for 60 sec‐\\nonds after seeing a Pod become healthy before moving on to updating the next Pod.\\nIn addition to waiting a period of time for a Pod to become healthy, you also want to\\nset a timeout that limits how long the system will wait. Suppose, for example, the new\\nversion of your service has a bug and immediately deadlocks. It will never become\\nready, and in the absence of a timeout, the deployment controller will stall your rollout forever.\\nThe correct behavior in such a situation is to time out the rollout. This in turn marks\\nthe rollout as failed. This failure status can be used to trigger alerting that can indicate\\nto an operator that there is a problem with the rollout.\\nAt first blush, timing out a rollout might seem like an unnecessary\\ncomplication. However, increasingly, things like rollouts are being\\ntriggered by fully automated systems with little to no human\\ninvolvement. In such a situation, timing out becomes a critical\\nexception, which can either trigger an automated rollback of the\\nrelease or create a ticket/event that triggers human intervention.\\nTo set the timeout period, the deployment parameter progressDeadlineSeconds is\\nused:\\nDeployment Strategies | 127\\n...\\nspec:\\n progressDeadlineSeconds: 600\\n...\\nThis example sets the progress deadline to 10 minutes. If any particular stage in the\\nrollout fails to progress in 10 minutes, then the deployment is marked as failed, and\\nall attempts to move the deployment forward are halted.\\nIt is important to note that this timeout is given in terms of deployment progress, not\\nthe overall length of a deployment. In this context, progress is defined as any time the\\ndeployment creates or deletes a Pod. When that happens, the timeout clock is reset to\\nzero. Figure 10-2 is an illustration of the deployment lifecycle.\\nFigure 10-2. The Kubernetes deployment lifecycle\\nDeleting a Deployment\\nIf you ever want to delete a deployment, you can do it either with the imperative\\ncommand:\\n$ kubectl delete deployments kuard\\nor using the declarative YAML file we created earlier:\\n$ kubectl delete -f kuard-deployment.yaml\\nIn either case, by default, deleting a deployment deletes the entire service. It will\\ndelete not just the deployment, but also any ReplicaSets being managed by the\\ndeployment, as well as any Pods being managed by the ReplicaSets. As with Replica‐\\nSets, if this is not the desired behavior, you can use the --cascade=false flag to\\nexclusively delete the Deployment object.\\nMonitoring a Deployment\\nWhen it comes to a deployment, it is important to note that if it fails to makes pro‐\\ngress after a certain amount of time, the deployment will time out. When this hap‐\\npens, the status of the deployment will transition to a failed state. This status can be\\n128 | Chapter 10: Deployments\\nobtained from the status.conditions array, where there will be a Condition whose\\nType is Progressing and whose Status is False. A deployment in such a state has\\nfailed and will not progress further. To set how long the deployment controller should\\nwait before transitioning into this state, use the spec.progressDeadlineSeconds\\nfield.\\nSummary\\nAt the end of the day, the primary goal of Kubernetes is to make it easy for you to\\nbuild and deploy reliable distributed systems. This means not just instantiating the\\napplication once, but managing the regularly scheduled rollout of new versions of\\nthat software service. Deployments are a critical piece of reliable rollouts and rollout\\nmanagement for your services.\\nSummary | 129\\n\\nCHAPTER 11\\nDaemonSets\\nDeployments and ReplicaSets are generally about creating a service (e.g., a web\\nserver) with multiple replicas for redundancy. But that is not the only reason you may\\nwant to replicate a set of Pods within a cluster. Another reason to replicate a set of\\nPods is to schedule a single Pod on every node within the cluster. Generally, the moti‐\\nvation for replicating a Pod to every node is to land some sort of agent or daemon on\\neach node, and the Kubernetes object for achieving this is the DaemonSet.\\nA DaemonSet ensures a copy of a Pod is running across a set of nodes in a Kuber‐\\nnetes cluster. DaemonSets are used to deploy system daemons such as log collectors\\nand monitoring agents, which typically must run on every node. DaemonSets share\\nsimilar functionality with ReplicaSets; both create Pods that are expected to be longrunning services and ensure that the desired state and the observed state of the clus‐\\nter match.\\nGiven the similarities between DaemonSets and ReplicaSets, it’s important to under‐\\nstand when to use one over the other. ReplicaSets should be used when your applica‐\\ntion is completely decoupled from the node and you can run multiple copies on a\\ngiven node without special consideration. DaemonSets should be used when a single\\ncopy of your application must run on all or a subset of the nodes in the cluster.\\nYou should generally not use scheduling restrictions or other parameters to ensure\\nthat Pods do not colocate on the same node. If you find yourself wanting a single Pod\\nper node, then a DaemonSet is the correct Kubernetes resource to use. Likewise, if\\nyou find yourself building a homogeneous replicated service to serve user traffic, then\\na ReplicaSet is probably the right Kubernetes resource to use.\\nYou can use labels to run DaemonSet Pods on specific nodes; for example, you may\\nwant to run specialized intrusion-detection software on nodes that are exposed to the\\nedge network.\\n131\\nYou can also use DaemonSets to install software on nodes in a cloud-based cluster.\\nFor many cloud services, an upgrade or scaling of a cluster can delete and/or recreate\\nnew virtual machines. This dynamic immutable infrastructure approach can cause\\nproblems if you want (or are required by central IT) to have specific software on\\nevery node. To ensure that specific software is installed on every machine despite\\nupgrades and scale events, a DaemonSet is the right approach. You can even mount\\nthe host filesystem and run scripts that install RPM/DEB packages onto the host\\noperating system. In this way, you can have a cloud-native cluster that still meets the\\nenterprise requirements of your IT department.\\nDaemonSet Scheduler\\nBy default a DaemonSet will create a copy of a Pod on every node unless a node selec‐\\ntor is used, which will limit eligible nodes to those with a matching set of labels. Dae‐\\nmonSets determine which node a Pod will run on at Pod creation time by specifying\\nthe nodeName field in the Pod spec. As a result, Pods created by DaemonSets are\\nignored by the Kubernetes scheduler.\\nLike ReplicaSets, DaemonSets are managed by a reconciliation control loop that\\nmeasures the desired state (a Pod is present on all nodes) with the observed state (is\\nthe Pod present on a particular node?). Given this information, the DaemonSet con‐\\ntroller creates a Pod on each node that doesn’t currently have a matching Pod.\\nIf a new node is added to the cluster, then the DaemonSet controller notices that it is\\nmissing a Pod and adds the Pod to the new node.\\nDaemonSets and ReplicaSets are a great demonstration of the value\\nof Kubernetes’s decoupled architecture. It might seem that the right\\ndesign would be for a ReplicaSet to own the Pods it manages, and\\nfor Pods to be subresources of a ReplicaSet. Likewise, the Pods\\nmanaged by a DaemonSet would be subresources of that Daemon‐\\nSet. However, this kind of encapsulation would require that tools\\nfor dealing with Pods be written two different times, once for Dae‐\\nmonSets and once for ReplicaSets. Instead, Kubernetes uses a\\ndecoupled approach where Pods are top-level objects. This means\\nthat every tool you have learned for introspecting Pods in the con‐\\ntext of ReplicaSets (e.g., kubectl logs <pod-name>) is equally\\napplicable to Pods created by DaemonSets.\\nCreating DaemonSets\\nDaemonSets are created by submitting a DaemonSet configuration to the Kubernetes\\nAPI server. The DaemonSet in Example 11-1 will create a fluentd logging agent on\\nevery node in the target cluster.\\n132 | Chapter 11: DaemonSets\\nExample 11-1. fluentd.yaml\\napiVersion: extensions/v1beta1\\nkind: DaemonSet\\nmetadata:\\n name: fluentd\\n labels:\\n app: fluentd\\nspec:\\n template:\\n metadata:\\n labels:\\n app: fluentd\\n spec:\\n containers:\\n - name: fluentd\\n image: fluent/fluentd:v0.14.10\\n resources:\\n limits:\\n memory: 200Mi\\n requests:\\n cpu: 100m\\n memory: 200Mi\\n volumeMounts:\\n - name: varlog\\n mountPath: /var/log\\n - name: varlibdockercontainers\\n mountPath: /var/lib/docker/containers\\n readOnly: true\\n terminationGracePeriodSeconds: 30\\n volumes:\\n - name: varlog\\n hostPath:\\n path: /var/log\\n - name: varlibdockercontainers\\n hostPath:\\n path: /var/lib/docker/containers\\nDaemonSets require a unique name across all DaemonSets in a given Kubernetes\\nnamespace. Each DaemonSet must include a Pod template spec, which will be used to\\ncreate Pods as needed. This is where the similarities between ReplicaSets and Dae‐\\nmonSets end. Unlike ReplicaSets, DaemonSets will create Pods on every node in the\\ncluster by default unless a node selector is used.\\nOnce you have a valid DaemonSet configuration in place, you can use the kubectl\\napply command to submit the DaemonSet to the Kubernetes API. In this section we\\nwill create a DaemonSet to ensure the fluentd HTTP server is running on every\\nnode in our cluster:\\n$ kubectl apply -f fluentd.yaml\\ndaemonset \"fluentd\" created\\nCreating DaemonSets | 133\\nOnce the fluentd DaemonSet has been successfully submitted to the Kubernetes\\nAPI, you can query its current state using the kubectl describe command:\\n$ kubectl describe daemonset fluentd\\nName: fluentd\\nImage(s): fluent/fluentd:v0.14.10\\nSelector: app=fluentd\\nNode-Selector: <none>\\nLabels: app=fluentd\\nDesired Number of Nodes Scheduled: 3\\nCurrent Number of Nodes Scheduled: 3\\nNumber of Nodes Misscheduled: 0\\nPods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed\\nThis output indicates a fluentd Pod was successfully deployed to all three nodes in\\nour cluster. We can verify this using the kubectl get pods command with the -o flag\\nto print the nodes where each fluentd Pod was assigned:\\n$ kubectl get pods -o wide\\nNAME AGE NODE\\nfluentd-1q6c6 13m k0-default-pool-35609c18-z7tb\\nfluentd-mwi7h 13m k0-default-pool-35609c18-ydae\\nfluentd-zr6l7 13m k0-default-pool-35609c18-pol3\\nWith the fluentd DaemonSet in place, adding a new node to the cluster will result in\\na fluentd Pod being deployed to that node automatically:\\n$ kubectl get pods -o wide\\nNAME AGE NODE\\nfluentd-1q6c6 13m k0-default-pool-35609c18-z7tb\\nfluentd-mwi7h 13m k0-default-pool-35609c18-ydae\\nfluentd-oipmq 43s k0-default-pool-35609c18-0xnl\\nfluentd-zr6l7 13m k0-default-pool-35609c18-pol3\\nThis is exactly the behavior you want when managing logging daemons and other\\ncluster-wide services. No action was required from our end; this is how the Kuber‐\\nnetes DaemonSet controller reconciles its observed state with our desired state.\\nLimiting DaemonSets to Speci\\x80c Nodes\\nThe most common use case for DaemonSets is to run a Pod across every node in a\\nKubernetes cluster. However, there are some cases where you want to deploy a Pod to\\nonly a subset of nodes. For example, maybe you have a workload that requires a GPU\\nor access to fast storage only available on a subset of nodes in your cluster. In cases\\nlike these, node labels can be used to tag specific nodes that meet workload\\nrequirements.\\n134 | Chapter 11: DaemonSets\\nAdding Labels to Nodes\\nThe first step in limiting DaemonSets to specific nodes is to add the desired set of\\nlabels to a subset of nodes. This can be achieved using the kubectl label command.\\nThe following command adds the ssd=true label to a single node:\\n$ kubectl label nodes k0-default-pool-35609c18-z7tb ssd=true\\nnode \"k0-default-pool-35609c18-z7tb\" labeled\\nJust like with other Kubernetes resources, listing nodes without a label selector\\nreturns all nodes in the cluster:\\n$ kubectl get nodes\\nNAME STATUS AGE\\nk0-default-pool-35609c18-0xnl Ready 23m\\nk0-default-pool-35609c18-pol3 Ready 1d\\nk0-default-pool-35609c18-ydae Ready 1d\\nk0-default-pool-35609c18-z7tb Ready 1d\\nUsing a label selector, we can filter nodes based on labels. To list only the nodes that\\nhave the ssd label set to true, use the kubectl get nodes command with the\\n--selector flag:\\n$ kubectl get nodes --selector ssd=true\\nNAME STATUS AGE\\nk0-default-pool-35609c18-z7tb Ready 1d\\nNode Selectors\\nNode selectors can be used to limit what nodes a Pod can run on in a given Kuber‐\\nnetes cluster. Node selectors are defined as part of the Pod spec when creating a Dae‐\\nmonSet. The DaemonSet configuration in Example 11-2 limits NGINX to running\\nonly on nodes with the ssd=true label set.\\nExample 11-2. nginx-fast-storage.yaml\\napiVersion: extensions/v1beta1\\nkind: \"DaemonSet\"\\nmetadata:\\n labels:\\n app: nginx\\n ssd: \"true\"\\n name: nginx-fast-storage\\nspec:\\n template:\\n metadata:\\n labels:\\n app: nginx\\n ssd: \"true\"\\n spec:\\nLimiting DaemonSets to Speci\\x80c Nodes | 135\\n nodeSelector:\\n ssd: \"true\"\\n containers:\\n - name: nginx\\n image: nginx:1.10.0\\nLet’s see what happens when we submit the nginx-fast-storage DaemonSet to the\\nKubernetes API:\\n$ kubectl apply -f nginx-fast-storage.yaml\\ndaemonset \"nginx-fast-storage\" created\\nSince there is only one node with the ssd=true label, the nginx-fast-storage Pod\\nwill only run on that node:\\n$ kubectl get pods -o wide\\nNAME STATUS NODE\\nnginx-fast-storage-7b90t Running k0-default-pool-35609c18-z7tb\\nAdding the ssd=true label to additional nodes will cause the nginx-fast-storage\\nPod to be deployed on those nodes. The inverse is also true: if a required label is\\nremoved from a node, the Pod will be removed by the DaemonSet controller.\\nRemoving labels from a node that are required by a DaemonSet’s\\nnode selector will cause the Pod being managed by that DaemonSet\\nto be removed from the node.\\nUpdating a DaemonSet\\nDaemonSets are great for deploying services across an entire cluster, but what about\\nupgrades? Prior to Kubernetes 1.6, the only way to update Pods managed by a Dae‐\\nmonSet was to update the DaemonSet and then manually delete each Pod that was\\nmanaged by the DaemonSet so that it would be re-created with the new configura‐\\ntion. With the release of Kubernetes 1.6, DaemonSets gained an equivalent to the\\nDeployment object that manages a DaemonSet rollout inside the cluster.\\nRolling Update of a DaemonSet\\nDaemonSets can be rolled out using the same RollingUpdate strategy that deploy‐\\nments use. You can configure the update strategy using the spec.updateStrat\\negy.type field, which should have the value RollingUpdate. When a DaemonSet has\\nan update strategy of RollingUpdate, any change to the spec.template field (or\\nsubfields) in the DaemonSet will initiate a rolling update.\\n136 | Chapter 11: DaemonSets\\nAs with rolling updates of deployments (see Chapter 10), the RollingUpdate strategy\\ngradually updates members of a DaemonSet until all of the Pods are running the new\\nconfiguration. There are two parameters that control the rolling update of a\\nDaemonSet:\\n• spec.minReadySeconds, which determines how long a Pod must be “ready”\\nbefore the rolling update proceeds to upgrade subsequent Pods\\n• spec.updateStrategy.rollingUpdate.maxUnavailable, which indicates how\\nmany Pods may be simultaneously updated by the rolling update\\nYou will likely want to set spec.minReadySeconds to a reasonably long value, for\\nexample 30–60 seconds, to ensure that your Pod is truly healthy before the rollout\\nproceeds.\\nThe setting for spec.updateStrategy.rollingUpdate.maxUnavailable is more\\nlikely to be application-dependent. Setting it to 1 is a safe, general-purpose strategy,\\nbut it also takes a while to complete the rollout (number of nodes × minReadySec\\nonds). Increasing the maximum unavailability will make your rollout move faster, but\\nincreases the “blast radius” of a failed rollout. The characteristics of your application\\nand cluster environment dictate the relative values of speed versus safety. A good\\napproach might be to set maxUnavailable to 1 and only increase it if users or admin‐\\nistrators complain about DaemonSet rollout speed.\\nOnce a rolling update has started, you can use the kubectl rollout commands to\\nsee the current status of a DaemonSet rollout.\\nFor example, kubectl rollout status daemonSets my-daemon-set will show the\\ncurrent rollout status of a DaemonSet named my-daemon-set.\\nDeleting a DaemonSet\\nDeleting a DaemonSet is pretty straightforward using the kubectl delete com‐\\nmand. Just be sure to supply the correct name of the DaemonSet you would like to\\ndelete:\\n$ kubectl delete -f fluentd.yaml\\nDeleting a DaemonSet will also delete all the Pods being managed\\nby that DaemonSet. Set the --cascade flag to false to ensure only\\nthe DaemonSet is deleted and not the Pods.\\nDeleting a DaemonSet | 137\\nSummary\\nDaemonSets provide an easy-to-use abstraction for running a set of Pods on every\\nnode in a Kubernetes cluster, or, if the case requires it, on a subset of nodes based on\\nlabels. The DaemonSet provides its own controller and scheduler to ensure key serv‐\\nices like monitoring agents are always up and running on the right nodes in your\\ncluster.\\nFor some applications, you simply want to schedule a certain number of replicas; you\\ndon’t really care where they run as long as they have sufficient resources and distribu‐\\ntion to operate reliably. However, there is a different class of applications, like agents\\nand monitoring applications, that need to be present on every machine in a cluster to\\nfunction properly. These DaemonSets aren’t really traditional serving applications,\\nbut rather add additional capabilities and features to the Kubernetes cluster itself.\\nBecause the DaemonSet is an active declarative object managed by a controller, it\\nmakes it easy to declare your intent that an agent run on every machine without\\nexplicitly placing it on every machine. This is especially useful in the context of an\\nautoscaled Kubernetes cluster where nodes may constantly be coming and going\\nwithout user intervention. In such cases, the DaemonSet automatically adds the\\nproper agents to each node as it is added to the cluster by the autoscaler.\\n138 | Chapter 11: DaemonSets\\nCHAPTER 12\\nJobs\\nSo far we have focused on long-running processes such as databases and web applica‐\\ntions. These types of workloads run until either they are upgraded or the service is no\\nlonger needed. While long-running processes make up the large majority of work‐\\nloads that run on a Kubernetes cluster, there is often a need to run short-lived, oneoff tasks. The Job object is made for handling these types of tasks.\\nA job creates Pods that run until successful termination (i.e., exit with 0). In contrast,\\na regular Pod will continually restart regardless of its exit code. Jobs are useful for\\nthings you only want to do once, such as database migrations or batch jobs. If run as a\\nregular Pod, your database migration task would run in a loop, continually repopulat‐\\ning the database after every exit.\\nIn this chapter we’ll explore the most common job patterns afforded by Kubernetes.\\nWe will also leverage these patterns in real-life scenarios.\\nThe Job Object\\nThe Job object is responsible for creating and managing Pods defined in a template in\\nthe job specification. These Pods generally run until successful completion. The Job\\nobject coordinates running a number of Pods in parallel.\\nIf the Pod fails before a successful termination, the job controller will create a new\\nPod based on the Pod template in the job specification. Given that Pods have to be\\nscheduled, there is a chance that your job will not execute if the required resources\\nare not found by the scheduler. Also, due to the nature of distributed systems there is\\na small chance, during certain failure scenarios, that duplicate Pods will be created for\\na specific task.\\n139\\nJob Patterns\\nJobs are designed to manage batch-like workloads where work items are processed by\\none or more Pods. By default, each job runs a single Pod once until successful termi‐\\nnation. This job pattern is defined by two primary attributes of a job, namely the\\nnumber of job completions and the number of Pods to run in parallel. In the case of\\nthe “run once until completion” pattern, the completions and parallelism parame‐\\nters are set to 1.\\nTable 12-1 highlights job patterns based on the combination of completions and\\nparallelism for a job configuration.\\nTable 12-1. Job patterns\\nType Use case Behavior completions parallelism\\nOne shot Database migrations A single Pod running once until\\nsuccessful termination\\n1 1\\nParallel \\x80xed\\ncompletions\\nMultiple Pods processing a\\nset of work in parallel\\nOne or more Pods running one or more\\ntimes until reaching a \\x80xed completion\\ncount\\n1+ 1+\\nWork queue:\\nparallel jobs\\nMultiple Pods processing\\nfrom a centralized work\\nqueue\\nOne or more Pods running once until\\nsuccessful termination\\n1 2+\\nOne Shot\\nOne-shot jobs provide a way to run a single Pod once until successful termination.\\nWhile this may sound like an easy task, there is some work involved in pulling this\\noff. First, a Pod must be created and submitted to the Kubernetes API. This is done\\nusing a Pod template defined in the job configuration. Once a job is up and running,\\nthe Pod backing the job must be monitored for successful termination. A job can fail\\nfor any number of reasons, including an application error, an uncaught exception\\nduring runtime, or a node failure before the job has a chance to complete. In all cases,\\nthe job controller is responsible for recreating the Pod until a successful termination\\noccurs.\\nThere are multiple ways to create a one-shot job in Kubernetes. The easiest is to use\\nthe kubectl command-line tool:\\n$ kubectl run -i oneshot \\\\\\n --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\\\n --restart=OnFailure \\\\\\n -- --keygen-enable \\\\\\n --keygen-exit-on-complete \\\\\\n --keygen-num-to-gen 10\\n...\\n140 | Chapter 12: Jobs\\n(ID 0) Workload starting\\n(ID 0 1/10) Item done: SHA256:nAsUsG54XoKRkJwyN+OShkUPKew3mwq7OCc\\n(ID 0 2/10) Item done: SHA256:HVKX1ANns6SgF/er1lyo+ZCdnB8geFGt0/8\\n(ID 0 3/10) Item done: SHA256:irjCLRov3mTT0P0JfsvUyhKRQ1TdGR8H1jg\\n(ID 0 4/10) Item done: SHA256:nbQAIVY/yrhmEGk3Ui2sAHuxb/o6mYO0qRk\\n(ID 0 5/10) Item done: SHA256:CCpBoXNlXOMQvR2v38yqimXGAa/w2Tym+aI\\n(ID 0 6/10) Item done: SHA256:wEY2TTIDz4ATjcr1iimxavCzZzNjRmbOQp8\\n(ID 0 7/10) Item done: SHA256:t3JSrCt7sQweBgqG5CrbMoBulwk4lfDWiTI\\n(ID 0 8/10) Item done: SHA256:E84/Vze7KKyjCh9OZh02MkXJGoty9PhaCec\\n(ID 0 9/10) Item done: SHA256:UOmYex79qqbI1MhcIfG4hDnGKonlsij2k3s\\n(ID 0 10/10) Item done: SHA256:WCR8wIGOFag84Bsa8f/9QHuKqF+0mEnCADY\\n(ID 0) Workload exiting\\nThere are some things to note here:\\n• The -i option to kubectl indicates that this is an interactive command. kubectl\\nwill wait until the job is running and then show the log output from the first (and\\nin this case only) Pod in the job.\\n• --restart=OnFailure is the option that tells kubectl to create a Job object.\\n• All of the options after -- are command-line arguments to the container image.\\nThese instruct our test server (kuard) to generate 10 4,096-bit SSH keys and then\\nexit.\\n• Your output may not match this exactly. kubectl often misses the first couple of\\nlines of output with the -i option.\\nAfter the job has completed, the Job object and related Pod are still around. This is so\\nthat you can inspect the log output. Note that this job won’t show up in kubectl get\\njobs unless you pass the -a flag. Without this flag, kubectl hides completed jobs.\\nDelete the job before continuing:\\n$ kubectl delete jobs oneshot\\nThe other option for creating a one-shot job is using a configuration file, as shown in\\nExample 12-1.\\nExample 12-1. job-oneshot.yaml\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n name: oneshot\\nspec:\\n template:\\n spec:\\n containers:\\n - name: kuard\\n image: gcr.io/kuar-demo/kuard-amd64:blue\\n imagePullPolicy: Always\\nJob Patterns | 141\\n args:\\n - \"--keygen-enable\"\\n - \"--keygen-exit-on-complete\"\\n - \"--keygen-num-to-gen=10\"\\n restartPolicy: OnFailure\\nSubmit the job using the kubectl apply command:\\n$ kubectl apply -f job-oneshot.yaml\\njob \"oneshot\" created\\nThen describe the oneshot job:\\n$ kubectl describe jobs oneshot\\nName: oneshot\\nNamespace: default\\nImage(s): gcr.io/kuar-demo/kuard-amd64:blue\\nSelector: controller-uid=cf87484b-e664-11e6-8222-42010a8a007b\\nParallelism: 1\\nCompletions: 1\\nStart Time: Sun, 29 Jan 2017 12:52:13 -0800\\nLabels: Job=oneshot\\nPods Statuses: 0 Running / 1 Succeeded / 0 Failed\\nNo volumes.\\nEvents:\\n ... Reason Message\\n ... ------ -------\\n ... SuccessfulCreate Created pod: oneshot-4kfdt\\nYou can view the results of the job by looking at the logs of the Pod that was created:\\n$ kubectl logs oneshot-4kfdt\\n...\\nServing on :8080\\n(ID 0) Workload starting\\n(ID 0 1/10) Item done: SHA256:+r6b4W81DbEjxMcD3LHjU+EIGnLEzbpxITKn8IqhkPI\\n(ID 0 2/10) Item done: SHA256:mzHewajaY1KA8VluSLOnNMk9fDE5zdn7vvBS5Ne8AxM\\n(ID 0 3/10) Item done: SHA256:TRtEQHfflJmwkqnNyGgQm/IvXNykSBIg8c03h0g3onE\\n(ID 0 4/10) Item done: SHA256:tSwPYH/J347il/mgqTxRRdeZcOazEtgZlA8A3/HWbro\\n(ID 0 5/10) Item done: SHA256:IP8XtguJ6GbWwLHqjKecVfdS96B17nnO21I/TNc1j9k\\n(ID 0 6/10) Item done: SHA256:ZfNxdQvuST/6ZzEVkyxdRG98p73c/5TM99SEbPeRWfc\\n(ID 0 7/10) Item done: SHA256:tH+CNl/IUl/HUuKdMsq2XEmDQ8oAvmhMO6Iwj8ZEOj0\\n(ID 0 8/10) Item done: SHA256:3GfsUaALVEHQcGNLBOu4Qd1zqqqJ8j738i5r+I5XwVI\\n(ID 0 9/10) Item done: SHA256:5wV4L/xEiHSJXwLUT2fHf0SCKM2g3XH3sVtNbgskCXw\\n(ID 0 10/10) Item done: SHA256:bPqqOonwSbjzLqe9ZuVRmZkz+DBjaNTZ9HwmQhbdWLI\\n(ID 0) Workload exiting\\nCongratulations, your job has run successfully!\\n142 | Chapter 12: Jobs\\nYou may have noticed that we didn’t specify any labels when creat‐\\ning the Job object. Like with other controllers (DaemonSets, Repli‐\\ncaSets, deployments, etc.) that use labels to identify a set of Pods,\\nunexpected behaviors can happen if a Pod is reused across objects.\\nBecause jobs have a finite beginning and ending, it is common for\\nusers to create many of them. This makes picking unique labels\\nmore difficult and more critical. For this reason, the Job object will\\nautomatically pick a unique label and use it to identify the Pods it\\ncreates. In advanced scenarios (such as swapping out a running job\\nwithout killing the Pods it is managing), users can choose to turn\\noff this automatic behavior and manually specify labels and\\nselectors.\\nPod failure\\nWe just saw how a job can complete successfully. But what happens if something\\nfails? Let’s try that out and see what happens.\\nLet’s modify the arguments to kuard in our configuration file to cause it to fail out\\nwith a nonzero exit code after generating three keys, as shown in Example 12-2.\\nExample 12-2. job-oneshot-failure1.yaml\\n...\\nspec:\\n template:\\n spec:\\n containers:\\n ...\\n args:\\n - \"--keygen-enable\"\\n - \"--keygen-exit-on-complete\"\\n - \"--keygen-exit-code=1\"\\n - \"--keygen-num-to-gen=3\"\\n...\\nNow launch this with kubectl apply -f job-oneshot-failure1.yaml. Let it run\\nfor a bit and then look at the Pod status:\\n$ kubectl get pod -a -l job-name=oneshot\\nNAME READY STATUS RESTARTS AGE\\noneshot-3ddk0 0/1 CrashLoopBackOff 4 3m\\nHere we see that the same Pod has restarted four times. Kubernetes is in CrashLoop\\nBackOff for this Pod. It is not uncommon to have a bug someplace that causes a pro‐\\ngram to crash as soon as it starts. In that case, Kubernetes will wait a bit before\\nJob Patterns | 143\\nrestarting the Pod to avoid a crash loop eating resources on the node. This is all han‐\\ndled local to the node by the kubelet without the job being involved at all.\\nKill the job (kubectl delete jobs oneshot), and let’s try something else. Modify the\\nconfig file again and change the restartPolicy from OnFailure to Never. Launch\\nthis with kubectl apply -f jobs-oneshot-failure2.yaml.\\nIf we let this run for a bit and then look at related Pods we’ll find something\\ninteresting:\\n$ kubectl get pod -l job-name=oneshot -a\\nNAME READY STATUS RESTARTS AGE\\noneshot-0wm49 0/1 Error 0 1m\\noneshot-6h9s2 0/1 Error 0 39s\\noneshot-hkzw0 1/1 Running 0 6s\\noneshot-k5swz 0/1 Error 0 28s\\noneshot-m1rdw 0/1 Error 0 19s\\noneshot-x157b 0/1 Error 0 57s\\nWhat we see is that we have multiple Pods here that have errored out. By setting\\nrestartPolicy: Never we are telling the kubelet not to restart the Pod on failure,\\nbut rather just declare the Pod as failed. The Job object then notices and creates a\\nreplacement Pod. If you aren’t careful, this’ll create a lot of “junk” in your cluster. For\\nthis reason, we suggest you use restartPolicy: OnFailure so failed Pods are rerun\\nin place.\\nClean this up with kubectl delete jobs oneshot.\\nSo far we’ve seen a program fail by exiting with a nonzero exit code. But workers can\\nfail in other ways. Specifically, they can get stuck and not make any forward progress.\\nTo help cover this case, you can use liveness probes with jobs. If the liveness probe\\npolicy determines that a Pod is dead, it’ll be restarted/replaced for you.\\nParallelism\\nGenerating keys can be slow. Let’s start a bunch of workers together to make key gen‐\\neration faster. We’re going to use a combination of the completions and\\nparallelism parameters. Our goal is to generate 100 keys by having 10 runs of\\nkuard with each run generating 10 keys. But we don’t want to swamp our cluster, so\\nwe’ll limit ourselves to only five Pods at a time.\\nThis translates to setting completions to 10 and parallelism to 5. The config is\\nshown in Example 12-3.\\n144 | Chapter 12: Jobs\\nExample 12-3. job-parallel.yaml\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n name: parallel\\n labels:\\n chapter: jobs\\nspec:\\n parallelism: 5\\n completions: 10\\n template:\\n metadata:\\n labels:\\n chapter: jobs\\n spec:\\n containers:\\n - name: kuard\\n image: gcr.io/kuar-demo/kuard-amd64:blue\\n imagePullPolicy: Always\\n args:\\n - \"--keygen-enable\"\\n - \"--keygen-exit-on-complete\"\\n - \"--keygen-num-to-gen=10\"\\n restartPolicy: OnFailure\\nStart it up:\\n$ kubectl apply -f job-parallel.yaml\\njob \"parallel\" created\\nNow watch as the Pods come up, do their thing, and exit. New Pods are created until\\n10 have completed altogether. Here we use the --watch flag to have kubectl stay\\naround and list changes as they happen:\\n$ kubectl get pods -w\\nNAME READY STATUS RESTARTS AGE\\nparallel-55tlv 1/1 Running 0 5s\\nparallel-5s7s9 1/1 Running 0 5s\\nparallel-jp7bj 1/1 Running 0 5s\\nparallel-lssmn 1/1 Running 0 5s\\nparallel-qxcxp 1/1 Running 0 5s\\nNAME READY STATUS RESTARTS AGE\\nparallel-jp7bj 0/1 Completed 0 26s\\nparallel-tzp9n 0/1 Pending 0 0s\\nparallel-tzp9n 0/1 Pending 0 0s\\nparallel-tzp9n 0/1 ContainerCreating 0 1s\\nparallel-tzp9n 1/1 Running 0 1s\\nparallel-tzp9n 0/1 Completed 0 48s\\nparallel-x1kmr 0/1 Pending 0 0s\\nparallel-x1kmr 0/1 Pending 0 0s\\nparallel-x1kmr 0/1 ContainerCreating 0 0s\\nJob Patterns | 145\\nparallel-x1kmr 1/1 Running 0 1s\\nparallel-5s7s9 0/1 Completed 0 1m\\nparallel-tprfj 0/1 Pending 0 0s\\nparallel-tprfj 0/1 Pending 0 0s\\nparallel-tprfj 0/1 ContainerCreating 0 0s\\nparallel-tprfj 1/1 Running 0 2s\\nparallel-x1kmr 0/1 Completed 0 52s\\nparallel-bgvz5 0/1 Pending 0 0s\\nparallel-bgvz5 0/1 Pending 0 0s\\nparallel-bgvz5 0/1 ContainerCreating 0 0s\\nparallel-bgvz5 1/1 Running 0 2s\\nparallel-qxcxp 0/1 Completed 0 2m\\nparallel-xplw2 0/1 Pending 0 1s\\nparallel-xplw2 0/1 Pending 0 1s\\nparallel-xplw2 0/1 ContainerCreating 0 1s\\nparallel-xplw2 1/1 Running 0 3s\\nparallel-bgvz5 0/1 Completed 0 40s\\nparallel-55tlv 0/1 Completed 0 2m\\nparallel-lssmn 0/1 Completed 0 2m\\nFeel free to study the completed jobs and check out their logs to see the fingerprints\\nof the keys they generated. Clean up by deleting the finished Job object with kubectl\\ndelete job parallel.\\nWork Queues\\nA common use case for jobs is to process work from a work queue. In this scenario,\\nsome task creates a number of work items and publishes them to a work queue. A\\nworker job can be run to process each work item until the work queue is empty\\n(Figure 12-1).\\nFigure 12-1. Parallel jobs\\nStarting a work queue\\nWe start by launching a centralized work queue service. kuard has a simple memorybased work queue system built in. We will start an instance of kuard to act as a coor‐\\ndinator for all the work to be done.\\nNext, we create a simple ReplicaSet to manage a singleton work queue daemon. We\\nare using a ReplicaSet to ensure that a new Pod will get created in the face of machine\\nfailure, as shown in Example 12-4.\\n146 | Chapter 12: Jobs\\nExample 12-4. rs-queue.yaml\\napiVersion: extensions/v1beta1\\nkind: ReplicaSet\\nmetadata:\\n labels:\\n app: work-queue\\n component: queue\\n chapter: jobs\\n name: queue\\nspec:\\n replicas: 1\\n template:\\n metadata:\\n labels:\\n app: work-queue\\n component: queue\\n chapter: jobs\\n spec:\\n containers:\\n - name: queue\\n image: \"gcr.io/kuar-demo/kuard-amd64:blue\"\\n imagePullPolicy: Always\\nRun the work queue with the following command:\\n$ kubectl apply -f rs-queue.yaml\\nAt this point the work queue daemon should be up and running. Let’s use port for‐\\nwarding to connect to it. Leave this command running in a terminal window:\\n$ QUEUE_POD=$(kubectl get pods -l app=work-queue,component=queue \\\\\\n -o jsonpath=\\'{.items[0].metadata.name}\\')\\n$ kubectl port-forward $QUEUE_POD 8080:8080\\nForwarding from 127.0.0.1:8080 -> 8080\\nForwarding from [::1]:8080 -> 8080\\nYou can open your browser to http://localhost:8080 and see the kuard interface.\\nSwitch to the “MemQ Server” tab to keep an eye on what is going on.\\nWith the work queue server in place, we should expose it using a service. This will\\nmake it easy for producers and consumers to locate the work queue via DNS, as\\nExample 12-5 shows.\\nExample 12-5. service-queue.yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n labels:\\n app: work-queue\\n component: queue\\nJob Patterns | 147\\n chapter: jobs\\n name: queue\\nspec:\\n ports:\\n - port: 8080\\n protocol: TCP\\n targetPort: 8080\\n selector:\\n app: work-queue\\n component: queue\\nCreate the queue service with kubectl:\\n$ kubectl apply -f service-queue.yaml\\nservice \"queue\" created\\nLoading up the queue\\nWe are now ready to put a bunch of work items in the queue. For the sake of simplic‐\\nity we’ll just use curl to drive the API for the work queue server and insert a bunch\\nof work items. curl will communicate to the work queue through the kubectl portforward we set up earlier, as shown in Example 12-6.\\nExample 12-6. load-queue.sh\\n# Create a work queue called \\'keygen\\'\\ncurl -X PUT localhost:8080/memq/server/queues/keygen\\n# Create 100 work items and load up the queue.\\nfor i in work-item-{0..99}; do\\n curl -X POST localhost:8080/memq/server/queues/keygen/enqueue \\\\\\n -d \"$i\"\\ndone\\nRun these commands, and you should see 100 JSON objects output to your terminal\\nwith a unique message identifier for each work item. You can confirm the status of\\nthe queue by looking at the “MemQ Server” tab in the UI, or you can ask the work\\nqueue API directly:\\n$ curl 127.0.0.1:8080/memq/server/stats\\n{\\n \"kind\": \"stats\",\\n \"queues\": [\\n {\\n \"depth\": 100,\\n \"dequeued\": 0,\\n \"drained\": 0,\\n \"enqueued\": 100,\\n \"name\": \"keygen\"\\n }\\n148 | Chapter 12: Jobs\\n ]\\n}\\nNow we are ready to kick off a job to consume the work queue until it’s empty.\\nCreating the consumer job\\nThis is where things get interesting! kuard is also able to act in consumer mode. We\\ncan set it up to draw work items from the work queue, create a key, and then exit\\nonce the queue is empty, as shown in Example 12-7.\\nExample 12-7. job-consumers.yaml\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n labels:\\n app: message-queue\\n component: consumer\\n chapter: jobs\\n name: consumers\\nspec:\\n parallelism: 5\\n template:\\n metadata:\\n labels:\\n app: message-queue\\n component: consumer\\n chapter: jobs\\n spec:\\n containers:\\n - name: worker\\n image: \"gcr.io/kuar-demo/kuard-amd64:blue\"\\n imagePullPolicy: Always\\n args:\\n - \"--keygen-enable\"\\n - \"--keygen-exit-on-complete\"\\n - \"--keygen-memq-server=http://queue:8080/memq/server\"\\n - \"--keygen-memq-queue=keygen\"\\n restartPolicy: OnFailure\\nHere, we are telling the job to start up five Pods in parallel. As the completions\\nparameter is unset, we put the job into a worker pool mode. Once the first Pod exits\\nwith a zero exit code, the job will start winding down and will not start any new Pods.\\nThis means that none of the workers should exit until the work is done and they are\\nall in the process of finishing up.\\nJob Patterns | 149\\nNow, create the consumers job:\\n$ kubectl apply -f job-consumers.yaml\\njob \"consumers\" created\\nOnce the job has been created, you can view the Pods backing the job:\\n$ kubectl get pods\\nNAME READY STATUS RESTARTS AGE\\nqueue-43s87 1/1 Running 0 5m\\nconsumers-6wjxc 1/1 Running 0 2m\\nconsumers-7l5mh 1/1 Running 0 2m\\nconsumers-hvz42 1/1 Running 0 2m\\nconsumers-pc8hr 1/1 Running 0 2m\\nconsumers-w20cc 1/1 Running 0 2m\\nNote there are five Pods running in parallel. These Pods will continue to run until the\\nwork queue is empty. You can watch as it happens in the UI on the work queue server.\\nAs the queue empties, the consumer Pods will exit cleanly and the consumers job will\\nbe considered complete.\\nCleaning up\\nUsing labels, we can clean up all of the stuff we created in this section:\\n$ kubectl delete rs,svc,job -l chapter=jobs\\nCronJobs\\nSometimes you want to schedule a job to be run at a certain interval. To achieve this\\nyou can declare a CronJob in Kubernetes, which is responsible for creating a new Job\\nobject at a particular interval. The declaration of a CronJob looks like:\\napiVersion: batch/v1beta1\\nkind: CronJob\\nmetadata:\\n name: example-cron\\nspec:\\n # Run every fifth hour\\n schedule: \"0 */5 * * *\"\\n jobTemplate:\\n spec:\\n template:\\n spec:\\n containers:\\n - name: batch-job\\n image: my-batch-image\\n restartPolicy: OnFailure\\nNote the spec.schedule field, which contains the interval for the CronJob in stan‐\\ndard cron format.\\n150 | Chapter 12: Jobs\\nYou can save this file as cron-job.yaml, and create the CronJob with kubectl create\\n-f cron-job.yaml. If you are interested in the current state of a CronJob, you can use\\nkubectl describe <cron-job> to get the details.\\nSummary\\nOn a single cluster, Kubernetes can handle both long-running workloads such as web\\napplications and short-lived workloads such as batch jobs. The job abstraction allows\\nyou to model batch job patterns ranging from simple one-time tasks to parallel jobs\\nthat process many items until work has been exhausted.\\nJobs are a low-level primitive and can be used directly for simple workloads. How‐\\never, Kubernetes is built from the ground up to be extensible by higher-level objects.\\nJobs are no exception; they can easily be used by higher-level orchestration systems to\\ntake on more complex tasks.\\nSummary | 151\\n\\nCHAPTER 13\\nCon\\x80gMaps and Secrets\\nIt is a good practice to make container images as reusable as possible. The same\\nimage should be able to be used for development, staging, and production. It is even\\nbetter if the same image is general-purpose enough to be used across applications and\\nservices. Testing and versioning get riskier and more complicated if images need to be\\nrecreated for each new environment. But then how do we specialize the use of that\\nimage at runtime?\\nThis is where ConfigMaps and secrets come into play. ConfigMaps are used to pro‐\\nvide configuration information for workloads. This can either be fine-grained infor‐\\nmation (a short string) or a composite value in the form of a file. Secrets are similar to\\nConfigMaps but focused on making sensitive information available to the workload.\\nThey can be used for things like credentials or TLS certificates.\\nCon\\x80gMaps\\nOne way to think of a ConfigMap is as a Kubernetes object that defines a small file‐\\nsystem. Another way is as a set of variables that can be used when defining the envi‐\\nronment or command line for your containers. The key thing is that the ConfigMap\\nis combined with the Pod right before it is run. This means that the container image\\nand the Pod definition itself can be reused across many apps by just changing the\\nConfigMap that is used.\\nCreating Con\\x80gMaps\\nLet’s jump right in and create a ConfigMap. Like many objects in Kubernetes, you can\\ncreate these in an immediate, imperative way, or you can create them from a manifest\\non disk. We’ll start with the imperative method.\\n153\\nFirst, suppose we have a file on disk (called my-config.txt) that we want to make avail‐\\nable to the Pod in question, as shown in Example 13-1.\\nExample 13-1. my-config.txt\\n# This is a sample config file that I might use to configure an application\\nparameter1 = value1\\nparameter2 = value2\\nNext, let’s create a ConfigMap with that file. We’ll also add a couple of simple key/\\nvalue pairs here. These are referred to as literal values on the command line:\\n$ kubectl create configmap my-config \\\\\\n --from-file=my-config.txt \\\\\\n --from-literal=extra-param=extra-value \\\\\\n --from-literal=another-param=another-value\\nThe equivalent YAML for the ConfigMap object we just created is:\\n$ kubectl get configmaps my-config -o yaml\\napiVersion: v1\\ndata:\\n another-param: another-value\\n extra-param: extra-value\\n my-config.txt: |\\n # This is a sample config file that I might use to configure an application\\n parameter1 = value1\\n parameter2 = value2\\nkind: ConfigMap\\nmetadata:\\n creationTimestamp: ...\\n name: my-config\\n namespace: default\\n resourceVersion: \"13556\"\\n selfLink: /api/v1/namespaces/default/configmaps/my-config\\n uid: 3641c553-f7de-11e6-98c9-06135271a273\\nAs you can see, the ConfigMap is really just some key/value pairs stored in an object.\\nThe interesting stuff happens when you try to use a ConfigMap.\\nUsing a Con\\x80gMap\\nThere are three main ways to use a ConfigMap:\\nFilesystem\\nYou can mount a ConfigMap into a Pod. A file is created for each entry based on\\nthe key name. The contents of that file are set to the value.\\n154 | Chapter 13: Con\\x80gMaps and Secrets\\nEnvironment variable\\nA ConfigMap can be used to dynamically set the value of an environment\\nvariable.\\nCommand-line argument\\nKubernetes supports dynamically creating the command line for a container\\nbased on ConfigMap values.\\nLet’s create a manifest for kuard that pulls all of these together, as shown in\\nExample 13-2.\\nExample 13-2. kuard-config.yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n name: kuard-config\\nspec:\\n containers:\\n - name: test-container\\n image: gcr.io/kuar-demo/kuard-amd64:blue\\n imagePullPolicy: Always\\n command:\\n - \"/kuard\"\\n - \"$(EXTRA_PARAM)\"\\n env:\\n - name: ANOTHER_PARAM\\n valueFrom:\\n configMapKeyRef:\\n name: my-config\\n key: another-param\\n - name: EXTRA_PARAM\\n valueFrom:\\n configMapKeyRef:\\n name: my-config\\n key: extra-param\\n volumeMounts:\\n - name: config-volume\\n mountPath: /config\\n volumes:\\n - name: config-volume\\n configMap:\\n name: my-config\\n restartPolicy: Never\\nFor the filesystem method, we create a new volume inside the Pod and give it the\\nname config-volume. We then define this volume to be a ConfigMap volume and\\npoint at the ConfigMap to mount. We have to specify where this gets mounted into\\nthe kuard container with a volumeMount. In this case we are mounting it at /config.\\nCon\\x80gMaps | 155\\nEnvironment variables are specified with a special valueFrom member. This refer‐\\nences the ConfigMap and the data key to use within that ConfigMap.\\nCommand-line arguments build on environment variables. Kubernetes will perform\\nthe correct substitution with a special $(<env-var-name>) syntax.\\nRun this Pod, and let’s port forward to examine how the app sees the world:\\n$ kubectl apply -f kuard-config.yaml\\n$ kubectl port-forward kuard-config 8080\\nNow point your browser at http://localhost:8080. We can look at how we’ve injected\\nconfiguration values into the program in all three ways.\\nClick the “Server Env” tab on the left. This will show the command line that the app\\nwas launched with along with its environment, as shown in Figure 13-1.\\nFigure 13-1. kuard showing its environment\\n156 | Chapter 13: Con\\x80gMaps and Secrets\\nHere we can see that we’ve added two environment variables (ANOTHER_PARAM and\\nEXTRA_PARAM) whose values are set via the ConfigMap. Furthermore, we’ve added an\\nargument to the command line of kuard based on the EXTRA_PARAM value.\\nNext, click the “File system browser” tab (Figure 13-2). This lets you explore the file‐\\nsystem as the application sees it. You should see an entry called /config. This is a vol‐\\nume created based on our ConfigMap. If you navigate into that, you’ll see that a file\\nhas been created for each entry of the ConfigMap. You’ll also see some hidden files\\n(prepended with ..) that are used to do a clean swap of new values when the Config‐\\nMap is updated.\\nFigure 13-2. The /config directory as seen through kuard\\nSecrets\\nWhile ConfigMaps are great for most configuration data, there is certain data that is\\nextra-sensitive. This can include passwords, security tokens, or other types of private\\nSecrets | 157\\nkeys. Collectively, we call this type of data “secrets.” Kubernetes has native support for\\nstoring and handling this data with care.\\nSecrets enable container images to be created without bundling sensitive data. This\\nallows containers to remain portable across environments. Secrets are exposed to\\nPods via explicit declaration in Pod manifests and the Kubernetes API. In this way,\\nthe Kubernetes secrets API provides an application-centric mechanism for exposing\\nsensitive configuration information to applications in a way that’s easy to audit and\\nleverages native OS isolation primitives.\\nBy default, Kubernetes secrets are stored in plain text in the etcd\\nstorage for the cluster. Depending on your requirements, this may\\nnot be sufficient security for you. In particular, anyone who has\\ncluster administration rights in your cluster will be able to read all\\nof the secrets in the cluster. In recent versions of Kubernetes, sup‐\\nport has been added for encrypting the secrets with a user-supplied\\nkey, generally integrated into a cloud key store. Additionally, most\\ncloud key stores have integration with Kubernetes flexible volumes,\\nenabling you to skip Kubernetes secrets entirely and rely exclu‐\\nsively on the cloud provider’s key store. All of these options should\\nprovide you with sufficient tools to craft a security profile that suits\\nyour needs.\\nThe remainder of this section will explore how to create and manage Kubernetes\\nsecrets, and also lay out best practices for exposing secrets to Pods that require them.\\nCreating Secrets\\nSecrets are created using the Kubernetes API or the kubectl command-line tool.\\nSecrets hold one or more data elements as a collection of key/value pairs.\\nIn this section we will create a secret to store a TLS key and certificate for the kuard\\napplication that meets the storage requirements listed previously.\\nThe kuard container image does not bundle a TLS certificate or\\nkey. This allows the kuard container to remain portable across\\nenvironments and distributable through public Docker\\nrepositories.\\nThe first step in creating a secret is to obtain the raw data we want to store. The TLS\\nkey and certificate for the kuard application can be downloaded by running the fol‐\\nlowing commands:\\n$ curl -o kuard.crt https://storage.googleapis.com/kuar-demo/kuard.crt\\n$ curl -o kuard.key https://storage.googleapis.com/kuar-demo/kuard.key\\n158 | Chapter 13: Con\\x80gMaps and Secrets\\nThese certificates are shared with the world and they provide no\\nactual security. Please do not use them except as a learning tool in\\nthese examples.\\nWith the kuard.crt and kuard.key files stored locally, we are ready to create a secret.\\nCreate a secret named kuard-tls using the create secret command:\\n$ kubectl create secret generic kuard-tls \\\\\\n --from-file=kuard.crt \\\\\\n --from-file=kuard.key\\nThe kuard-tls secret has been created with two data elements. Run the following\\ncommand to get details:\\n$ kubectl describe secrets kuard-tls\\nName: kuard-tls\\nNamespace: default\\nLabels: <none>\\nAnnotations: <none>\\nType: Opaque\\nData\\n====\\nkuard.crt: 1050 bytes\\nkuard.key: 1679 bytes\\nWith the kuard-tls secret in place, we can consume it from a Pod by using a secrets\\nvolume.\\nConsuming Secrets\\nSecrets can be consumed using the Kubernetes REST API by applications that know\\nhow to call that API directly. However, our goal is to keep applications portable. Not\\nonly should they run well in Kubernetes, but they should run, unmodified, on other\\nplatforms.\\nInstead of accessing secrets through the API server, we can use a secrets volume.\\nSecrets volumes\\nSecret data can be exposed to Pods using the secrets volume type. Secrets volumes are\\nmanaged by the kubelet and are created at Pod creation time. Secrets are stored on\\ntmpfs volumes (aka RAM disks), and as such are not written to disk on nodes.\\nEach data element of a secret is stored in a separate file under the target mount point\\nspecified in the volume mount. The kuard-tls secret contains two data elements:\\nSecrets | 159\\nkuard.crt and kuard.key. Mounting the kuard-tls secrets volume to /tls results in\\nthe following files:\\n/tls/kuard.crt\\n/tls/kuard.key\\nThe Pod manifest in Example 13-3 demonstrates how to declare a secrets volume,\\nwhich exposes the kuard-tls secret to the kuard container under /tls.\\nExample 13-3. kuard-secret.yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n name: kuard-tls\\nspec:\\n containers:\\n - name: kuard-tls\\n image: gcr.io/kuar-demo/kuard-amd64:blue\\n imagePullPolicy: Always\\n volumeMounts:\\n - name: tls-certs\\n mountPath: \"/tls\"\\n readOnly: true\\n volumes:\\n - name: tls-certs\\n secret:\\n secretName: kuard-tls\\nCreate the kuard-tls Pod using kubectl and observe the log output from the run‐\\nning Pod:\\n$ kubectl apply -f kuard-secret.yaml\\nConnect to the Pod by running:\\n$ kubectl port-forward kuard-tls 8443:8443\\nNow navigate your browser to https://localhost:8443. You should see some invalid cer‐\\ntificate warnings as this is a self-signed certificate for kuard.example.com. If you navi‐\\ngate past this warning, you should see the kuard server hosted via HTTPS. Use the\\n“File system browser” tab to find the certificates on disk.\\nPrivate Docker Registries\\nA special use case for secrets is to store access credentials for private Docker regis‐\\ntries. Kubernetes supports using images stored on private registries, but access to\\nthose images requires credentials. Private images can be stored across one or more\\nprivate registries. This presents a challenge for managing credentials for each private\\nregistry on every possible node in the cluster.\\n160 | Chapter 13: Con\\x80gMaps and Secrets\\nImage pull secrets leverage the secrets API to automate the distribution of private reg‐\\nistry credentials. Image pull secrets are stored just like normal secrets but are con‐\\nsumed through the spec.imagePullSecrets Pod specification field.\\nUse the create secret docker-registry to create this special kind of secret:\\n$ kubectl create secret docker-registry my-image-pull-secret \\\\\\n --docker-username=<username> \\\\\\n --docker-password=<password> \\\\\\n --docker-email=<email-address>\\nEnable access to the private repository by referencing the image pull secret in the Pod\\nmanifest file, as shown in Example 13-4.\\nExample 13-4. kuard-secret-ips.yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n name: kuard-tls\\nspec:\\n containers:\\n - name: kuard-tls\\n image: gcr.io/kuar-demo/kuard-amd64:blue\\n imagePullPolicy: Always\\n volumeMounts:\\n - name: tls-certs\\n mountPath: \"/tls\"\\n readOnly: true\\n imagePullSecrets:\\n - name: my-image-pull-secret\\n volumes:\\n - name: tls-certs\\n secret:\\n secretName: kuard-tls\\nIf you are repeatedly pulling from the same registry, you can add the secrets to the\\ndefault service account associated with each Pod to avoid having to specify the secrets\\nin every Pod you create.\\nNaming Constraints\\nThe key names for data items inside of a secret or ConfigMap are defined to map to\\nvalid environment variable names. They may begin with a dot followed by a letter or\\nnumber. Following characters include dots, dashes, and underscores. Dots cannot be\\nrepeated and dots and underscores or dashes cannot be adjacent to each other.\\nMore formally, this means that they must conform to the regular expression\\nNaming Constraints | 161\\n^[.[?[a-zAZ0-9[([.[?[a-zA-Z0-9[+[-_a-zA-Z0-9[?)*$. Some examples of valid\\nand invalid names for ConfigMaps and secrets are given in Table 13-1.\\nTable 13-1. ConfigMap and secret key examples\\nValid key name Invalid key name\\n.auth_token Token..properties\\nKey.pem auth file.json\\nconfig_file _password.txt\\nWhen selecting a key name, consider that these keys can be\\nexposed to Pods via a volume mount. Pick a name that is going to\\nmake sense when specified on a command line or in a config file.\\nStoring a TLS key as key.pem is more clear than tls-key when\\nconfiguring applications to access secrets.\\nConfigMap data values are simple UTF-8 text specified directly in the manifest. As of\\nKubernetes 1.6, ConfigMaps are unable to store binary data.\\nSecret data values hold arbitrary data encoded using base64. The use of base64\\nencoding makes it possible to store binary data. This does, however, make it more\\ndifficult to manage secrets that are stored in YAML files as the base64-encoded value\\nmust be put in the YAML. Note that the maximum size for a ConfigMap or secret is\\n1 MB.\\nManaging Con\\x80gMaps and Secrets\\nSecrets and ConfigMaps are managed through the Kubernetes API. The usual\\ncreate, delete, get, and describe commands work for manipulating these objects.\\nListing\\nYou can use the kubectl get secrets command to list all secrets in the current\\nnamespace:\\n$ kubectl get secrets\\nNAME TYPE DATA AGE\\ndefault-token-f5jq2 kubernetes.io/service-account-token 3 1h\\nkuard-tls Opaque 2 20m\\nSimilarly, you can list all of the ConfigMaps in a namespace:\\n$ kubectl get configmaps\\nNAME DATA AGE\\nmy-config 3 1m\\n162 | Chapter 13: Con\\x80gMaps and Secrets\\nkubectl describe can be used to get more details on a single object:\\n$ kubectl describe configmap my-config\\nName: my-config\\nNamespace: default\\nLabels: <none>\\nAnnotations: <none>\\nData\\n====\\nanother-param: 13 bytes\\nextra-param: 11 bytes\\nmy-config.txt: 116 bytes\\nFinally, you can see the raw data (including values in secrets!) with something like\\nkubectl get configmap my-config -o yaml or kubectl get secret kuard-tls\\n-o yaml.\\nCreating\\nThe easiest way to create a secret or a ConfigMap is via kubectl create secret\\ngeneric or kubectl create configmap. There are a variety of ways to specify the\\ndata items that go into the secret or ConfigMap. These can be combined in a single\\ncommand:\\n--from-file=<filename>\\nLoad from the file with the secret data key the same as the filename.\\n--from-file=<key>=<filename>\\nLoad from the file with the secret data key explicitly specified.\\n--from-file=<directory>\\nLoad all the files in the specified directory where the filename is an acceptable\\nkey name.\\n--from-literal=<key>=<value>\\nUse the specified key/value pair directly.\\nUpdating\\nYou can update a ConfigMap or secret and have it reflected in running programs.\\nThere is no need to restart if the application is configured to reread configuration val‐\\nues. This is a rare feature but might be something you put in your own applications.\\nThe following are three ways to update ConfigMaps or secrets.\\nManaging Con\\x80gMaps and Secrets | 163\\nUpdate from \\x80le\\nIf you have a manifest for your ConfigMap or secret, you can just edit it directly and\\npush a new version with kubectl replace -f <filename>. You can also use kubectl\\napply -f <filename> if you previously created the resource with kubectl apply.\\nDue to the way that datafiles are encoded into these objects, updating a configuration\\ncan be a bit cumbersome as there is no provision in kubectl to load data from an\\nexternal file. The data must be stored directly in the YAML manifest.\\nThe most common use case is when the ConfigMap is defined as part of a directory\\nor list of resources and everything is created and updated together. Oftentimes these\\nmanifests will be checked into source control.\\nIt is generally a bad idea to check secret YAML files into source\\ncontrol. It is too easy to push these files someplace public and leak\\nyour secrets.\\nRecreate and update\\nIf you store the inputs into your ConfigMaps or secrets as separate files on disk (as\\nopposed to embedded into YAML directly), you can use kubectl to recreate the man‐\\nifest and then use it to update the object.\\nThis will look something like this:\\n$ kubectl create secret generic kuard-tls \\\\\\n --from-file=kuard.crt --from-file=kuard.key \\\\\\n --dry-run -o yaml | kubectl replace -f -\\nThis command line first creates a new secret with the same name as our existing\\nsecret. If we just stopped there, the Kubernetes API server would return an error\\ncomplaining that we are trying to create a secret that already exists. Instead, we tell\\nkubectl not to actually send the data to the server but instead to dump the YAML\\nthat it would have sent to the API server to stdout. We then pipe that to kubectl\\nreplace and use -f - to tell it to read from stdin. In this way, we can update a secret\\nfrom files on disk without having to manually base64-encode data.\\nEdit current version\\nThe final way to update a ConfigMap is to use kubectl edit to bring up a version of\\nthe ConfigMap in your editor so you can tweak it (you could also do this with a\\nsecret, but you’d be stuck managing the base64 encoding of values on your own):\\n$ kubectl edit configmap my-config\\n164 | Chapter 13: Con\\x80gMaps and Secrets\\nYou should see the ConfigMap definition in your editor. Make your desired changes\\nand then save and close your editor. The new version of the object will be pushed to\\nthe Kubernetes API server.\\nLive updates\\nOnce a ConfigMap or secret is updated using the API, it’ll be automatically pushed to\\nall volumes that use that ConfigMap or secret. It may take a few seconds, but the file\\nlisting and contents of the files, as seen by kuard, will be updated with these new val‐\\nues. Using this live update feature you can update the configuration of applications\\nwithout restarting them.\\nCurrently there is no built-in way to signal an application when a new version of a\\nConfigMap is deployed. It is up to the application (or some helper script) to look for\\nthe config files to change and reload them.\\nUsing the file browser in kuard (accessed through kubectl port-forward) is a great\\nway to interactively play with dynamically updating secrets and ConfigMaps.\\nSummary\\nConfigMaps and secrets are a great way to provide dynamic configuration in your\\napplication. They allow you to create a container image (and Pod definition) once\\nand reuse it in different contexts. This can include using the exact same image as you\\nmove from dev to staging to production. It can also include using a single image\\nacross multiple teams and services. Separating configuration from application code\\nwill make your applications more reliable and reusable.\\nSummary | 165\\n\\nCHAPTER 14\\nRole-Based Access Control for Kubernetes\\nAt this point, nearly every Kubernetes cluster you encounter has role-based access\\ncontrol (RBAC) enabled. So you likely have at least partially encountered RBAC\\nbefore. Perhaps you initially couldn’t access your cluster until you used some magical\\nincantation to add a RoleBinding to map a user to a role. However, even though you\\nmay have had some exposure to RBAC, you may not have had a great deal of experi‐\\nence understanding RBAC in Kubernetes, what it is for, and how to use it successfully.\\nThat is the subject of this chapter.\\nRBAC was introduced into Kubernetes with version 1.5 and became generally avail‐\\nable in Kubernetes 1.8. Role-based access control provides a mechanism for restrict‐\\ning both access to and actions on Kubernetes APIs to ensure that only appropriate\\nusers have access to APIs in the cluster. RBAC is a critical component to both harden\\naccess to the Kubernetes cluster where you are deploying your application and (possi‐\\nbly more importantly) prevent unexpected accidents where one person in the wrong\\nnamespace mistakenly takes down production when they think they are destroying\\ntheir test cluster.\\nMultitenant security in Kubernetes is a complex, multifaceted topic\\nworthy of its own volume. While RBAC can be quite useful in lim‐\\niting access to the Kubernetes API, it’s important to remember that\\nanyone who can run arbitrary code inside the Kubernetes cluster\\ncan effectively obtain root privileges on the entire cluster. There are\\napproaches that you can take to make such attacks harder and\\nmore expensive, and a correct RBAC setup is part of this defense.\\nBut if you are focused on hostile multitenant security, do not\\nbelieve that RBAC by itself is sufficient to protect you. You must\\nisolate the Pods running in your cluster to provide effective multitenant security. Generally this is done with a hypervisor isolated\\ncontainer, or some sort of container sandbox, or both.\\n167\\nBefore we dive into the details of RBAC in Kubernetes, it’s valuable to have a highlevel understanding of RBAC as a concept, as well as authentication and authoriza‐\\ntion more generally.\\nEvery request to Kubernetes is first authenticated. Authentication provides the iden‐\\ntity of the caller issuing the request. It could be as simple as saying that the request is\\nunauthenticated, or it could integrate deeply with a pluggable authentication provider\\n(e.g., Azure Active Directory) to establish an identity within that third-party system.\\nInterestingly enough, Kubernetes does not have a built-in identity store, focusing\\ninstead on integrating other identity sources within itself.\\nOnce users have been properly identified, the authorization phase determines\\nwhether they are authorized to perform the request. Authorization is a combination\\nof the identity of the user, the resource (effectively the HTTP path), and the verb or\\naction the user is attempting to perform. If the particular user is authorized for per‐\\nforming that action on that resource, then the request is allowed to proceed. Other‐\\nwise, an HTTP 403 error is returned. More details of this process are given in the\\nfollowing sections.\\nRole-Based Access Control\\nTo properly manage access in Kubernetes, it’s critical to understand how identity,\\nroles, and role bindings interact to control who can do what with what resources. At\\nfirst, RBAC can seem like a challenge to understand, with a series of interconnected,\\nabstract concepts; but once understood, managing cluster access is straightforward\\nand safe.\\nIdentity in Kubernetes\\nEvery request that comes to Kubernetes is associated with some identity. Even a\\nrequest with no identity is associated with the system:unauthenticated group.\\nKubernetes makes a distinction between user identities and service account identities.\\nService accounts are created and managed by Kubernetes itself and are generally asso‐\\nciated with components running inside the cluster. User accounts are all other\\naccounts associated with actual users of the cluster, and often include automation like\\ncontinuous delivery as a service that runs outside of the cluster.\\nKubernetes uses a generic interface for authentication providers. Each of the provid‐\\ners supplies a username and optionally the set of groups to which the user belongs.\\nKubernetes supports a number of different authentication providers, including:\\n• HTTP Basic Authentication (largely deprecated)\\n• x509 client certificates\\n168 | Chapter 14: Role-Based Access Control for Kubernetes\\n• Static token files on the host\\n• Cloud authentication providers like Azure Active Directory and AWS Identity\\nand Access Management (IAM)\\n• Authentication webhooks\\nWhile most managed Kubernetes installations configure authentication for you, if\\nyou are deploying your own authentication you will need to configure flags on the\\nKubernetes API server appropriately.\\nUnderstanding Roles and Role Bindings\\nIdentity is just the beginning of authorization in Kubernetes. Once the system knows\\nthe identity of the request, it needs to determine if the request is authorized for that\\nuser. To achieve this, it uses the general concept of roles and role bindings.\\nA role is a set of abstract capabilities. For example, the appdev role might represent\\nthe ability to create Pods and services. A role binding is an assignment of a role to one\\nor more identities. Thus, binding the appdev role to the user identity alice indicates\\nthat Alice has the ability to create Pods and services.\\nRoles and Role Bindings in Kubernetes\\nIn Kubernetes there are two pairs of related resources that represent roles and role\\nbindings. One pair applies to just a namespace (Role and RoleBinding) while the\\nother pair applies across the cluster (ClusterRole and ClusterRoleBinding).\\nLet’s examine Role and RoleBinding first. Role resources are namespaced, and repre‐\\nsent capabilities within that single namespace. You cannot use namespaced roles for\\nnon-namespaced resources (e.g., CustomResourceDefinitions), and binding a Role\\nBinding to a role only provides authorization within the Kubernetes namespace that\\ncontains both the Role and the RoleDefinition.\\nAs a concrete example, here is a simple role that gives an identity the ability to create\\nand modify Pods and services:\\nkind: Role\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n namespace: default\\n name: pod-and-services\\nrules:\\n- apiGroups: [\"\"]\\n resources: [\"pods\", \"services\"]\\n verbs: [\"create\", \"delete\", \"get\", \"list\", \"patch\", \"update\", \"watch\"]\\nRole-Based Access Control | 169\\nTo bind this Role to the user alice, we need to create a RoleBinding that looks as\\nfollows. This role binding also binds the group mydevs to the same role:\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n namespace: default\\n name: pods-and-services\\nsubjects:\\n- apiGroup: rbac.authorization.k8s.io\\n kind: User\\n name: alice\\n- apiGroup: rbac.authorization.k8s.io\\n kind: Group\\n name: mydevs\\nroleRef:\\n apiGroup: rbac.authorization.k8s.io\\n kind: Role\\n name: pod-and-services\\nOf course, sometimes you want to create a role that applies to the entire cluster, or\\nyou want to limit access to cluster-level resources. To achieve this, you use the\\nClusterRole and ClusterRoleBinding resources. They are largely identical to their\\nnamespaced peers, but with larger scope.\\nVerbs for Kubernetes roles\\nRoles are defined in terms of both a resource (e.g., “Pods”) and a verb that describes\\nan action that can be performed on that resource. The verbs correspond roughly to\\nHTTP methods. The commonly used verbs in Kubernetes RBAC are listed in\\nTable 14-1.\\nTable 14-1. Common Kubernetes RBAC verbs\\nVerb HTTP method Description\\ncreate POST Create a new resource.\\ndelete DELETE Delete an existing resource.\\nget GET Get a resource.\\nlist GET List a collection of resources.\\npatch PATCH Modify an existing resource via a partial change.\\nupdate PUT Modify an existing resource via a complete object.\\nwatch GET Watch for streaming updates to a resource.\\nproxy GET Connect to resource via a streaming WebSocket proxy.\\n170 | Chapter 14: Role-Based Access Control for Kubernetes\\nUsing built-in roles\\nOf course, designing your own roles can be complicated and time-consuming. Fur‐\\nthermore, Kubernetes has a large number of well-known system identities (e.g., a\\nscheduler) that require a known set of capabilities. Consequently, Kubernetes has a\\nlarge number of built-in cluster roles. You can view these by running:\\n$ kubectl get clusterroles\\nWhile most of these built-in roles are for system utilities, four are designed for\\ngeneric end users:\\n• The cluster-admin role provides complete access to the entire cluster.\\n• The admin role provides complete access to a complete namespace.\\n• The edit role allows an end user to modify things in a namespace.\\n• The view role allows for read-only access to a namespace.\\nMost clusters already have numerous ClusterRole bindings set up, and you can view\\nthese bindings with kubectl get clusterrolebindings.\\nAuto-reconciliation of built-in roles\\nWhen the Kubernetes API server starts up, it automatically installs a number of\\ndefault ClusterRoles that are defined in the code of the API server itself. This means\\nthat if you modify any built-in cluster role, those modifications are transient.\\nWhenever the API server is restarted (e.g., for an upgrade) your changes will be\\noverwritten.\\nTo prevent this from happening, before you make any other modifications you need\\nto add the rbac.authorization.kubernetes.io/autoupdate annotation with a\\nvalue of false to the built-in ClusterRole resource. If this annotation is set to false,\\nthe API server will not overwrite the modified ClusterRole resource.\\nBy default, the Kubernetes API server installs a cluster role that\\nallows system:unauthenticated users access to the API server’s\\nAPI discovery endpoint. For any cluster exposed to a hostile envi‐\\nronment (e.g., the public internet) this is a bad idea, and there has\\nbeen at least one serious security vulnerability via this exposure.\\nConsequently, if you are running a Kubernetes service on the pub‐\\nlic internet or an other hostile environment, you should ensure that\\nthe --anonymous-auth=false flag is set on your API server.\\nRole-Based Access Control | 171\\nTechniques for Managing RBAC\\nManaging RBAC for a cluster can be complicated and frustrating. Possibly more con‐\\ncerning is that misconfigured RBAC can lead to security issues. Fortunately, there are\\nseveral tools and techniques that make managing RBAC easier.\\nTesting Authorization with can-i\\nThe first useful tool is the auth can-i command for kubectl. This tool is very useful\\nfor testing if a particular user can do a particular action. You can use can-i to validate\\nconfiguration settings as you configure your cluster, or you can ask users to use the\\ntool to validate their access when filing errors or bug reports.\\nIn its simplest usage, the can-i command takes a verb and a resource.\\nFor example, this command will indicate if the current kubectl user is authorized to\\ncreate Pods:\\n$ kubectl auth can-i create pods\\nYou can also test subresources like logs or port forwarding with the --subresource\\ncommand-line flag:\\n$ kubectl auth can-i get pods --subresource=logs\\nManaging RBAC in Source Control\\nLike all resources in Kubernetes, RBAC resources are modeled using JSON or YAML.\\nGiven this text-based representation it makes sense to store these resources in version\\ncontrol. Indeed, the strong need for audit, accountability, and rollback for changes to\\nRBAC policy means that version control for RBAC resources is essential.\\nFortunately, the kubectl command-line tool comes with a reconcile command that\\noperates somewhat like kubectl apply and will reconcile a text-based set of roles and\\nrole bindings with the current state of the cluster.\\nYou can run:\\n$ kubectl auth reconcile -f some-rbac-config.yaml\\nand the data in the file will be reconciled with the cluster. If you want to see changes\\nbefore they are made, you can add the --dry-run flag to the command to print but\\nnot submit the changes.\\nAdvanced Topics\\nOnce you orient to the basics of role-based access control it is relatively easy to man‐\\nage access to a Kubernetes cluster, but when managing a large number of users or\\n172 | Chapter 14: Role-Based Access Control for Kubernetes\\nroles, there are additional advanced capabilities you can use to manage RBAC at\\nscale.\\nAggregating ClusterRoles\\nSometimes you want to be able to define roles that are combinations of other roles.\\nOne option would be to simply clone all of the rules from one ClusterRole into\\nanother ClusterRole, but this is complicated and error-prone, since changes to one\\nClusterRole aren’t automatically reflected in the other. Instead, Kubernetes RBAC\\nsupports the usage of an aggregation rule to combine multiple roles together in a new\\nrole. This new role combines all of the capabilities of all of the aggregate roles\\ntogether, and any changes to any of the constituent subroles will automatically be\\npropogated back into the aggregate role.\\nLike with all other aggregations or groupings in Kubernetes, the ClusterRoles to be\\naggregated are specified using label selectors. In this particular case, the aggregation\\nRule field in the ClusterRole resource contains a clusterRoleSelector field, which\\nin turn is a label selector. All ClusterRole resources that match this selector are\\ndynamically aggregated into the rules array in the aggregate ClusterRole resource.\\nA best practice for managing ClusterRole resources is to create a number of finegrained cluster roles and then aggregate them together to form higher-level or\\nbroadly defined cluster roles. This is how the built-in cluster roles are defined. For\\nexample, you can see that the built-in edit role looks like this:\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n name: edit\\n ...\\naggregationRule:\\n clusterRoleSelectors:\\n - matchLabels:\\n rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\\n...\\nThis means that the edit role is defined to be the aggregate of all ClusterRole\\nobjects that have a label of rbac.authorization.k8s.io/aggregate-to-edit set to\\ntrue.\\nUsing Groups for Bindings\\nWhen managing a large number of people in different organizations with similar\\naccess to the cluster, it’s generally a best practice to use groups to manage the roles\\nthat define access to the cluster, rather than individually adding bindings to specific\\nidentities. When you bind a group to a ClusterRole or a namespace Role, anyone\\nAdvanced Topics | 173\\nwho is a member of that group gains access to the resources and verbs defined by that\\nrole. Thus, to enable any individual to gain access to the group’s role, that individual\\nneeds to be added to the group.\\nThere are several reasons why using groups is a preferred strategy for managing\\naccess at scale. The first is that in any large organization, access to the cluster is\\ndefined in terms of the team that someone is part of, rather than their specific iden‐\\ntity. For example, someone who is part of the frontend operations team will need\\naccess to both view and edit the resources associated with the frontends, while they\\nmay only need view/read access to resources associated with the backend. Granting\\nprivileges to a group makes the association between the specific team and its capabili‐\\nties clear. When granting roles to individuals, it’s much harder to clearly understand\\nthe appropriate (i.e., minimal) privileges required for each team, especially when an\\nindividual may be part of multiple different teams.\\nAdditional benefits of binding roles to groups instead of individuals are simplicity\\nand consistency. When someone joins or leaves a team, it is straightforward to simply\\nadd or remove them to or from a group in a single operation. If you instead have to\\nremove a number of different role bindings for their identity, you may either remove\\ntoo few or too many bindings, resulting in unnecessary access or preventing them\\nfrom being able to do necessary actions. Additionally, because there is only a single\\nset of group role bindings to maintain, you don’t have to do lots of work to ensure\\nthat all team members have the same, consistent set of permissions.\\nFurthermore, many group systems enable “just in time” (JIT) access such that people\\nare only temporarily added to a group in response to an event (say, a page in the mid‐\\ndle of the night) rather than having standing access. This means that you can both\\naudit who had access at any particular time and ensure that, in general, even a com‐\\npromised identity can’t have access to your production infrastructure.\\nFinally, in many cases these same groups are used to manage access to other resour‐\\nces, from facilities to documents and machine logins. Thus, using the same groups for\\naccess control to Kubernetes dramatically simplifies management.\\nTo bind a group to a ClusterRole you use a Group kind for the subject in the\\nbinding:\\n...\\nsubjects:\\n- apiGroup: rbac.authorization.k8s.io\\n kind: Group\\n name: my-great-groups-name\\n...\\nIn Kubernetes, groups are supplied by authentication providers. There is no strong\\nnotion of a group within Kubernetes, only that an identity can be part of one or more\\ngroups, and those groups can be associated with a Role or ClusterRole via a binding.\\n174 | Chapter 14: Role-Based Access Control for Kubernetes\\nSummary\\nWhen you begin with a small cluster and a small team, it is sufficient to have every\\nmember of the team have equivalent access to the cluster. But as teams grow and\\nproducts become more mission critical, limiting access to parts of the cluster is cru‐\\ncial. In a well-designed cluster, access is limited to the minimal set of people and\\ncapabilities needed to efficiently manage the applications in the cluster. Understand‐\\ning how Kubernetes implements RBAC and how those capabilities can be used to\\ncontrol access to your cluster is important for both developers and cluster adminis‐\\ntrators. As with building out testing infrastructure, best practice is to set up proper\\nRBAC earlier rather than later. It’s far easier to start with the right foundation than to\\ntry to retrofit it later on. Hopefully, the information in this chapter has provided the\\nnecessary grounding for adding RBAC to your cluster.\\nSummary | 175\\n\\nCHAPTER 15\\nIntegrating Storage Solutions\\nand Kubernetes\\nIn many cases, decoupling state from applications and building your microservices to\\nbe as stateless as possible results in maximally reliable, manageable systems.\\nHowever, nearly every system that has any complexity has state in the system some‐\\nwhere, from the records in a database to the index shards that serve results for a web\\nsearch engine. At some point, you have to have data stored somewhere.\\nIntegrating this data with containers and container orchestration solutions is often\\nthe most complicated aspect of building a distributed system. This complexity largely\\nstems from the fact that the move to containerized architectures is also a move\\ntoward decoupled, immutable, and declarative application development. These pat‐\\nterns are relatively easy to apply to stateless web applications, but even “cloud-native”\\nstorage solutions like Cassandra or MongoDB involve some sort of manual or imper‐\\native steps to set up a reliable, replicated solution.\\nAs an example of this, consider setting up a ReplicaSet in MongoDB, which involves\\ndeploying the Mongo daemon and then running an imperative command to identify\\nthe leader, as well as the participants in the Mongo cluster. Of course, these steps can\\nbe scripted, but in a containerized world it is difficult to see how to integrate such\\ncommands into a deployment. Likewise, even getting DNS-resolvable names for indi‐\\nvidual containers in a replicated set of containers is challenging.\\nAdditional complexity comes from the fact that there is data gravity. Most container‐\\nized systems aren’t built in a vacuum; they are usually adapted from existing systems\\ndeployed onto VMs, and these systems likely include data that has to be imported or\\nmigrated.\\n177\\nFinally, evolution to the cloud often means that storage is an externalized cloud ser‐\\nvice, and in that context it can never really exist inside of the Kubernetes cluster.\\nThis chapter covers a variety of approaches for integrating storage into containerized\\nmicroservices in Kubernetes. First, we cover how to import existing external storage\\nsolutions (either cloud services or running on VMs) into Kubernetes. Next, we\\nexplore how to run reliable singletons inside of Kubernetes that enable you to have an\\nenvironment that largely matches the VMs where you previously deployed storage\\nsolutions. Finally, we cover StatefulSets, which are still under development but repre‐\\nsent the future of stateful workloads in Kubernetes.\\nImporting External Services\\nIn many cases, you have an existing machine running in your network that has some\\nsort of database running on it. In this situation you may not want to immediately\\nmove that database into containers and Kubernetes. Perhaps it is run by a different\\nteam, or you are doing a gradual move, or the task of migrating the data is simply\\nmore trouble than it’s worth.\\nRegardless of the reasons for staying put, this legacy server and service are not going\\nto move into Kubernetes—but it’s still worthwhile to represent this server in Kuber‐\\nnetes. When you do this, you get to take advantage of all of the built-in naming and\\nservice-discovery primitives provided by Kubernetes. Additionally, this enables you\\nto configure all your applications so that it looks like the database that is running on a\\nmachine somewhere is actually a Kubernetes service. This means that it is trivial to\\nreplace it with a database that is a Kubernetes service. For example, in production,\\nyou may rely on your legacy database that is running on a machine, but for continu‐\\nous testing you may deploy a test database as a transient container. Since it is created\\nand destroyed for each test run, data persistence isn’t important in the continuous\\ntesting case. Representing both databases as Kubernetes services enables you to main‐\\ntain identical configurations in both testing and production. High fidelity between\\ntest and production ensures that passing tests will lead to successful deployment in\\nproduction.\\nTo see concretely how you maintain high fidelity between development and produc‐\\ntion, remember that all Kubernetes objects are deployed into namespaces. Imagine\\nthat we have test and production namespaces defined. The test service is imported\\nusing an object like:\\nkind: Service\\nmetadata:\\n name: my-database\\n # note \\'test\\' namespace here\\n namespace: test\\n...\\n178 | Chapter 15: Integrating Storage Solutions and Kubernetes\\nThe production service looks the same, except it uses a different namespace:\\nkind: Service\\nmetadata:\\n name: my-database\\n # note \\'prod\\' namespace here\\n namespace: prod\\n...\\nWhen you deploy a Pod into the test namespace and it looks up the service named\\nmy-database, it will receive a pointer to my-database.test.svc.cluster.internal,\\nwhich in turn points to the test database. In contrast, when a Pod deployed in the\\nprod namespace looks up the same name (my-database) it will receive a pointer to\\nmy-database.prod.svc.cluster.internal, which is the production database. Thus,\\nthe same service name, in two different namespaces, resolves to two different serv‐\\nices. For more details on how this works, see Chapter 7.\\nThe following techniques all use database or other storage services,\\nbut these approaches can be used equally well with other services\\nthat aren’t running inside your Kubernetes cluster.\\nServices Without Selectors\\nWhen we first introduced services, we talked at length about label queries and how\\nthey were used to identify the dynamic set of Pods that were the backends for a par‐\\nticular service. With external services, however, there is no such label query. Instead,\\nyou generally have a DNS name that points to the specific server running the data‐\\nbase. For our example, let’s assume that this server is named database.company.com.\\nTo import this external database service into Kubernetes, we start by creating a ser‐\\nvice without a Pod selector that references the DNS name of the database server\\n(Example 15-1).\\nExample 15-1. dns-service.yaml\\nkind: Service\\napiVersion: v1\\nmetadata:\\n name: external-database\\nspec:\\n type: ExternalName\\n externalName: database.company.com\\nWhen a typical Kubernetes service is created, an IP address is also created and the\\nKubernetes DNS service is populated with an A record that points to that IP address.\\nWhen you create a service of type ExternalName, the Kubernetes DNS service is\\nImporting External Services | 179\\ninstead populated with a CNAME record that points to the external name you speci‐\\nfied (database.company.com in this case). When an application in the cluster does a\\nDNS lookup for the hostname external-database.svc.default.cluster, the DNS\\nprotocol aliases that name to database.company.com. This then resolves to the IP\\naddress of your external database server. In this way, all containers in Kubernetes\\nbelieve that they are talking to a service that is backed with other containers, when in\\nfact they are being redirected to the external database.\\nNote that this is not restricted to databases you are running on your own infrastruc‐\\nture. Many cloud databases and other services provide you with a DNS name to use\\nwhen accessing the database (e.g., my-database.databases.cloudprovider.com).\\nYou can use this DNS name as the externalName. This imports the cloud-provided\\ndatabase into the namespace of your Kubernetes cluster.\\nSometimes, however, you don’t have a DNS address for an external database service,\\njust an IP address. In such cases, it is still possible to import this service as a Kuber‐\\nnetes service, but the operation is a little different. First, you create a Service without\\na label selector, but also without the ExternalName type we used before\\n(Example 15-2).\\nExample 15-2. external-ip-service.yaml\\nkind: Service\\napiVersion: v1\\nmetadata:\\n name: external-ip-database\\nAt this point, Kubernetes will allocate a virtual IP address for this service and popu‐\\nlate an A record for it. However, because there is no selector for the service, there will\\nbe no endpoints populated for the load balancer to redirect traffic to.\\nGiven that this is an external service, the user is responsible for populating the end‐\\npoints manually with an Endpoints resource (Example 15-3).\\nExample 15-3. external-ip-endpoints.yaml\\nkind: Endpoints\\napiVersion: v1\\nmetadata:\\n name: external-ip-database\\nsubsets:\\n - addresses:\\n - ip: 192.168.0.1\\n ports:\\n - port: 3306\\n180 | Chapter 15: Integrating Storage Solutions and Kubernetes\\nIf you have more than one IP address for redundancy, you can repeat them in the\\naddresses array. Once the endpoints are populated, the load balancer will start redi‐\\nrecting traffic from your Kubernetes service to the IP address endpoint(s).\\nBecause the user has assumed responsibility for keeping the IP\\naddress of the server up to date, you need to either ensure that it\\nnever changes or make sure that some automated process updates\\nthe Endpoints record.\\nLimitations of External Services: Health Checking\\nExternal services in Kubernetes have one significant restriction: they do not perform\\nany health checking. The user is responsible for ensuring that the endpoint or DNS\\nname supplied to Kubernetes is as reliable as necessary for the application.\\nRunning Reliable Singletons\\nThe challenge of running storage solutions in Kubernetes is often that primitives like\\nReplicaSet expect that every container is identical and replaceable, but for most stor‐\\nage solutions this isn’t the case. One option to address this is to use Kubernetes primi‐\\ntives, but not attempt to replicate the storage. Instead, simply run a single Pod that\\nruns the database or other storage solution. In this way the challenges of running\\nreplicated storage in Kubernetes don’t occur, since there is no replication.\\nAt first blush, this might seem to run counter to the principles of building reliable\\ndistributed systems, but in general, it is no less reliable than running your database or\\nstorage infrastructure on a single virtual or physical machine, which is how many\\npeople currently have built their systems. Indeed, in reality, if you structure the sys‐\\ntem properly the only thing you are sacrificing is potential downtime for upgrades or\\nin case of machine failure. While for large-scale or mission-critical systems this may\\nnot be acceptable, for many smaller-scale applications this kind of limited downtime\\nis a reasonable trade-off for the reduced complexity. If this is not true for you, feel\\nfree to skip this section and either import existing services as described in the previ‐\\nous section, or move on to Kubernetes-native StatefulSets, described in the following\\nsection. For everyone else, we’ll review how to build reliable singletons for data\\nstorage.\\nRunning a MySQL Singleton\\nIn this section, we’ll describe how to run a reliable singleton instance of the MySQL\\ndatabase as a Pod in Kubernetes, and how to expose that singleton to other applica‐\\ntions in the cluster.\\nTo do this, we are going to create three basic objects:\\nRunning Reliable Singletons | 181\\n• A persistent volume to manage the lifespan of the on-disk storage independently\\nfrom the lifespan of the running MySQL application\\n• A MySQL Pod that will run the MySQL application\\n• A service that will expose this Pod to other containers in the cluster\\nIn Chapter 5 we described persistent volumes, but a quick review makes sense. A per‐\\nsistent volume is a storage location that has a lifetime independent of any Pod or con‐\\ntainer. This is very useful in the case of persistent storage solutions where the on-disk\\nrepresentation of a database should survive even if the containers running the data‐\\nbase application crash, or move to different machines. If the application moves to a\\ndifferent machine, the volume should move with it, and data should be preserved.\\nSeparating the data storage out as a persistent volume makes this possible.\\nTo begin, we’ll create a persistent volume for our MySQL database to use. This exam‐\\nple uses NFS for maximum portability, but Kubernetes supports many different per‐\\nsistent volume drive types. For example, there are persistent volume drivers for all\\nmajor public cloud providers, as well as many private cloud providers. To use these\\nsolutions, simply replace nfs with the appropriate cloud provider volume type (e.g.,\\nazure, awsElasticBlockStore, or gcePersistentDisk). In all cases, this change is all\\nyou need. Kubernetes knows how to create the appropriate storage disk in the respec‐\\ntive cloud provider. This is a great example of how Kubernetes simplifies the develop‐\\nment of reliable distributed systems.\\nExample 15-4 shows the PersistentVolume object.\\nExample 15-4. nfs-volume.yaml\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n name: database\\n labels:\\n volume: my-volume\\nspec:\\n accessModes:\\n - ReadWriteMany\\n capacity:\\n storage: 1Gi\\n nfs:\\n server: 192.168.0.1\\n path: \"/exports\"\\nThis defines an NFS PersistentVolume object with 1 GB of storage space.\\nWe can create this persistent volume as usual with:\\n$ kubectl apply -f nfs-volume.yaml\\n182 | Chapter 15: Integrating Storage Solutions and Kubernetes\\nNow that we have a persistent volume created, we need to claim that persistent vol‐\\nume for our Pod. We do this with a PersistentVolumeClaim object (Example 15-5).\\nExample 15-5. nfs-volume-claim.yaml\\nkind: PersistentVolumeClaim\\napiVersion: v1\\nmetadata:\\n name: database\\nspec:\\n accessModes:\\n - ReadWriteMany\\n resources:\\n requests:\\n storage: 1Gi\\n selector:\\n matchLabels:\\n volume: my-volume\\nThe selector field uses labels to find the matching volume we defined previously.\\nThis kind of indirection may seem overly complicated, but it has a purpose—it serves\\nto isolate our Pod definition from our storage definition. You can declare volumes\\ndirectly inside a Pod specification, but this locks that Pod specification to a particular\\nvolume provider (e.g., a specific public or private cloud). By using volume claims, you\\ncan keep your Pod specifications cloud-agnostic; simply create different volumes,\\nspecific to the cloud, and use a PersistentVolumeClaim to bind them together. Fur‐\\nthermore, in many cases, the persistent volume controller will actually automatically\\ncreate a volume for you—there are more details of this process in the following\\nsection.\\nNow that we’ve claimed our volume, we can use a ReplicaSet to construct our single‐\\nton Pod. It might seem odd that we are using a ReplicaSet to manage a single Pod, but\\nit is necessary for reliability. Remember that once scheduled to a machine, a bare Pod\\nis bound to that machine forever. If the machine fails, then any Pods that are on that\\nmachine that are not being managed by a higher-level controller like a ReplicaSet\\nvanish along with the machine and are not rescheduled elsewhere. Consequently, to\\nensure that our database Pod is rescheduled in the presence of machine failures, we\\nuse the higher-level ReplicaSet controller, with a replica size of one, to manage our\\ndatabase (Example 15-6).\\nExample 15-6. mysql-replicaset.yaml\\napiVersion: extensions/v1\\nkind: ReplicaSet\\nmetadata:\\n name: mysql\\nRunning Reliable Singletons | 183\\n # labels so that we can bind a Service to this Pod\\n labels:\\n app: mysql\\nspec:\\n replicas: 1\\n selector:\\n matchLabels:\\n app: mysql\\n template:\\n metadata:\\n labels:\\n app: mysql\\n spec:\\n containers:\\n - name: database\\n image: mysql\\n resources:\\n requests:\\n cpu: 1\\n memory: 2Gi\\n env:\\n # Environment variables are not a best practice for security,\\n # but we\\'re using them here for brevity in the example.\\n # See Chapter 11 for better options.\\n - name: MYSQL_ROOT_PASSWORD\\n value: some-password-here\\n livenessProbe:\\n tcpSocket:\\n port: 3306\\n ports:\\n - containerPort: 3306\\n volumeMounts:\\n - name: database\\n # /var/lib/mysql is where MySQL stores its databases\\n mountPath: \"/var/lib/mysql\"\\n volumes:\\n - name: database\\n persistentVolumeClaim:\\n claimName: database\\nOnce we create the ReplicaSet it will, in turn, create a Pod running MySQL using the\\npersistent disk we originally created. The final step is to expose this as a Kubernetes\\nservice (Example 15-7).\\nExample 15-7. mysql-service.yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n name: mysql\\nspec:\\n184 | Chapter 15: Integrating Storage Solutions and Kubernetes\\n ports:\\n - port: 3306\\n protocol: TCP\\n selector:\\n app: mysql\\nNow we have a reliable singleton MySQL instance running in our cluster and exposed\\nas a service named mysql, which we can access at the full domain name\\nmysql.svc.default.cluster.\\nSimilar instructions can be used for a variety of data stores, and if your needs are sim‐\\nple and you can survive limited downtime in the face of a machine failure or when\\nyou need to upgrade the database software, a reliable singleton may be the right\\napproach to storage for your application.\\nDynamic Volume Provisioning\\nMany clusters also include dynamic volume provisioning. With dynamic volume pro‐\\nvisioning, the cluster operator creates one or more StorageClass objects.\\nExample 15-8 shows a default storage class that automatically provisions disk objects\\non the Microsoft Azure platform.\\nExample 15-8. storageclass.yaml\\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:\\n name: default\\n annotations:\\n storageclass.beta.kubernetes.io/is-default-class: \"true\"\\n labels:\\n kubernetes.io/cluster-service: \"true\"\\nprovisioner: kubernetes.io/azure-disk\\nOnce a storage class has been created for a cluster, you can refer to this storage class\\nin your persistent volume claim, rather than referring to any specific persistent vol‐\\nume. When the dynamic provisioner sees this storage claim, it uses the appropriate\\nvolume driver to create the volume and bind it to your persistent volume claim.\\nExample 15-9 shows an example of a PersistentVolumeClaim that uses the default\\nstorage class we just defined to claim a newly created persistent volume.\\nExample 15-9. dynamic-volume-claim.yaml\\nkind: PersistentVolumeClaim\\napiVersion: v1\\nmetadata:\\nRunning Reliable Singletons | 185\\n name: my-claim\\n annotations:\\n volume.beta.kubernetes.io/storage-class: default\\nspec:\\n accessModes:\\n - ReadWriteOnce\\n resources:\\n requests:\\n storage: 10Gi\\nThe volume.beta.kubernetes.io/storage-class annotation is what links this claim\\nback up to the storage class we created.\\nAutomatic provisioning of a persistent volume is a great feature\\nthat makes it significantly easier to build and manage stateful appli‐\\ncations in Kubernetes. However, the lifespan of these persistent\\nvolumes is dictated by the reclamation policy of the PersistentVo\\nlumeClaim and the default is to bind that lifespan to the lifespan of\\nthe Pod that creates the volume.\\nThis means that if you happen to delete the Pod (e.g., via a scaledown or other event), then the volume is deleted as well. While this\\nmay be what you want in certain circumstances, you need to be\\ncareful to ensure that you don’t accidentally delete your persistent\\nvolumes.\\nPersistent volumes are great for traditional applications that require storage, but if\\nyou need to develop high-availability, scalable storage in a Kubernetes-native fashion,\\nthe newly released StatefulSet object can be used instead. We’ll describe how to\\ndeploy MongoDB using StatefulSets in the next section.\\nKubernetes-Native Storage with StatefulSets\\nWhen Kubernetes was first developed, there was a heavy emphasis on homogeneity\\nfor all replicas in a replicated set. In this design, no replica had an individual identity\\nor configuration. It was up to the application developer to determine a design that\\ncould establish this identity for their application.\\nWhile this approach provides a great deal of isolation for the orchestration system, it\\nalso makes it quite difficult to develop stateful applications. After significant input\\nfrom the community and a great deal of experimentation with various existing state‐\\nful applications, StatefulSets were introduced in Kubernetes version 1.5.\\n186 | Chapter 15: Integrating Storage Solutions and Kubernetes\\nProperties of StatefulSets\\nStatefulSets are replicated groups of Pods, similar to ReplicaSets. But unlike a Replica‐\\nSet, they have certain unique properties:\\n• Each replica gets a persistent hostname with a unique index (e.g., database-0,\\ndatabase-1, etc.).\\n• Each replica is created in order from lowest to highest index, and creation will\\nblock until the Pod at the previous index is healthy and available. This also\\napplies to scaling up.\\n• When a StatefulSet is deleted, each of the managed replica Pods is also deleted in\\norder from highest to lowest. This also applies to scaling down the number of\\nreplicas.\\nIt turns out that this simple set of requirements makes it drastically easier to deploy\\nstorage applications on Kubernetes. For example, the combination of stable host‐\\nnames (e.g., database-0) and the ordering constraints mean that all replicas, other\\nthan the first one, can reliably reference database-0 for the purposes of discovery\\nand establishing replication quorum.\\nManually Replicated MongoDB with StatefulSets\\nIn this section, we’ll deploy a replicated MongoDB cluster. For now, the replication\\nsetup itself will be done manually to give you a feel for how StatefulSets work. Even‐\\ntually we will automate this setup as well.\\nTo start, we’ll create a replicated set of three MongoDB Pods using a StatefulSet object\\n(Example 15-10).\\nExample 15-10. mongo-simple.yaml\\napiVersion: apps/v1\\nkind: StatefulSet\\nmetadata:\\n name: mongo\\nspec:\\n serviceName: \"mongo\"\\n replicas: 3\\n template:\\n metadata:\\n labels:\\n app: mongo\\n spec:\\n containers:\\n - name: mongodb\\n image: mongo:3.4.1\\nKubernetes-Native Storage with StatefulSets | 187\\n command:\\n - mongod\\n - --replSet\\n - rs0\\n ports:\\n - containerPort: 27017\\n name: peer\\nAs you can see, the definition is similar to the ReplicaSet definitions we’ve seen previ‐\\nously. The only changes are in the apiVersion and kind fields.\\nCreate the StatefulSet:\\n$ kubectl apply -f mongo-simple.yaml\\nOnce created, the differences between a ReplicaSet and a StatefulSet become appa‐\\nrent. Run kubectl get pods and you will likely see:\\nNAME READY STATUS RESTARTS AGE\\nmongo-0 1/1 Running 0 1m\\nmongo-1 0/1 ContainerCreating 0 10s\\nThere are two important differences between this and what you would see with a\\nReplicaSet. The first is that each replicated Pod has a numeric index (0, 1, …), instead\\nof the random suffix that is added by the ReplicaSet controller. The second is that the\\nPods are being slowly created in order, not all at once as they would be with a\\nReplicaSet.\\nOnce the StatefulSet is created, we also need to create a “headless” service to manage\\nthe DNS entries for the StatefulSet. In Kubernetes a service is called “headless” if it\\ndoesn’t have a cluster virtual IP address. Since with StatefulSets each Pod has a unique\\nidentity, it doesn’t really make sense to have a load-balancing IP address for the repli‐\\ncated service. You can create a headless service using clusterIP: None in the service\\nspecification (Example 15-11).\\nExample 15-11. mongo-service.yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n name: mongo\\nspec:\\n ports:\\n - port: 27017\\n name: peer\\n clusterIP: None\\n selector:\\n app: mongo\\n188 | Chapter 15: Integrating Storage Solutions and Kubernetes\\nOnce you create that service, there are usually four DNS entries that are populated.\\nAs usual, mongo.default.svc.cluster.local is created, but unlike with a standard\\nservice, doing a DNS lookup on this hostname provides all the addresses in the State‐\\nfulSet. In addition, entries are created for mongo-0.mongo.default.svc.cluster\\n.local as well as mongo-1.mongo and mongo-2.mongo. Each of these resolves to the\\nspecific IP address of the replica index in the StatefulSet. Thus, with StatefulSets you\\nget well-defined, persistent names for each replica in the set. This is often very useful\\nwhen you are configuring a replicated storage solution. You can see these DNS entries\\nin action by running the following commands in one of the Mongo replicas:\\n$ kubectl run -it --rm --image busybox busybox ping mongo-1.mongo\\nNext, we’re going to manually set up Mongo replication using these per-Pod\\nhostnames.\\nWe’ll choose mongo-0.mongo to be our initial primary. Run the mongo tool in that Pod:\\n$ kubectl exec -it mongo-0 mongo\\n> rs.initiate( {\\n _id: \"rs0\",\\n members:[ { _id: 0, host: \"mongo-0.mongo:27017\" } ]\\n });\\n OK\\nThis command tells mongodb to initiate the ReplicaSet rs0 with mongo-0.mongo as the\\nprimary replica.\\nThe rs0 name is arbitrary. You can use whatever you’d like, but\\nyou’ll need to change it in the mongo.yaml StatefulSet definition as\\nwell.\\nOnce you have initiated the Mongo ReplicaSet, you can add the remaining replicas by\\nrunning the following commands in the mongo tool on the mongo-0.mongo Pod:\\n> rs.add(\"mongo-1.mongo:27017\");\\n> rs.add(\"mongo-2.mongo:27017\");\\nAs you can see, we are using the replica-specific DNS names to add them as replicas\\nin our Mongo cluster. At this point, we’re done. Our replicated MongoDB is up and\\nrunning. But it’s really not as automated as we’d like it to be—in the next section, we’ll\\nsee how to use scripts to automate the setup.\\nAutomating MongoDB Cluster Creation\\nTo automate the deployment of our StatefulSet-based MongoDB cluster, we’re going\\nto add an additional container to our Pods to perform the initialization.\\nKubernetes-Native Storage with StatefulSets | 189\\nTo configure this Pod without having to build a new Docker image, we’re going to use\\na ConfigMap to add a script into the existing MongoDB image. Here’s the container\\nwe’re adding:\\n...\\n - name: init-mongo\\n image: mongo:3.4.1\\n command:\\n - bash\\n - /config/init.sh\\n volumeMounts:\\n - name: config\\n mountPath: /config\\n volumes:\\n - name: config\\n configMap:\\n name: \"mongo-init\"\\nNote that it is mounting a ConfigMap volume whose name is mongo-init. This Con‐\\nfigMap holds a script that performs our initialization. First, the script determines\\nwhether it is running on mongo-0 or not. If it is on mongo-0, it creates the ReplicaSet\\nusing the same command we ran imperatively previously. If it is on a different Mongo\\nreplica, it waits until the ReplicaSet exists, and then it registers itself as a member of\\nthat ReplicaSet.\\nExample 15-12 has the complete ConfigMap object.\\nExample 15-12. mongo-configmap.yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n name: mongo-init\\ndata:\\n init.sh: |\\n #!/bin/bash\\n # Need to wait for the readiness health check to pass so that the\\n # mongo names resolve. This is kind of wonky.\\n until ping -c 1 ${HOSTNAME}.mongo; do\\n echo \"waiting for DNS (${HOSTNAME}.mongo)...\"\\n sleep 2\\n done\\n until /usr/bin/mongo --eval \\'printjson(db.serverStatus())\\'; do\\n echo \"connecting to local mongo...\"\\n sleep 2\\n done\\n echo \"connected to local.\"\\n190 | Chapter 15: Integrating Storage Solutions and Kubernetes\\n HOST=mongo-0.mongo:27017\\n until /usr/bin/mongo --host=${HOST} --eval \\'printjson(db.serverStatus())\\'; do\\n echo \"connecting to remote mongo...\"\\n sleep 2\\n done\\n echo \"connected to remote.\"\\n if [[ \"${HOSTNAME}\" != \\'mongo-0\\' ]]; then\\n until /usr/bin/mongo --host=${HOST} --eval=\"printjson(rs.status())\" \\\\\\n | grep -v \"no replset config has been received\"; do\\n echo \"waiting for replication set initialization\"\\n sleep 2\\n done\\n echo \"adding self to mongo-0\"\\n /usr/bin/mongo --host=${HOST} \\\\\\n --eval=\"printjson(rs.add(\\'${HOSTNAME}.mongo\\'))\"\\n fi\\n if [[ \"${HOSTNAME}\" == \\'mongo-0\\' ]]; then\\n echo \"initializing replica set\"\\n /usr/bin/mongo --eval=\"printjson(rs.initiate(\\\\\\n {\\'_id\\': \\'rs0\\', \\'members\\': [{\\'_id\\': 0, \\\\\\n \\'host\\': \\'mongo-0.mongo:27017\\'}]}))\"\\n fi\\n echo \"initialized\"\\n while true; do\\n sleep 3600\\n done\\nThis script currently sleeps forever after initializing the cluster.\\nEvery container in a Pod has to have the same RestartPolicy.\\nSince we do not want our main Mongo container to be restarted,\\nwe need to have our initialization container run forever too, or else\\nKubernetes might think our Mongo Pod is unhealthy.\\nPutting it all together, Example 15-13 is the complete StatefulSet that uses the\\nConfigMap.\\nExample 15-13. mongo.yaml\\napiVersion: apps/v1\\nkind: StatefulSet\\nmetadata:\\n name: mongo\\nspec:\\n serviceName: \"mongo\"\\n replicas: 3\\n template:\\nKubernetes-Native Storage with StatefulSets | 191\\n metadata:\\n labels:\\n app: mongo\\n spec:\\n containers:\\n - name: mongodb\\n image: mongo:3.4.1\\n command:\\n - mongod\\n - --replSet\\n - rs0\\n ports:\\n - containerPort: 27017\\n name: web\\n # This container initializes the mongodb server, then sleeps.\\n - name: init-mongo\\n image: mongo:3.4.1\\n command:\\n - bash\\n - /config/init.sh\\n volumeMounts:\\n - name: config\\n mountPath: /config\\n volumes:\\n - name: config\\n configMap:\\n name: \"mongo-init\"\\nGiven all of these files, you can create a Mongo cluster with:\\n$ kubectl apply -f mongo-config-map.yaml\\n$ kubectl apply -f mongo-service.yaml\\n$ kubectl apply -f mongo.yaml\\nOr if you want, you can combine them all into a single YAML file where the individ‐\\nual objects are separated by ---. Ensure that you keep the same ordering, since the\\nStatefulSet definition relies on the ConfigMap definition existing.\\nPersistent Volumes and StatefulSets\\nFor persistent storage, you need to mount a persistent volume into the /data/db direc‐\\ntory. In the Pod template, you need to update it to mount a persistent volume claim to\\nthat directory:\\n...\\n volumeMounts:\\n - name: database\\n mountPath: /data/db\\nWhile this approach is similar to the one we saw with reliable singletons, because the\\nStatefulSet replicates more than one Pod you cannot simply reference a persistent\\n192 | Chapter 15: Integrating Storage Solutions and Kubernetes\\nvolume claim. Instead, you need to add a persistent volume claim template. You can\\nthink of the claim template as being identical to the Pod template, but instead of cre‐\\nating Pods, it creates volume claims. You need to add the following onto the bottom\\nof your StatefulSet definition:\\n volumeClaimTemplates:\\n - metadata:\\n name: database\\n annotations:\\n volume.alpha.kubernetes.io/storage-class: anything\\n spec:\\n accessModes: [ \"ReadWriteOnce\" ]\\n resources:\\n requests:\\n storage: 100Gi\\nWhen you add a volume claim template to a StatefulSet definition, each time the\\nStatefulSet controller creates a Pod that is part of the StatefulSet it will create a persis‐\\ntent volume claim based on this template as part of that Pod.\\nIn order for these replicated persistent volumes to work correctly,\\nyou either need to have autoprovisioning set up for persistent vol‐\\numes, or you need to prepopulate a collection of persistent volume\\nobjects for the StatefulSet controller to draw from. If there are no\\nclaims that can be created, the StatefulSet controller will not be able\\nto create the corresponding Pods.\\nOne Final Thing: Readiness Probes\\nThe final piece in productionizing our MongoDB cluster is to add liveness checks to\\nour Mongo-serving containers. As we learned in “Health Checks” on page 54, the\\nliveness probe is used to determine if a container is operating correctly. For the liven‐\\ness checks, we can use the mongo tool itself by adding the following to the Pod tem‐\\nplate in the StatefulSet object:\\n...\\n livenessProbe:\\n exec:\\n command:\\n - /usr/bin/mongo\\n - --eval\\n - db.serverStatus()\\n initialDelaySeconds: 10\\n timeoutSeconds: 10\\n ...\\nKubernetes-Native Storage with StatefulSets | 193\\nSummary\\nOnce we have combined StatefulSets, persistent volume claims, and liveness probing,\\nwe have a hardened, scalable cloud-native MongoDB installation running on Kuber‐\\nnetes. While this example dealt with MongoDB, the steps for creating StatefulSets to\\nmanage other storage solutions are quite similar and similar patterns can be followed.\\n194 | Chapter 15: Integrating Storage Solutions and Kubernetes\\nCHAPTER 16\\nExtending Kubernetes\\nFrom the beginning, it was clear that Kubernetes was going to be more than its core\\nset of APIs; once an application is orchestrated within the cluster, there are countless\\nother useful tools and utilities that can be represented and deployed as API objects in\\nthe Kubernetes cluster. The challenge was how to embrace this explosion of objects\\nand use cases without having an API that sprawled without bound.\\nTo resolve this tension between extended use cases and API sprawl, significant effort\\nwas put into making the Kubernetes API extensible. This extensibility meant that\\ncluster operators could customize their clusters with the additional components that\\nsuited their needs. This extensibility enables people to augment their clusters them‐\\nselves, consume community-developed cluster add-ons, and even develop extensions\\nthat are bundled and sold in an ecosystem of cluster plug-ins. Extensibility has also\\ngiven rise to whole new patterns of managing systems, such as the operator pattern.\\nRegardless of whether you are building your own extensions or consuming operators\\nfrom the ecosystem, understanding how the Kubernetes API server is extended and\\nhow extensions can be built and delivered is a key component to unlocking the com‐\\nplete power of Kubernetes and its ecosystem. As more and more advanced tools and\\nplatforms are built on top of Kubernetes using these extensibility mechanisms, a\\nworking knowledge of how they operate is critical to understanding how to build\\napplications in a modern Kubernetes cluster.\\nWhat It Means to Extend Kubernetes\\nIn general, extensions to the Kubernetes API server either add new functionality to a\\ncluster or limit and tweak the ways that users can interact with their clusters. There is\\na rich ecosystem of plug-ins that cluster administrators can use to add additional\\nservices and capabilities to their clusters. It’s worth noting that extending the cluster\\n195\\nis a very high-privilege thing to do. It is not a capability that should be extended to\\narbitrary users or arbitrary code, because cluster administrator privileges are required\\nto extend a cluster. Even cluster administrators should be careful and use diligence\\nwhen installing third-party tools. Some extensions, like admission controllers, can be\\nused to view all objects being created in the cluster, and could easily be used as a vec‐\\ntor to steal secrets or run malicious code. Additionally, extending a cluster makes it\\ndifferent than stock Kubernetes. When running on multiple clusters, it is very valua‐\\nble to build tooling to maintain consistency of experience across the clusters, and this\\nincludes the extensions that are installed.\\nPoints of Extensibility\\nThere are many different ways to extend Kubernetes, from CustomResourceDefini‐\\ntions through to Container Network Interface (CNI) plug-ins. This chapter is going\\nto focus on the extensions to the API server via adding new resource types or admis‐\\nsion controllers to API requests. We will not cover the CNI/CSI/CRI (Container Net‐\\nwork Interface/Container Storage Interface/Container Runtime Interface) extensions,\\nas they are more commonly used by Kubernetes cluster providers as opposed to the\\nend users of Kubernetes, for whom this book was written.\\nIn addition to admission controllers and API extensions, there are actually a number\\nof ways to “extend” your cluster without ever modifying the API server at all. These\\ninclude DaemonSets that install automatic logging and monitoring, tools that scan\\nyour services for cross-site scripting (XSS) vulnerabilities, and more. Before embark‐\\ning on extending your cluster yourself, however, it’s worth considering the landscape\\nof things that are possible within the confines of the existing Kubernetes APIs.\\nTo help understand the role of admission controllers and CustomResourceDefini‐\\ntions, it is very helpful to understand the flow of requests through the Kubernetes\\nAPI server, which is shown in Figure 16-1.\\nFigure 16-1. API server request flow\\nAdmission controllers are called prior to the API object being written into the back‐\\ning storage. Admission controllers can reject or modify API requests. There are sev‐\\neral admission controllers that are built into the Kubernetes API server; for example,\\nthe limit range admission controller that sets default limits for Pods without them.\\nMany other systems use custom admission controllers to auto-inject sidecar contain‐\\ners into all Pods created on the system to enable “auto-magic” experiences.\\n196 | Chapter 16: Extending Kubernetes\\nThe other form of extension, which can also be used in conjunction with admission\\ncontrollers, is custom resources. With custom resources, whole new API objects are\\nadded to the Kubernetes API surface area. These new API objects can be added to\\nnamespaces, are subject to RBAC, and can be accessed with existing tools like\\nkubectl as well as via the Kubernetes API.\\nThe following sections describe these Kubernetes extension points in greater detail\\nand give both use cases and hands-on examples of how to extend your cluster.\\nThe first thing that you do to create a custom resource is to create a CustomResour‐\\nceDefinition. This object is actually a meta-resource; that is, a resource that is the def‐\\ninition of another resource.\\nAs a concrete example, consider defining a new resource to represent load tests in\\nyour cluster. When a new LoadTest resource is created, a load test is spun up in your\\nKubernetes cluster and drives traffic to a service.\\nThe first step in creating this new resource is defining it through a CustomResource‐\\nDefinition. An example definition looks as follows:\\napiVersion: apiextensions.k8s.io/v1beta1\\nkind: CustomResourceDefinition\\nmetadata:\\n name: loadtests.beta.kuar.com\\nspec:\\n group: beta.kuar.com\\n versions:\\n - name: v1\\n served: true\\n storage: true\\n scope: Namespaced\\n names:\\n plural: loadtests\\n singular: loadtest\\n kind: LoadTest\\n shortNames:\\n - lt\\nYou can see that this is a Kubernetes object like any other. It has a metadata\\nsubobject, and within that subobject the resource is named. However, in the case of\\ncustom resources, the name is special. It has to be the following format: <resourceplural>.<api-group>. The reason for this is to ensure that each resource definition\\nis unique in the cluster, because the name of each CustomResourceDefinition has to\\nmatch this pattern, and no two objects in the cluster can have the same name. We are\\nthus guaranteed that no two CustomResourceDefinitions define the same resource.\\nIn addition to metadata, the CustomResourceDefinition has a spec subobject. This is\\nwhere the resource itself is defined. In that spec object, there is an apigroup field\\nwhich supplies the API group for the resource. As mentioned previously, it must\\nPoints of Extensibility | 197\\nmatch the suffix of the CustomResourceDefinition’s name. Additionally, there is a list\\nof versions for the resource. In addition to the name of the version (e.g., v1, v2, etc.),\\nthere are fields that indicate if that version is served by the API server and which ver‐\\nsion is used for storing data in the backing storage for the API server. The storage\\nfield must be true for only a single version for the resource. There is also a scope field\\nto indicate if the resource is namespaced or not (the default is namespaced), and a\\nnames field that allows for the definition of the singular, plural, and kind values for\\nthe resource. It also allows the definition of convenience “short names” for the\\nresource for use in kubectl and elsewhere.\\nGiven this definition, you can create the resource in the Kubernetes API server. But\\nfirst, to show the true nature of dynamic resource types, try to list our loadtests\\nresource using kubectl:\\n$ kubectl get loadtests\\nYou’ll see that there is no such resource currently defined.\\nNow use loadtest-resource.yaml to create this resource:\\n$ kubectl create -f loadtest-resource.yaml\\nThen get the loadtests resource again:\\n$ kubectl get loadtests\\nThis time you’ll see that there is a LoadTest resource type defined, though there are\\nstill no instances of this resource type.\\nLet’s change that by creating a new LoadTest resource.\\nAs with all built-in Kubernetes API objects, you can use YAML or JSON to define a\\ncustom resource (in this case our LoadTest). See the following definition:\\napiVersion: beta.kuar.com/v1\\nkind: LoadTest\\nmetadata:\\n name: my-loadtest\\nspec:\\n service: my-service\\n scheme: https\\n requestsPerSecond: 1000\\n paths:\\n - /index.html\\n - /login.html\\n - /shares/my-shares/\\nOne thing that you’ll note is that we never defined the schema for the custom\\nresource in the CustomResourceDefinition. It actually is possible to provide an\\nOpenAPI specification for a custom resource, but this complexity is generally not\\n198 | Chapter 16: Extending Kubernetes\\nworth it for simple resource types. If you do want to perform validation, you can reg‐\\nister a validating admission controller, as described in the following sections.\\nYou can now use this loadtest.yaml file to create a resource just like you would with\\nany built-in type:\\n$ kubectl create -f loadtest.yaml\\nNow when you list the loadtests resource, you’ll see your newly created resource:\\n$ kubectl get loadtests\\nThis may be exciting, but it doesn’t really do anything yet. Sure, you can use this sim‐\\nple CRUD (Create/Read/Update/Delete) API to manipulate the data for LoadTest\\nobjects, but no actual load tests are created in response to this new API we defined.\\nThis is because there is no controller present in the cluster to react and take action\\nwhen a LoadTest object is defined. The LoadTest custom resource is only half of the\\ninfrastructure needed to add LoadTests to our cluster. The other is a piece of code\\nthat will continuously monitor the custom resources and create, modify, or delete\\nLoadTests as necessary to implement the API.\\nJust like the user of the API, the controller interacts with the API server to list\\nLoadTests and watches for any changes that might occur. This interaction between\\ncontroller and API server is shown in Figure 16-2.\\nFigure 16-2. CustomResourceDefinition interactions\\nThe code for such a controller can range from simple to complex. The simplest con‐\\ntrollers run a for loop and repeatedly poll for new custom objects, and then take\\nactions to create or delete the resources that implement those custom objects (e.g., the\\nLoadTest worker pods).\\nHowever, this polling-based approach is inefficient: the period of the polling loop\\nadds unnecessary latency, and the overhead of polling may add unnecessary load on\\nthe API server. A more efficient approach is to use the watch API on the API server,\\nwhich provides a stream of updates when they occur, eliminating both the latency\\nand overhead of polling. However, using this API correctly in a bug-free way is com‐\\nplicated. As a result, if you want to use watches, it is highly recommended that you\\nuse a well-supported mechanism such as the Informer pattern exposed in the\\nclient-go library.\\nPoints of Extensibility | 199\\nNow that we have created a custom resource and implemented it via a controller, we\\nhave the basic functionality of a new resource in our cluster. However, many parts of\\nwhat it means to be a well-functioning resource are missing. The two most important\\nare validation and defaulting. Validation is the process of ensuring that LoadTest\\nobjects sent to the API server are well formed and can be used to create load tests,\\nwhile defaulting makes it easier for people to use our resources by providing auto‐\\nmatic, commonly used values by default. We’ll now cover adding these capabilities to\\nour custom resource.\\nAs mentioned earlier, one option for adding validation is via an OpenAPI specifica‐\\ntion for our objects. This can be useful for basic validation of the presence of required\\nfields or the absence of unknown fields. A complete OpenAPI tutorial is beyond the\\nscope of this book, but there are lots of resources online, including the complete\\nKubernetes API specification.\\nGenerally speaking, an API schema is actually insufficient for validation of API\\nobjects. For example, in our loadtests example, we may want to validate that the\\nLoadTest object has a valid scheme (e.g., http or https) or that requestsPerSecond is a\\nnonzero positive number.\\nTo accomplish this, we will use a validating admission controller. As discussed previ‐\\nously, admission controllers intercept requests to the API server before they are pro‐\\ncessed and can reject or modify the requests in flight. Admission controllers can be\\nadded to a cluster via the dynamic admission control system. A dynamic admission\\ncontroller is a simple HTTP application. The API server connects to the admission\\ncontroller via either a Kubernetes Service object or an arbitrary URL. This means that\\nadmission controllers can optionally run outside of the cluster—for example, in a\\ncloud provider’s Function-as-a-Service offering, like Azure Functions or AWS\\nLambda.\\nTo install our validating admission controller, we need to specify it as a Kubernetes\\nValidatingWebhookConfiguration. This object specifies the endpoint where the\\nadmission controller runs, as well as the resource (in this case LoadTest) and the\\naction (in this case CREATE) where the admission controller should be run. You can\\nsee the full definition for the validating admission controller in the following code:\\napiVersion: admissionregistration.k8s.io/v1beta1\\nkind: ValidatingWebhookConfiguration\\nmetadata:\\n name: kuar-validator\\nwebhooks:\\n- name: validator.kuar.com\\n rules:\\n - apiGroups:\\n - \"beta.kuar.com\"\\n apiVersions:\\n - v1\\n200 | Chapter 16: Extending Kubernetes\\n operations:\\n - CREATE\\n resources:\\n - loadtests\\n clientConfig:\\n # Substitute the appropriate IP address for your webhook\\n url: https://192.168.1.233:8080\\n # This should be the base64-encoded CA certificate for your cluster,\\n # you can find it in your ${KUBECONFIG} file\\n caBundle: REPLACEME\\nFortunately for security, but unfortunately for complexity, webhooks that are accessed\\nby the Kubernetes API server can only be accessed via HTTPS. This means that we\\nneed to generate a certificate to serve the webhook. The easiest way to do this is to use\\nthe cluster’s ability to generate new certificates using its own certificate authority\\n(CA).\\nFirst, we need a private key and a certificate signing request (CSR). Here’s a simple Go\\nprogram that generates these:\\npackage main\\nimport (\\n \"crypto/rand\"\\n \"crypto/rsa\"\\n \"crypto/x509\"\\n \"crypto/x509/pkix\"\\n \"encoding/asn1\"\\n \"encoding/pem\"\\n \"net/url\"\\n \"os\"\\n)\\nfunc main() {\\n host := os.Args[1]\\n name := \"server\"\\n key, err := rsa.GenerateKey(rand.Reader, 1024)\\n if err != nil {\\n panic(err)\\n }\\n keyDer := x509.MarshalPKCS1PrivateKey(key)\\n keyBlock := pem.Block{\\n Type: \"RSA PRIVATE KEY\",\\n Bytes: keyDer,\\n }\\n keyFile, err := os.Create(name + \".key\")\\n if err != nil {\\n panic(err)\\n }\\n pem.Encode(keyFile, &keyBlock)\\n keyFile.Close()\\nPoints of Extensibility | 201\\n commonName := \"myuser\"\\n emailAddress := \"someone@myco.com\"\\n org := \"My Co, Inc.\"\\n orgUnit := \"Widget Farmers\"\\n city := \"Seattle\"\\n state := \"WA\"\\n country := \"US\"\\n subject := pkix.Name{\\n CommonName: commonName,\\n Country: []string{country},\\n Locality: []string{city},\\n Organization: []string{org},\\n OrganizationalUnit: []string{orgUnit},\\n Province: []string{state},\\n }\\n uri, err := url.ParseRequestURI(host)\\n if err != nil {\\n panic(err)\\n }\\n asn1, err := asn1.Marshal(subject.ToRDNSequence())\\n if err != nil {\\n panic(err)\\n }\\n csr := x509.CertificateRequest{\\n RawSubject: asn1,\\n EmailAddresses: []string{emailAddress},\\n SignatureAlgorithm: x509.SHA256WithRSA,\\n URIs: []*url.URL{uri},\\n }\\n bytes, err := x509.CreateCertificateRequest(rand.Reader, &csr, key)\\n if err != nil {\\n panic(err)\\n }\\n csrFile, err := os.Create(name + \".csr\")\\n if err != nil {\\n panic(err)\\n }\\n pem.Encode(csrFile, &pem.Block{Type: \"CERTIFICATE REQUEST\", Bytes: bytes})\\n csrFile.Close()\\n}\\nYou can run this program with:\\n$ go run csr-gen.go <URL-for-webook>\\nand it will generate two files, server.csr and server-key.pem.\\n202 | Chapter 16: Extending Kubernetes\\nYou can then create a certificate signing request for the Kubernetes API server using\\nthe following YAML:\\napiVersion: certificates.k8s.io/v1beta1\\nkind: CertificateSigningRequest\\nmetadata:\\n name: validating-controller.default\\nspec:\\n groups:\\n - system:authenticated\\n request: REPLACEME\\n usages:\\n usages:\\n - digital signature\\n - key encipherment\\n - key agreement\\n - server auth\\nYou will notice for the request field the value is REPLACEME; this needs to be replaced\\nwith the base64-encoded certificate signing request we produced in the preceding\\ncode:\\n$ perl -pi -e s/REPLACEME/$(base64 server.csr | tr -d \\'\\\\n\\')/ \\\\\\nadmission-controller-csr.yaml\\nNow that your certificate signing request is ready, you can send it to the API server to\\nget the certificate:\\n$ kubectl create -f admission-controller-csr.yaml\\nNext, you need to approve that request:\\n$ kubectl certificate approve validating-controller.default\\nOnce approved, you can download the new certificate:\\n$ kubectl get csr validating-controller.default -o json | \\\\\\n jq -r .status.certificate | base64 -d > server.crt\\nWith the certificate, you are finally ready to create an SSL-based admission controller\\n(phew!). When the admission controller code receives a request, it contains an object\\nof type AdmissionReview, which contains metadata about the request as well as the\\nbody of the request itself. In our validating admission controller we have only regis‐\\ntered for a single resource type and a single action (CREATE), so we don’t need to\\nexamine the request metadata. Instead, we dive directly into the resource itself and\\nvalidate that requestsPerSecond is positive and the URL scheme is valid. If they\\naren’t, we return a JSON body disallowing the request.\\nImplementing an admission controller to provide defaulting is similar to the steps\\njust described, but instead of using a ValidatingWebhookConfiguration you use a\\nMutatingWebhookConfiguration, and you need to provide a JSONPatch object to\\nmutate the request object before it is stored.\\nPoints of Extensibility | 203\\nHere’s a TypeScript snippet that you can add to your validating admission controller\\nto add defaulting. If the paths field in the loadtest is of length zero, add a single\\npath for /index.html:\\n if (needsPatch(loadtest)) {\\n const patch = [\\n { \\'op\\': \\'add\\', \\'path\\': \\'/spec/paths\\', \\'value\\': [\\'/index.html\\'] },\\n ]\\n response[\\'patch\\'] = Buffer.from(JSON.stringify(patch))\\n .toString(\\'base64\\');\\n response[\\'patchType\\'] = \\'JSONPatch\\';\\n }\\nYou can then register this webhook as a MutatingWebhookConfiguration by simply\\nchanging the kind field in the YAML object and saving the file as mutatingcontroller.yaml. Then create the controller by running:\\n$ kubectl create -f mutating-controller.yaml\\nAt this point you’ve seen a complete example of how to extend the Kubernetes API\\nserver using custom resources and admission controllers. The following section\\ndescribes some general patterns for various extensions.\\nPatterns for Custom Resources\\nNot all custom resources are identical. There are a variety of different reasons for\\nextending the Kubernetes API surface area, and the following sections discuss some\\ngeneral patterns you may want to consider.\\nJust Data\\nThe easiest pattern for API extension is the notion of “just data.” In this pattern, you\\nare simply using the API server for storage and retrieval of information for your\\napplication. It is important to note that you should not use the Kubernetes API server\\nfor application data storage. The Kubernetes API server is not designed to be a key/\\nvalue store for your app; instead, API extensions should be control or configuration\\nobjects that help you manage the deployment or runtime of your application. An\\nexample use case for the “just data” pattern might be configuration for canary deploy‐\\nments of your application—for example, directing 10% of all traffic to an experimen‐\\ntal backend. While in theory such configuration information could also be stored in a\\nConfigMap, ConfigMaps are essentially untyped, and sometimes using a more\\nstrongly typed API extension object provides clarity and ease of use.\\nExtensions that are just data don’t need a corresponding controller to activate them,\\nbut they may have validating or mutating admission controllers to ensure that they\\nare well formed. For example, in the canary use case a validating controller might\\nensure that all percentages in the canary object sum to 100%.\\n204 | Chapter 16: Extending Kubernetes\\nCompilers\\nA slightly more complicated pattern is the “compiler” or “abstraction” pattern. In this\\npattern the API extension object represents a higher-level abstraction that is “com‐\\npiled” into a combination of lower-level Kubernetes objects. The LoadTest extension\\nin the previous example is an example of this compiler abstraction pattern. A user\\nconsumes the extension as a high-level concept, in this case a loadtest, but it comes\\ninto being by being deployed as a collection of Kubernetes Pods and services. To ach‐\\nieve this, a compiled abstraction requires an API controller to be running somewhere\\nin the cluster, to watch the current LoadTests and create the “compiled” representa‐\\ntion (and likewise delete representations that no longer exist). In contrast to the oper‐\\nator pattern described next, however, there is no online health maintenance for\\ncompiled abstractions; it is delegated down to the lower-level objects (e.g., Pods).\\nOperators\\nWhile compiler extensions provide easy-to-use abstractions, extensions that use the\\n“operator” pattern provide online, proactive management of the resources created by\\nthe extensions. These extensions likely provide a higher-level abstraction (for exam‐\\nple, a database) that is compiled down to a lower-level representation, but they also\\nprovide online functionality, such as snapshot backups of the database, or upgrade\\nnotifications when a new version of the software is available. To achieve this, the con‐\\ntroller not only monitors the extension API to add or remove things as necessary, but\\nalso monitors the running state of the application supplied by the extension (e.g., a\\ndatabase) and takes actions to remediate unhealthy databases, take snapshots, or\\nrestore from a snapshot if a failure occurs. Operators are the most complicated pat‐\\ntern for API extension of Kubernetes, but they are also the most powerful, enabling\\nusers to get easy access to “self-driving” abstractions that are responsible not just for\\ndeployment, but also health checking and repair.\\nGetting Started\\nGetting started extending the Kubernetes API can be a daunting and exhausting\\nexperience. Fortunately, there is a great deal of code to help you out. The Kubebuilder\\nproject contains a library of code intended help you easily build reliable Kubernetes\\nAPI extensions. It’s a great resource to help you bootstrap your extension.\\nSummary\\nOne of the great “superpowers” of Kubernetes is its ecosystem, and one of the most\\nsignificant things powering this ecosystem is the extensibility of the Kubernetes API.\\nWhether you’re designing your own extensions to customize your cluster or consum‐\\ning off-the-shelf extensions as utilities, cluster services, or operators, API extensions\\nSummary | 205\\nare the key to making your cluster your own and building the right environment for\\nthe rapid development of reliable applications.\\n206 | Chapter 16: Extending Kubernetes\\nCHAPTER 17\\nDeploying Real-World Applications\\nThe previous chapters described a variety of API objects that are available in a Kuber‐\\nnetes cluster and ways in which those objects can best be used to construct reliable\\ndistributed systems. However, none of the preceding chapters really discussed how\\nyou might use the objects in practice to deploy a complete, real-world application.\\nThat is the focus of this chapter.\\nWe’ll take a look at four real-world applications:\\n• Jupyter, an open source scientific notebook\\n• Parse, an open source API server for mobile applications\\n• Ghost, a blogging and content management platform\\n• Redis, a lightweight, performant key/value store\\nThese complete examples should give you a better idea of how to structure your own\\ndeployments using Kubernetes.\\nJupyter\\nThe Jupyter Project is a web-based interactive scientific notebook for exploration and\\nvisualization. It is used by students and scientists around the world to build and\\nexplore data and data visualizations. Because it is both simple to deploy and interest‐\\ning to use, it’s a great first service to deploy on Kubernetes.\\nWe begin by creating a namespace to hold the Jupyter application:\\n$ kubectl create namespace jupyter\\n207\\nAnd then create a deployment of size one with the program itself:\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n labels:\\n run: jupyter\\n name: jupyter\\n namespace: jupyter\\nspec:\\n replicas: 1\\n selector:\\n matchLabels:\\n run: jupyter\\n template:\\n metadata:\\n labels:\\n run: jupyter\\n spec:\\n containers\\n - image: jupyter/scipy-notebook:abdb27a6dfbb\\n name: jupyter\\n dnsPolicy: ClusterFirst\\n restartPolicy: Always\\nCreate a file named jupyter.yaml with these contents. Once you have created this file,\\nyou can deploy it using:\\n$ kubectl create -f jupyter.yaml\\nNow you need to wait for the container to be created. The Jupyter container is quite\\nlarge (2 GB at the time of writing), and thus it may take a few minutes to start up.\\nYou can wait for the container to become ready using the watch command (on\\nmacOS, you’ll need to install this command using brew install watch):\\n$ watch kubectl get pods --namespace jupyter\\nOnce the Jupyter container is up and running, you need to obtain the initial login\\ntoken. You can do this by looking at the logs for the container:\\n$ pod_name=$(kubectl get pods --namespace jupyter --no-headers | awk \\'{print $1}\\') \\\\\\nkubectl logs --namespace jupyter ${pod_name}\\nYou should then copy the token (it will look something like /?\\ntoken=0195713c8e65088650fdd8b599db377b7ce6c9b10bd13766).\\nNext, set up port forwarding to the Jupyter container:\\n$ kubectl port-forward ${pod_name} 8888:8888\\nFinally, you can visit http://localhost:8888/?token=<token>, inserting the token that\\nyou copied from the logs earlier.\\n208 | Chapter 17: Deploying Real-World Applications\\nYou should now have the Jupyter dashboard loaded in your browser. You can find\\ntutorials to get oriented to Jupyter if you are so inclined on the Jupyter project site.\\nParse\\nThe Parse server is a cloud API dedicated to providing easy-to-use storage for mobile\\napplications. It provides a variety of different client libraries that make it easy to inte‐\\ngrate with Android, iOS, and other mobile platforms. Parse was purchased by Face‐\\nbook in 2013 and subsequently shut down. Fortunately for us, a compatible server\\nwas open sourced by the core Parse team and is available for us to use. This section\\ndescribes how to set up Parse in Kubernetes.\\nPrerequisites\\nParse uses a MongoDB cluster for its storage. Chapter 15 described how to set up a\\nreplicated MongoDB cluster using Kubernetes StatefulSets. This section assumes you\\nhave a three-replica Mongo cluster running in Kubernetes with the names\\nmongo-0.mongo, mongo-1.mongo, and mongo-2.mongo.\\nThese instructions also assume that you have a Docker login; if you don’t have one,\\nyou can get one for free at https://docker.com.\\nFinally, we assume you have a Kubernetes cluster deployed and the kubectl tool\\nproperly configured.\\nBuilding the parse-server\\nThe open source parse-server comes with a Dockerfile by default, for easy contain‐\\nerization. First, clone the Parse repository:\\n$ git clone https://github.com/ParsePlatform/parse-server\\nThen move into that directory and build the image:\\n$ cd parse-server\\n$ docker build -t ${DOCKER_USER}/parse-server .\\nFinally, push that image up to the Docker hub:\\n$ docker push ${DOCKER_USER}/parse-server\\nDeploying the parse-server\\nOnce you have the container image built, deploying the parse-server into your clus‐\\nter is fairly straightforward. Parse looks for three environment variables when being\\nconfigured:\\nParse | 209\\nPARSE_SERVER_APPLICATION_ID\\nAn identifier for authorizing your application\\nPARSE_SERVER_MASTER_KEY\\nAn identifier that authorizes the master (root) user\\nPARSE_SERVER_DATABASE_URI\\nThe URI for your MongoDB cluster\\nPutting this all together, you can deploy Parse as a Kubernetes deployment using the\\nYAML file in Example 17-1.\\nExample 17-1. parse.yaml\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n name: parse-server\\n namespace: default\\nspec:\\n replicas: 1\\n template:\\n metadata:\\n labels:\\n run: parse-server\\n spec:\\n containers:\\n - name: parse-server\\n image: ${DOCKER_USER}/parse-server\\n env:\\n - name: PARSE_SERVER_DATABASE_URI\\n value: \"mongodb://mongo-0.mongo:27017,\\\\\\n mongo-1.mongo:27017,mongo-2.mongo\\\\\\n :27017/dev?replicaSet=rs0\"\\n - name: PARSE_SERVER_APP_ID\\n value: my-app-id\\n - name: PARSE_SERVER_MASTER_KEY\\n value: my-master-key\\nTesting Parse\\nTo test your deployment, you need to expose it as a Kubernetes service. You can do\\nthat using the service definition in Example 17-2.\\nExample 17-2. parse-service.yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n210 | Chapter 17: Deploying Real-World Applications\\n name: parse-server\\n namespace: default\\nspec:\\n ports:\\n - port: 1337\\n protocol: TCP\\n targetPort: 1337\\n selector:\\n run: parse-server\\nNow your Parse server is up and running and ready to receive requests from your\\nmobile applications. Of course, in any real application you are likely going to want to\\nsecure the connection with HTTPS. You can see the parse-server GitHub page for\\nmore details on such a configuration.\\nGhost\\nGhost is a popular blogging engine with a clean interface written in JavaScript. It can\\neither use a file-based SQLite database or MySQL for storage.\\nCon\\x80guring Ghost\\nGhost is configured with a simple JavaScript file that describes the server. We will\\nstore this file as a configuration map. A simple development configuration for Ghost\\nlooks like Example 17-3.\\nExample 17-3. ghost-config.js\\nvar path = require(\\'path\\'),\\n config;\\nconfig = {\\n development: {\\n url: \\'http://localhost:2368\\',\\n database: {\\n client: \\'sqlite3\\',\\n connection: {\\n filename: path.join(process.env.GHOST_CONTENT,\\n \\'/data/ghost-dev.db\\')\\n },\\n debug: false\\n },\\n server: {\\n host: \\'0.0.0.0\\',\\n port: \\'2368\\'\\n },\\n paths: {\\n contentPath: path.join(process.env.GHOST_CONTENT, \\'/\\')\\n }\\nGhost | 211\\n }\\n};\\nmodule.exports = config;\\nOnce you have this configuration file saved to ghost-config.js, you can create a Kuber‐\\nnetes ConfigMap object using:\\n$ kubectl create cm --from-file ghost-config.js ghost-config\\nThis creates a ConfigMap that is named ghost-config. As with the Parse example,\\nwe will mount this configuration file as a volume inside of our container. We will\\ndeploy Ghost as a Deployment object, which defines this volume mount as part of the\\nPod template (Example 17-4).\\nExample 17-4. ghost.yaml\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n name: ghost\\nspec:\\n replicas: 1\\n selector:\\n matchLabels:\\n run: ghost\\n template:\\n metadata:\\n labels:\\n run: ghost\\n spec:\\n containers:\\n - image: ghost\\n name: ghost\\n command:\\n - sh\\n - -c\\n - cp /ghost-config/ghost-config.js /var/lib/ghost/config.js\\n && /usr/local/bin/docker-entrypoint.sh node current/index.js\\n volumeMounts:\\n - mountPath: /ghost-config\\n name: config\\n volumes:\\n - name: config\\n configMap:\\n defaultMode: 420\\n name: ghost-config\\n212 | Chapter 17: Deploying Real-World Applications\\nOne thing to note here is that we are copying the config.js file from a different loca‐\\ntion into the location where Ghost expects to find it, since the ConfigMap can only\\nmount directories, not individual files. Ghost expects other files that are not in that\\nConfigMap to be present in its directory, and thus we cannot simply mount the entire\\nConfigMap into /var/lib/ghost.\\nYou can run this with:\\n$ kubectl apply -f ghost.yaml\\nOnce the Pod is up and running, you can expose it as a service with:\\n$ kubectl expose deployments ghost --port=2368\\nOnce the service is exposed, you can use the kubectl proxy command to access the\\nGhost server:\\n$ kubectl proxy\\nThen visit http://localhost:8001/api/v1/namespaces/default/services/ghost/proxy/ in\\nyour web browser to begin interacting with Ghost.\\nGhost + MySQL\\nOf course, this example isn’t very scalable, or even reliable, since the contents of the\\nblog are stored in a local file inside the container. A more scalable approach is to store\\nthe blog’s data in a MySQL database.\\nTo do this, first modify config.js to include:\\n...\\ndatabase: {\\n client: \\'mysql\\',\\n connection: {\\n host : \\'mysql\\',\\n user : \\'root\\',\\n password : \\'root\\',\\n database : \\'ghost_db\\',\\n charset : \\'utf8\\'\\n }\\n },\\n...\\nObviously, in a real-world deployment you’ll want to change the password from root\\nto something more secret.\\nNext, create a new ghost-config ConfigMap object:\\n$ kubectl create configmap ghost-config-mysql --from-file ghost-config.js\\nThen update the Ghost deployment to change the name of the ConfigMap mounted\\nfrom config-map to config-map-mysql:\\nGhost | 213\\n...\\n - configMap:\\n name: ghost-config-mysql\\n...\\nUsing the instructions from “Kubernetes-Native Storage with StatefulSets” on page\\n186, deploy a MySQL server in your Kubernetes cluster. Make sure that it has a ser‐\\nvice named mysql defined as well.\\nYou will need to create the database in the MySQL database:\\n$ kubectl exec -it mysql-zzmlw -- mysql -u root -p\\nEnter password:\\nWelcome to the MySQL monitor. Commands end with ; or \\\\g.\\n...\\nmysql> create database ghost_db;\\n...\\nFinally, perform a rollout to deploy this new configuration:\\n$ kubectl apply -f ghost.yaml\\nBecause your Ghost server is now decoupled from its database, you can scale up your\\nGhost server and it will continue to share the data across all replicas.\\nEdit ghost.yaml to set spec.replicas to 3, then run:\\n$ kubectl apply -f ghost.yaml\\nYour Ghost installation is now scaled up to three replicas.\\nRedis\\nRedis is a popular in-memory key/value store, with numerous additional features. It’s\\nan interesting application to deploy because it is a good example of the value of the\\nKubernetes Pod abstraction. This is because a reliable Redis installation actually is\\ntwo programs working together. The first is redis-server, which implements the\\nkey/value store, and the other is redis-sentinel, which implements health checking\\nand failover for a replicated Redis cluster.\\nWhen Redis is deployed in a replicated manner, there is a single master server that\\ncan be used for both read and write operations. Additionally, there are other replica\\nservers that duplicate the data written to the master and can be used for loadbalancing read operations. Any of these replicas can fail over to become the master if\\nthe original master fails. This failover is performed by the Redis sentinel. In our\\ndeployment, both a Redis server and a Redis sentinel are colocated in the same file.\\n214 | Chapter 17: Deploying Real-World Applications\\nCon\\x80guring Redis\\nAs before, we’re going to use Kubernetes ConfigMaps to configure our Redis installa‐\\ntion. Redis needs separate configurations for the master and slave replicas. To config‐\\nure the master, create a file named master.conf that contains the code in\\nExample 17-5.\\nExample 17-5. master.conf\\nbind 0.0.0.0\\nport 6379\\ndir /redis-data\\nThis directs Redis to bind to all network interfaces on port 6379 (the default Redis\\nport) and store its files in the /redis-data directory.\\nThe slave configuration is identical, but it adds a single slaveof directive. Create a\\nfile named slave.conf that contains what’s in Example 17-6.\\nExample 17-6. slave.conf\\nbind 0.0.0.0\\nport 6379\\ndir .\\nslaveof redis-0.redis 6379\\nNotice that we are using redis-0.redis for the name of the master. We will set up\\nthis name using a service and a StatefulSet.\\nWe also need a configuration for the Redis sentinel. Create a file named sentinel.conf\\nwith the contents of Example 17-7.\\nExample 17-7. sentinel.conf\\nbind 0.0.0.0\\nport 26379\\nsentinel monitor redis redis-0.redis 6379 2\\nsentinel parallel-syncs redis 1\\nsentinel down-after-milliseconds redis 10000\\nsentinel failover-timeout redis 20000\\nNow that we have all of our configuration files, we need to create a couple of simple\\nwrapper scripts to use in our StatefulSet deployment.\\nRedis | 215\\nThe first script simply looks at the hostname for the Pod and determines whether this\\nis the master or a slave, and launches Redis with the appropriate configuration. Cre‐\\nate a file named init.sh containing the code in Example 17-8.\\nExample 17-8. init.sh\\n#!/bin/bash\\nif [[ ${HOSTNAME} == \\'redis-0\\' ]]; then\\n redis-server /redis-config/master.conf\\nelse\\n redis-server /redis-config/slave.conf\\nfi\\nThe other script is for the sentinel. In this case it is necessary because we need to wait\\nfor the redis-0.redis DNS name to become available. Create a script named\\nsentinel.sh containing the code in Example 17-9.\\nExample 17-9. sentinel.sh\\n#!/bin/bash\\ncp /redis-config-src/*.* /redis-config\\nwhile ! ping -c 1 redis-0.redis; do\\n echo \\'Waiting for server\\'\\n sleep 1\\ndone\\nredis-sentinel /redis-config/sentinel.conf\\nNow we need to package all of these files into a ConfigMap object. You can do this\\nwith a single command line:\\n$ kubectl create configmap \\\\\\n --from-file=slave.conf=./slave.conf \\\\\\n --from-file=master.conf=./master.conf \\\\\\n --from-file=sentinel.conf=./sentinel.conf \\\\\\n --from-file=init.sh=./init.sh \\\\\\n --from-file=sentinel.sh=./sentinel.sh \\\\\\n redis-config\\nCreating a Redis Service\\nThe next step in deploying Redis is to create a Kubernetes service that will provide\\nnaming and discovery for the Redis replicas (e.g., redis-0.redis). To do this, we cre‐\\nate a service without a cluster IP address (Example 17-10).\\n216 | Chapter 17: Deploying Real-World Applications\\nExample 17-10. redis-service.yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n name: redis\\nspec:\\n ports:\\n - port: 6379\\n name: peer\\n clusterIP: None\\n selector:\\n app: redis\\nYou can create this service with kubectl apply -f redis-service.yaml. Don’t\\nworry that the Pods for the service don’t exist yet. Kubernetes doesn’t care; it will add\\nthe right names when the Pods are created.\\nDeploying Redis\\nWe’re ready to deploy our Redis cluster. To do this we’re going to use a StatefulSet. We\\nintroduced StatefulSets in “Manually Replicated MongoDB with StatefulSets” on page\\n187, when we discussed our MongoDB installation. StatefulSets provide indexing\\n(e.g., redis-0.redis) as well as ordered creation and deletion semantics (redis-0\\nwill always be created before redis-1, and so on). They’re quite useful for stateful\\napplications like Redis, but honestly, they basically look like Kubernetes deployments.\\nExample 17-11 shows what the StatefulSet looks like for our Redis cluster.\\nExample 17-11. redis.yaml\\napiVersion: apps/v1beta1\\nkind: StatefulSet\\nmetadata:\\n name: redis\\nspec:\\n replicas: 3\\n serviceName: redis\\n template:\\n metadata:\\n labels:\\n app: redis\\n spec:\\n containers:\\n - command: [sh, -c, source /redis-config/init.sh ]\\n image: redis:4.0.11-alpine\\n name: redis\\n ports:\\n - containerPort: 6379\\n name: redis\\nRedis | 217\\n volumeMounts:\\n - mountPath: /redis-config\\n name: config\\n - mountPath: /redis-data\\n name: data\\n - command: [sh, -c, source /redis-config-src/sentinel.sh]\\n image: redis:4.0.11-alpine\\n name: sentinel\\n volumeMounts:\\n - mountPath: /redis-config-src\\n name: config\\n - mountPath: /redis-config\\n name: data\\n volumes:\\n - configMap:\\n defaultMode: 420\\n name: redis-config\\n name: config\\n - emptyDir:\\n name: data\\n volumeMounts:\\n - mountPath: /redis-config\\n name: config\\n - mountPath: /redis-data\\n name: data\\n - command: [sh, -c, source /redis-config/sentinel.sh]\\n image: redis:3.2.7-alpine\\n name: sentinel\\n volumeMounts:\\n - mountPath: /redis-config\\n name: config\\nYou can see that there are two containers in this Pod. One runs the init.sh script that\\nwe created and the main Redis server, and the other is the sentinel that monitors the\\nservers.\\nYou can also see that there are two volumes defined in the Pod. One is the volume\\nthat uses our ConfigMap to configure the two Redis applications, and the other is a\\nsimple emptyDir volume that is mapped into the Redis server container to hold the\\napplication data so that it survives a container restart. For a more reliable Redis\\ninstallation, this could be a network-attached disk, as discussed in Chapter 15.\\nNow that we’ve defined our Redis cluster, we can create it using:\\n$ kubectl apply -f redis.yaml\\nPlaying with Our Redis Cluster\\nTo demonstrate that we’ve actually successfully created a Redis cluster, we can per‐\\nform some tests.\\n218 | Chapter 17: Deploying Real-World Applications\\nFirst, we can determine which server the Redis sentinel believes is the master. To do\\nthis, we can run the redis-cli command in one of the Pods:\\n$ kubectl exec redis-2 -c redis \\\\\\n -- redis-cli -p 26379 sentinel get-master-addr-by-name redis\\nThis should print out the IP address of the redis-0 Pod. You can confirm this using\\nkubectl get pods -o wide.\\nNext, we’ll confirm that the replication is actually working.\\nTo do this, first try to read the value foo from one of the replicas:\\n$ kubectl exec redis-2 -c redis -- redis-cli -p 6379 get foo\\nYou should see no data in the response.\\nNext, try to write that data to a replica:\\n$ kubectl exec redis-2 -c redis -- redis-cli -p 6379 set foo 10\\nREADONLY You can\\'t write against a read only slave.\\nYou can’t write to a replica, because it’s read-only. Let’s try the same command against\\nredis-0, which is the master:\\n$ kubectl exec redis-0 -c redis -- redis-cli -p 6379 set foo 10\\nOK\\nNow try the original read from a replica:\\n$ kubectl exec redis-2 -c redis -- redis-cli -p 6379 get foo\\n10\\nThis shows that our cluster is set up correctly, and data is replicating between masters\\nand slaves.\\nSummary\\nIn this chapter we described how to deploy a variety of applications using assorted\\nKubernetes concepts. We saw how to put together service-based naming and discov‐\\nery to deploy web frontends like Ghost as well as API servers like Parse, and we saw\\nhow Pod abstraction makes it easy to deploy the components that make up a reliable\\nRedis cluster. Regardless of whether you will actually deploy these applications to\\nproduction, the examples demonstrated patterns that you can repeat to manage your\\napplications using Kubernetes. We hope that seeing the concepts we described in pre‐\\nvious chapters come to life in real-world examples helps you better understand how\\nto make Kubernetes work for you.\\nSummary | 219\\n\\nCHAPTER 18\\nOrganizing Your Application\\nThroughout this book we have described various components of an application built\\non top of Kubernetes. We have described how to wrap programs up as containers,\\nplace those containers in Pods, replicate those Pods with ReplicaSets, and roll out\\nsoftware each week with deployments. We have even described how to deploy stateful\\nand real-world applications that put together a collection of these objects into a single\\ndistributed system. But we have not covered how to actually work with such an appli‐\\ncation in a practical way. How can you lay out, share, manage, and update the various\\nconfigurations that make up your application? That is the topic for this chapter.\\nPrinciples to Guide Us\\nBefore digging into the concrete details of how to structure your application, it’s\\nworth considering the goals that drive this structure. Obviously, reliability and agility\\nare the general goals of developing a cloud-native application in Kubernetes, but\\nmoving to the next level of detail, how does this actually relate to how you design the\\nmaintenance and deployment of your application? The following sections describe\\nthe various principles that we can use as a guide to design a structure that best suits\\nthese goals. The principles are:\\n• Filesystems as the source of truth\\n• Code review to ensure the quality of changes\\n• Feature flags for staged roll forward and roll back\\n221\\nFilesystems as the Source of Truth\\nWhen you first begin to explore Kubernetes, as we did in the beginning of this book,\\nyou generally interact with it imperatively. You run commands like kubectl run or\\nkubectl edit to create and modify Pods or other objects running in your cluster.\\nEven when we started exploring how to write and use YAML or JSON files, this was\\npresented in an ad-hoc manner, as if the file itself is just a way station on the way to\\nmodifying the state of the cluster. In reality, in a true productionized application the\\nopposite should be true.\\nRather than viewing the state of the cluster—the data in etcd—as the source of truth,\\nit is optimal to view the filesystem of YAML objects as the source of truth for your\\napplication. The API objects deployed into your Kubernetes cluster(s) are then a\\nreflection of the truth stored in the filesystem.\\nThere are numerous reasons why this is the right point of view. The first and fore‐\\nmost is that it largely enables you to treat your cluster as if it is immutable infrastruc‐\\nture. As we have moved into cloud-native architectures, we have become increasingly\\ncomfortable with the notion that our applications and their containers are immutable\\ninfrastructure, but treating a cluster as such is less common. And yet, the same rea‐\\nsons for moving our applications to immutable infrastructure apply to our clusters. If\\nyour cluster is a snowflake made up by the ad-hoc application of various random\\nYAML files downloaded from the internet, it is as dangerous as a virtual machine that\\nhas been built from imperative bash scripts.\\nAdditionally, managing the cluster state via the filesystem makes it very easy to col‐\\nlaborate with multiple team members. Source-control systems are well understood\\nand can easily enable multiple different people to simultaneously edit the state of the\\ncluster while making conflicts (and the resolution of those conflicts) clear to\\neveryone.\\nThe combination of these motivations means that it is absolutely a\\nfirst principle that all applications deployed to Kubernetes should\\nfirst be described in files stored in a filesystem. The actual API\\nobjects are then just a projection of this filesystem into a particular\\ncluster.\\nThe Role of Code Review\\nIt wasn’t long ago that code review for application source code was a novel idea. But it\\nis clear now that the notion of multiple people looking at a piece of code before it is\\ncommitted to an application is a best practice for producing quality, reliable code.\\nIt is therefore surprising that the same is somewhat less true for the configurations\\nused to deploy those applications. All of the same reasons for reviewing code apply\\n222 | Chapter 18: Organizing Your Application\\ndirectly to application configurations also. But when you think about it, it is also\\nobvious that code review of these configurations is critical to the reliable deployment\\nof services. In our experience, most service outages are self-inflicted via unexpected\\nconsequences, typos, or other simple mistakes. Ensuring that at least two people look\\nat any configuration change significantly decreases the probability of such errors.\\nConsequently, the second principle of our application layout is that\\nit must facilitate the review of every change merged into the set of\\nfiles that represents the source of truth for our cluster.\\nFeature Gates and Guards\\nOnce your application source code and your deployment configuration files are in\\nsource control, one of the most common questions that occurs is how these reposito‐\\nries relate to one another. Should you use the same repository for application source\\ncode as well as configuration? This can work for small projects, but in larger projects\\nit often makes sense to separate the source code from the configuration to provide for\\na separation of concerns. Even if the same people are responsible for both building\\nand deploying the application, the perspectives of the builder versus the deployer are\\ndifferent enough that this separation of concerns makes sense.\\nIf that is the case, then how do you bridge the development of new features in source\\ncontrol with the deployment of those features into a production environment? This is\\nwhere feature gates and guards play an important role.\\nThe idea is that when some new feature is developed, that development takes place\\nentirely behind a feature flag or gate. This gate looks something like:\\nif (featureFlags.myFlag) {\\n // Feature implementation goes here\\n}\\nThere are a variety of benefits to this approach. First, it enables the committing of\\ncode to the production branch long before the feature is ready to ship. This enables\\nfeature development to stay much more closely aligned with the HEAD of a repository,\\nand thus you avoid the horrendous merge conflicts of a long-lived branch.\\nAdditionally, it means that the enabling a feature simply involves a configuration\\nchange to activate the flag. This makes it very clear what changed in the production\\nenvironment, and likewise makes it very simple to roll back the activation of the fea‐\\nture if it causes problems.\\nThe use of feature flags thus both simplifies debugging problems in production and\\nensures that disabling a feature doesn’t require a binary rollback to an older version of\\nPrinciples to Guide Us | 223\\nthe code that would remove all of the bug fixes and other improvements made by the\\nnewer version of the code.\\nThe third principle of application layout is that code lands in\\nsource control, by default off, behind a feature flag, and is only acti‐\\nvated through a code-reviewed change to configuration files.\\nManaging Your Application in Source Control\\nNow that we have determined that the filesystem should represent the source of truth\\nfor your cluster, the next important question is how to actually lay out the files in the\\nfilesystem. Obviously, filesystems contain hierarchical directories, and a sourcecontrol system adds concepts like tags and branches, so this section describes how to\\nput these together to represent and manage your application.\\nFilesystem Layout\\nFor the purposes of this section, we will describe how to lay out an instance of your\\napplication for a single cluster. In later sections we will describe how to parameterize\\nthis layout for multiple instances of your application. It’s worth noting that this orga‐\\nnization is worth getting right when you begin. Much like modifying the layout of\\npackages in source control, modifying your deployment configurations after the fact\\nis a complicated and expensive refactor that you’ll probably never get around to.\\nThe first cardinality on which you want to organize your application is the semantic\\ncomponent or layer (e.g., frontend, batch work queue, etc.). Though early on this\\nmight seem like overkill, since a single team manages all of these components, it sets\\nthe stage for team scaling—eventually, a different team (or subteam) may be responsi‐\\nble for each of these components.\\nThus, for an application with a frontend that uses two services, the filesystem might\\nlook like:\\nfrontend/\\nservice-1/\\nservice-2/\\nWithin each of these directories, the configurations for each application are stored.\\nThese are the YAML files that directly represent the current state of the cluster. It’s\\ngenerally useful to include both the service name and the object type within the same\\nfile.\\n224 | Chapter 18: Organizing Your Application\\nWhile Kubernetes allows for the creation of YAML files with multi‐\\nple objects in the same file, this should generally be considered an\\nanti-pattern. The only good reason for grouping a number of\\nobjects in the same file is if they are conceptually identical. When\\ndeciding what to include in a single YAML file, consider design\\nprinciples similar to those for defining a class or struct. If grouping\\nthe objects together doesn’t form a single concept, they probably\\nshouldn’t be in a single file.\\nThus, extending our previous example, the filesystem might look like:\\nfrontend/\\n frontend-deployment.yaml\\n frontend-service.yaml\\n frontend-ingress.yaml\\nservice-1/\\n service-1-deployment.yaml\\n service-1-service.yaml\\n service-1-configmap.yaml\\n...\\nManaging Periodic Versions\\nThe previous section described a file structure for laying out the various tiers in your\\napplication, but what about managing the releases of your application? It is very use‐\\nful to be able to look back historically and see what your application deployment pre‐\\nviously looked like. Similarly, it is very useful to be able to iterate a configuration\\nforward while still being able to deploy a stable release configuration.\\nConsequently, it’s handy to be able to simultaneously store and maintain multiple dif‐\\nferent revisions of your configuration. Given the file and version control approach,\\nthere are two different approaches that you can use. The first is to use tags, branches,\\nand source-control features. This is convenient because it maps to the same way that\\npeople manage revisions in source control, and it leads to a more simplified directory\\nstructure. The other option is to clone the configuration within the filesystem and use\\ndirectories for different revisions. This approach is convenient because it makes\\nsimultaneous viewing of the configurations very straightforward.\\nIn reality, the approaches are more or less identical, and it is ultimately an aesthetic\\nchoice between the two. Thus, we will discuss both approaches and let you or your\\nteam decide which you prefer.\\nVersioning with branches and tags\\nWhen you use branches and tags to manage configuration revisions, the directory\\nstructure is unchanged from the example in the previous section. When you are\\nready for a release, you place a source-control tag (e.g., git tag v1.0) in the configu‐\\nManaging Your Application in Source Control | 225\\nration source-control system. The tag represents the configuration used for that ver‐\\nsion, and the HEAD of source control continues to iterate forward.\\nThe world becomes somewhat more complicated when you need to update the\\nrelease configuration, but the approach models what you would do in source control.\\nFirst, you commit the change to the HEAD of the repository. Then you create a new\\nbranch named v1 at the v1.0 tag. You then cherry-pick the desired change onto the\\nrelease branch (git cherry-pick <edit>), and finally, you tag this branch with the\\nv1.1 tag to indicate a new point release.\\nOne common error when cherry-picking fixes into a release branch\\nis to only pick the change into the latest release. It’s a good idea to\\ncherry-pick it into all active releases, in case for some reason you\\nneed to roll back versions but the fix is still needed.\\nVersioning with directories\\nAn alternative to using source-control features is to use filesystem features. In this\\napproach, each versioned deployment exists within its own directory. For example,\\nthe filesystem for your application might look like this:\\nfrontend/\\n v1/\\n frontend-deployment.yaml\\n frontend-service.yaml\\n current/\\n frontend-deployment.yaml\\n frontend-service.yaml\\nservice-1/\\n v1/\\n service-1-deployment.yaml\\n service-1-service.yaml\\n v2/\\n service-1-deployment.yaml\\n service-1-service.yaml\\n current/\\n service-1-deployment.yaml\\n service-1-service.yaml\\n...\\nThus, each revision exists in a parallel directory structure within a directory associ‐\\nated with the release. All deployments occur from HEAD instead of from specific revi‐\\nsions or tags. When adding a new configuration, it is done to the files in the current\\ndirectory.\\nWhen creating a new release, the current directory is copied to create a new directory\\nassociated with the new release.\\n226 | Chapter 18: Organizing Your Application\\nWhen performing a bugfix change to a release, the pull request must modify the\\nYAML file in all the relevant release directories. This is a slightly better experience\\nthan the cherry-picking approach described earlier, since it is clear in a single change\\nrequest that all of the relevant versions are being updated with the same change,\\ninstead of requiring a cherry-pick per version.\\nStructuring Your Application for Development, Testing,\\nand Deployment\\nIn addition to structuring your application for a periodic release cadence, you also\\nwant to structure your application to enable agile development, quality testing, and\\nsafe deployment. This enables developers to rapidly make and test changes to the dis‐\\ntributed application, and to safely roll those changes out to customers.\\nGoals\\nThere are two goals for your application with regard to development and testing. The\\nfirst is that each developer should be able to easily develop new features for the appli‐\\ncation. In most cases, the developer is only working on a single component, and yet\\nthat component is interconnected to all of the other microservices within the cluster.\\nThus, to facilitate development it is essential that developers be able to work in their\\nown environment, yet with all services available.\\nThe other goal for structuring your application for testing is the ability to easily and\\naccurately test your application prior to deployment. This is essential to the ability to\\nquickly roll out features while maintaining high reliability.\\nProgression of a Release\\nTo achieve both of these goals, it is important to relate the stages of development to\\nthe release versions described earlier. The stages of a release are:\\nHEAD\\nThe bleeding edge of the configuration; the latest changes.\\nDevelopment\\nLargely stable, but not ready for deployment. Suitable for developers to use for\\nbuilding features.\\nStaging\\nThe beginnings of testing, unlikely to change unless problems are found.\\nCanary\\nThe first real release to users, used to test for problems with real-world traffic\\nand likewise give users a chance to test what is coming next.\\nStructuring Your Application for Development, Testing, and Deployment | 227\\nRelease\\nThe current production release.\\nIntroducing a development tag\\nRegardless of whether you structure releases using the filesystem or version control,\\nthe right way to model the development stage is via a source-control tag. This is\\nbecause development is necessarily fast-moving as it tracks stability only slightly\\nbehind HEAD.\\nTo introduce a development stage, a new development tag is added to the sourcecontrol system and an automated process is used to move this tag forward. On a peri‐\\nodic cadence, HEAD is tested via automated integration testing. If these tests pass, the\\ndevelopment tag is moved forward to HEAD. Thus, developers can track reasonably\\nclose to the latest changes when deploying their own environments, but they also can\\nbe assured that the deployed configurations have at least passed a limited smoke test.\\nMapping stages to revisions\\nIt might be tempting to introduce a new set of configurations for each of these stages,\\nbut in reality, the Cartesian product of versions and stages would create a mess that is\\nvery difficult to reason about. Instead, the right practice is to introduce a mapping\\nbetween revisions and stages.\\nRegardless of whether you are using the filesystem or source-control revisions to rep‐\\nresent different configuration versions, it is easy to implement a map from stage to\\nrevision. In the filesystem case you can use symbolic links to map a stage name to a\\nrevision:\\nfrontend/\\n canary/ -> v2/\\n release/ -> v1/\\n v1/\\n frontend-deployment.yaml\\n...\\nIn the case of version control, it is simply an additional tag at the same revision as the\\nappropriate version.\\nIn either case, the versioning of releases proceeds using the processes described previ‐\\nously, and separately the stages are moved forward to new versions as appropriate.\\nEffectively this means that there are two simultaneous processes, the first for cutting\\nnew release versions and the second for qualifying a release version for a particular\\nstage in the application lifecycle.\\n228 | Chapter 18: Organizing Your Application\\nParameterizing Your Application with Templates\\nOnce you have a Cartesian product of environments and stages, it becomes clear that\\nit is impractical or impossible to keep them all entirely identical. And yet, it is impor‐\\ntant to strive for the environments to be as identical as possible. Variance and drift\\nbetween different environments produces snowflakes and systems that are hard to\\nreason about. If your staging environment is different than your release environment,\\ncan you really trust the load tests that you ran in the staging environment to qualify a\\nrelease? To ensure that your environments stay as similar as possible, it is useful to\\nuse parameterized environments. Parameterized environments use templates for the\\nbulk of their configuration, but they mix in a limited set of parameters to produce the\\nfinal configuration. In this way most of the configuration is contained within a shared\\ntemplate, while the parameterization is limited in scope and maintained in a small\\nparameters file for easy visualization of differences between environments.\\nParameterizing with Helm and Templates\\nThere are a variety of different languages for creating parameterized configurations.\\nIn general they all divide the files into a template file, which contains the bulk of the\\nconfiguration, and a parameters file, which can be combined with the template to\\nproduce a complete configuration. In addition to parameters, most templating lan‐\\nguages allow parameters to have default values if no value is specified.\\nThe following gives examples of how to parameterize configurations using Helm, a\\npackage manager for Kubernetes. Despite what devotees of various languages may\\nsay, all parameterization languages are largely equivalent, and as with programming\\nlangauges, which one you prefer is largely a matter of personal or team style. Thus,\\nthe same patterns described here for Helm apply regardless of the templating lan‐\\nguage you choose.\\nThe Helm template language uses the “mustache” syntax, so for example:\\nmetadata:\\n name: {{ .Release.Name }}-deployment\\nindicates that Release.Name should be substituted into the name of a deployment.\\nTo pass a parameter for this value you use a values.yaml file with contents like:\\nRelease:\\n Name: my-release\\nWhich after parameter substitution results in:\\nmetadata:\\n name: my-release-deployment\\nParameterizing Your Application with Templates | 229\\nFilesystem Layout for Parameterization\\nNow that you understand how to parameterize your configurations, how do you\\napply that to the filesystem layouts we have described previously? To achieve this,\\ninstead of treating each deployment lifecycle stage as a pointer to a version, each\\ndeployment lifecycle is the combination of a parameters file and a pointer to a spe‐\\ncific version. For example, in a directory-based layout this might look like:\\nfrontend/\\n staging/\\n templates -> ../v2\\n staging-parameters.yaml\\n production/\\n templates -> ../v1\\n production-parameters.yaml\\n v1/\\n frontend-deployment.yaml\\n frontend-service.yaml\\n v2/\\n frontend-deployment.yaml\\n frontend-service.yaml\\n...\\nDoing this with version control looks similar, except that the parameters for each life‐\\ncycle stage are kept at the root of the configuration directory tree:\\nfrontend/\\n staging-parameters.yaml\\n templates/\\n frontend-deployment.YAML\\n....\\nDeploying Your Application Around the World\\nNow that you have multiple versions of your application moving through multiple\\nstages of deployment, the final step in structuring your configurations is to deploy\\nyour application around the world. But don’t think that these approaches are only for\\nlarge-scale applications. In reality, they can be used to scale from two different\\nregions to tens or hundreds around the world. In the world of the cloud, where an\\nentire region can fail, deploying to multiple regions (and managing that deployment)\\nis the only way to achieve sufficient uptime for demanding users.\\nArchitectures for Worldwide Deployment\\nGenerally speaking, each Kubernetes cluster is intended to live in a single region, and\\neach Kubernetes cluster is expected to contain a single, complete deployment of your\\napplication. Consequently, a worldwide deployment of an application consists of mul‐\\ntiple different Kubernetes clusters, each with its own application configuration.\\n230 | Chapter 18: Organizing Your Application\\nDescribing how to actually build a worldwide application, especially with complex\\nsubjects like data replication, is beyond the scope of this chapter, but we will describe\\nhow to arrange the application configurations in the filesystem.\\nUltimately, a particular region’s configuration is conceptually the same as a stage in\\nthe deployment lifecycle. Thus, adding multiple regions to your configuration is iden‐\\ntical to adding new lifecycle stages. For example, instead of:\\n• Development\\n• Staging\\n• Canary\\n• Production\\nYou might have:\\n• Development\\n• Staging\\n• Canary\\n• EastUS\\n• WestUS\\n• Europe\\n• Asia\\nModeling this in the filesystem for configuration, this looks like:\\nfrontend/\\n staging/\\n templates -> ../v3/\\n parameters.yaml\\n eastus/\\n templates -> ../v1/\\n parameters.yaml\\n westus/\\n templates -> ../v2/\\n parameters.yaml\\n ...\\nIf you instead are using version control and tags, the filesystem would look like:\\nfrontend/\\n staging-parameters.yaml\\n eastus-parameters.yaml\\n westus-parameters.yaml\\n templates/\\n frontend-deployment.yaml\\n...\\nDeploying Your Application Around the World | 231\\nUsing this structure, you would introduce a new tag for each region and use the file\\ncontents at that tag to deploy to that region.\\nImplementing Worldwide Deployment\\nNow that you have configurations for each region around the world, the question\\nbecomes one of how to update those various regions. One of the primary goals of\\nusing multiple regions is to ensure very high reliability and uptime. While it would be\\ntempting to assume that cloud and data center outages are the primary causes of\\ndowntime, the truth is that outages are generally caused by new versions of software\\nrolling out. Because of this, the key to a highly available system is limiting the effect\\nor “blast radius” of any change that you might make. Thus, as you roll out a version\\nacross a variety of regions, it makes sense to move carefully from region to region in\\norder to validate and gain confidence in one region before moving on to the next.\\nRolling out software across the world generally looks more like a workflow than a\\nsingle declarative update: you begin by updating the version in staging to the latest\\nversion and then proceed through all regions until it is rolled out everywhere. But\\nhow should you structure the various regions, and how long should you wait to vali‐\\ndate between regions?\\nTo determine the length of time between rollouts to regions, you want to consider the\\n“mean time to smoke” for your software. This is the time it takes on average after a\\nnew release is rolled out to a region for a problem (if it exists) to be discovered. Obvi‐\\nously, each problem is unique and can take a varying amount of time to make itself\\nknown, and that is why you want to understand the average time. Managing software\\nat scale is a business of probability, not certainty, so you want to wait for a time that\\nmakes the probability of an error low enough that you are comfortable moving on to\\nthe next region. Something like two to three times the mean time to smoke is proba‐\\nbly a reasonable place to start, but it is highly variable depending on your application.\\nTo determine the order of regions, it is important to consider the characteristics of\\nvarious regions. For example, you are likely to have high-traffic regions and lowtraffic regions. Depending on your application, you may have features that are more\\npopular in one geographic area or another. All of these characteristics should be con‐\\nsidered when putting together a release schedule. You likely want to begin by rolling\\nout to a low-traffic region. This ensures that any early problems you catch are limited\\nto an area of little impact. Though it is not a hard-and-fast rule, early problems are\\noften the most severe, since they manifest quickly enough to be caught in the first\\nregion you roll out to. Thus, minimizing the impact of such problems on your cus‐\\ntomers makes sense. Next, you likely want to roll out to a high-traffic region. Once\\nyou have successfully validated that your release works correctly via the low-traffic\\nregion, you want to validate that it works correctly at scale. The only way to do this is\\nto roll it out to a single high-traffic region. When you have successfully rolled out to\\n232 | Chapter 18: Organizing Your Application\\nboth a low- and a high-traffic region, you may have confidence that your application\\ncan safely roll out everywhere. However, if there are regional variations, you may\\nwant to also test more slowly across a variety of geographies before pushing your\\nrelease more broadly.\\nWhen you put your release schedule together, it is important to follow it completely\\nfor every release, no matter how big or how small. Many outages have been caused by\\npeople accelerating releases either to fix some other problem, or because they\\nbelieved it to be “safe.”\\nDashboards and Monitoring for Worldwide Deployments\\nIt may seem an odd concept when you are developing at a small scale, but one signifi‐\\ncant problem that you will likely run into at a medium or large scale is having differ‐\\nent versions of your application deployed to different regions. This can happen for a\\nvariety of reasons (e.g., because a release has failed, been aborted, or had problems in\\na particular region), and if you don’t track things carefully you can rapidly end up\\nwith an unmanageable snowflake of different versions deployed around the world.\\nFurthermore, as customers inquire about fixes to bugs they are experiencing, a com‐\\nmon question will become: “Is it deployed yet?”\\nThus, it is essential to develop dashboards that can tell you at a glance what version is\\nrunning in which region, as well as alerting that will fire when too many different ver‐\\nsions of your application are deployed. A best practice is to limit the number of active\\nversions to no more than three: one testing, one rolling out, and one being replaced\\nby the rollout. Any more active versions than this is asking for trouble.\\nSummary\\nThis chapter provides guidance on how to manage a Kubernetes application through\\nsoftware versions, deployment stages, and regions around the world. It highlights the\\nprinciples that are the foundation of organizing your application: relying on the file‐\\nsystem for organization, using code review to ensure quality changes, and relying on\\nfeature flags or gates to make it easy to incrementally add and remove functionality.\\nAs with everything, the recipes in this chapter should be taken as inspiration, rather\\nthan absolute truth. Read the guidance, and find the mix of approaches that works\\nbest for the particular circumstances of your application. But keep in mind that in\\nlaying out your application for deployment, you are setting a process that you will\\nlikely have to live with for a number of years.\\nSummary | 233\\n\\nAPPENDIX A\\nBuilding a Raspberry Pi Kubernetes Cluster\\nWhile Kubernetes is often experienced through the virtual world of public cloud\\ncomputing, where the closest you get to your cluster is a web browser or a terminal, it\\ncan be a very rewarding experience to physically build a Kubernetes cluster on bare\\nmetal. Likewise, nothing compares to physically pulling the power or network on a\\nnode and watching how Kubernetes reacts to heal your application to convince you of\\nits utility.\\nBuilding your own cluster might seem like both a challenging and an expensive\\neffort, but fortunately it is neither. The ability to purchase low-cost, system-on-chip\\ncomputer boards, as well as a great deal of work by the community to make Kuber‐\\nnetes easier to install, means that it is possible to build a small Kubernetes cluster in a\\nfew hours.\\nIn the following instructions, we focus on building a cluster of Raspberry Pi\\nmachines, but with slight adaptations the same instructions could be made to work\\nwith a variety of different single-board machines.\\nParts List\\nThe first thing you need to do is assemble the pieces for your cluster. In all of the\\nexamples here, we’ll assume a four-node cluster. You could build a cluster of three\\nnodes, or even a cluster of a hundred nodes if you wanted to, but four is a pretty good\\nnumber.\\nTo start, you’ll need to purchase (or scrounge) the various pieces needed to build the\\ncluster. Here is the shopping list, with some approximate prices as of the time of\\nwriting:\\n235\\n1. Four Raspberry Pi 3 boards (Raspberry Pi 2 will also work)—$160\\n2. Four SDHC memory cards, at least 8 GB (buy high-quality ones!)—$30–50\\n3. Four 12-inch Cat. 6 Ethernet cables—$10\\n4. Four 12-inch USB A–Micro USB cables—$10\\n5. One 5-port 10/100 Fast Ethernet switch—$10\\n6. One 5-port USB charger—$25\\n7. One Raspberry Pi stackable case capable of holding four Pis—$40 (or build your\\nown)\\n8. One USB-to-barrel plug for powering the Ethernet switch (optional)—$5\\nThe total for the cluster comes out to be about $300, which you can drop down to\\n$200 by building a three-node cluster and skipping the case and the USB power cable\\nfor the switch (though the case and the cable really clean up the whole cluster).\\nOne other note on memory cards: do not scrimp here. Low-end memory cards\\nbehave unpredictably and make your cluster really unstable. If you want to save some\\nmoney, buy a smaller, high-quality card. High-quality 8 GB cards can be had for\\naround $7 each online.\\nOnce you have your parts, you’re ready to move on to building the cluster.\\nThese instructions also assume that you have a device capable of\\nflashing an SDHC card. If you do not, you will need to purchase a\\nUSB → memory card reader/writer.\\nFlashing Images\\nThe default Raspbian image now supports Docker through the standard install meth‐\\nods, but to make things even easier, the Hypriot project provides images with Docker\\npreinstalled.\\nVisit the Hypriot downloads page and download the latest stable image. Unzip the\\nimage, and you should now have an .img file. The Hypriot project also provides really\\nexcellent documentation for writing this image to your memory card for each of\\nthese platforms:\\n• macOS\\n• Windows\\n• Linux\\n236 | Appendix A: Building a Raspberry Pi Kubernetes Cluster\\nWrite the same image onto each of your memory cards.\\nFirst Boot: Master\\nThe first thing to do is to boot just your master node. Assemble your cluster, and\\ndecide which is going to be the master node. Insert the memory card, plug the board\\ninto an HDMI output, and plug a keyboard into the USB port.\\nNext, attach the power to boot the board.\\nLog in at the prompt using the username pirate and the password hypriot.\\nThe very first thing you should do with your Raspberry Pi (or any\\nnew device) is to change the default password. The default pass‐\\nword for every type of install everywhere is well known by people\\nwho will misbehave given a default login to a system. This makes\\nthe internet less safe for everyone. Please change your default\\npasswords!\\nSetting Up Networking\\nThe next step is to set up networking on the master.\\nFirst, set up WiFi. This is going to be the link between your cluster and the outside\\nworld. Edit the /boot/user-data file. Update the WiFi SSID and password to match\\nyour environment. If you ever want to switch networks, this is the file you need to\\nedit. Once you have edited this, reboot with sudo reboot and validate that your net‐\\nworking is working.\\nThe next step in networking is to set up a static IP address for your cluster’s internal\\nnetwork. To do this, edit /etc/network/interfaces.d/eth0 to read:\\nallow-hotplug eth0\\niface eth0 inet static\\n address 10.0.0.1\\n netmask 255.255.255.0\\n broadcast 10.0.0.255\\n gateway 10.0.0.1\\nThis sets the main Ethernet interface to have the statically allocated address 10.0.0.1.\\nReboot the machine to claim the 10.0.0.1 address.\\nNext, we’re going to install DHCP on this master so it will allocate addresses to the\\nworker nodes. Run:\\n$ apt-get install isc-dhcp-server\\nThen configure the DHCP server as follows (/etc/dhcp/dhcpd.conf):\\nBuilding a Raspberry Pi Kubernetes Cluster | 237\\n# Set a domain name, can basically be anything\\noption domain-name \"cluster.home\";\\n# Use Google DNS by default, you can substitute ISP-supplied values here\\noption domain-name-servers 8.8.8.8, 8.8.4.4;\\n# We\\'ll use 10.0.0.X for our subnet\\nsubnet 10.0.0.0 netmask 255.255.255.0 {\\n range 10.0.0.1 10.0.0.10;\\n option subnet-mask 255.255.255.0;\\n option broadcast-address 10.0.0.255;\\n option routers 10.0.0.1;\\n}\\ndefault-lease-time 600;\\nmax-lease-time 7200;\\nauthoritative;\\nYou may also need to edit /etc/defaults/isc-dhcp-server to set the INTERFACES environ‐\\nment variable to eth0.\\nRestart the DHCP server with sudo systemctl restart isc-dhcp-server.\\nNow your machine should be handing out IP addresses. You can test this by hooking\\nup a second machine to the switch via Ethernet. This second machine should get the\\naddress 10.0.0.2 from the DHCP server.\\nRemember to edit the /etc/hostname file to rename this machine to node-1.\\nThe final step in setting up networking is setting up network address translation\\n(NAT) so that your nodes can reach the public internet (if you want them to be able\\nto do so).\\nEdit /etc/sysctl.conf and set net.ipv4.ip_forward=1 to turn on IP forwarding. You\\nshould then reboot the server for this to take effect. Alternately you can run sudo\\nsysctl net.ipv4.ip_forward=1 to make the change without rebooting. If you\\nchoose to do this you will still want to edit /etc/sysctl.conf to make the setting\\npermanent.\\nThen edit /etc/rc.local (or the equivalent) and add iptables rules for forwarding\\nfrom eth0 to wlan0 (and back):\\niptables -t nat -A POSTROUTING -o wlan0 -j MASQUERADE\\niptables -A FORWARD -i wlan0 -o eth0 -m state \\\\\\n --state RELATED,ESTABLISHED -j ACCEPT\\niptables -A FORWARD -i eth0 -o wlan0 -j ACCEPT\\nAt this point, the basic networking setup should be complete. Plug in and power up\\nthe remaining two boards (you should see them assigned the addresses 10.0.0.3 and\\n10.0.0.4). Edit the /etc/hostname file on each machine to name them node-2 and\\nnode-3, respectively.\\n238 | Appendix A: Building a Raspberry Pi Kubernetes Cluster\\nValidate this by first looking at /var/lib/dhcp/dhcpd.leases, and then SSH to the nodes\\n(remember again to change the default password first thing). Validate that the nodes\\ncan connect to the external internet.\\nExtra credit\\nThere are a couple of extra steps you can take that will make it easier to manage your\\ncluster.\\nThe first is to edit /etc/hosts on each machine to map the names to the right addresses.\\nOn each machine, add:\\n...\\n10.0.0.1 kubernetes\\n10.0.0.2 node-1\\n10.0.0.3 node-2\\n10.0.0.4 node-3\\n...\\nNow you can use those names when connecting to those machines.\\nThe second is to set up passwordless SSH access. To do this, run ssh-keygen and then\\ncopy the $HOME/.ssh/id_rsa.pub file into /home/pirate/.ssh/authorized_keys on\\nnode-1, node-2, and node-3.\\nInstalling Kubernetes\\nAt this point you should have all nodes up, with IP addresses and capable of accessing\\nthe internet. Now it’s time to install Kubernetes on all of the nodes.\\nUsing SSH, run the following commands on all nodes to install the kubelet and\\nkubeadm tools. You will need to be root to execute these commands. Use sudo su to\\nelevate to the root user.\\nFirst, add the encryption key for the packages:\\n# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\\nThen add the repository to your list of repositories:\\n# echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" \\\\\\n >> /etc/apt/sources.list.d/kubernetes.list\\nFinally, update and install the Kubernetes tools. This will also update all packages on\\nyour system for good measure:\\n# apt-get update\\n$ apt-get upgrade\\n$ apt-get install -y kubelet kubeadm kubectl kubernetes-cni\\nBuilding a Raspberry Pi Kubernetes Cluster | 239\\nKubernetes uses a number of different kernel cgroups when it\\nstarts. It expects these capabilities to be present and can fail to start\\nif they are not. Make sure that you are running the latest kernel\\navailable in the Raspberry Pi distribution.\\nSetting Up the Cluster\\nOn the master node (the one running DHCP and connected to the internet), run:\\n$ sudo kubeadm init --pod-network-cidr 10.244.0.0/16 \\\\\\n --apiserver-advertise-address 10.0.0.1 \\\\\\n --apiserver-cert-extra-sans kubernetes.cluster.home\\nNote that you are advertising your internal-facing IP address, not your external\\naddress.\\nEventually, this will print out a command for joining nodes to your cluster. It will\\nlook something like:\\n$ kubeadm join --token=<token> 10.0.0.1\\nSSH onto each of the worker nodes in your cluster and run that command.\\nWhen all of that is done, you should be able to run this command and see your work‐\\ning cluster:\\n$ kubectl get nodes\\nSetting up cluster networking\\nYou have your node-level networking set up, but you need to set up the Pod-to-Pod\\nnetworking. Since all of the nodes in your cluster are running on the same physical\\nEthernet network, you can simply set up the correct routing rules in the host kernels.\\nThe easiest way to manage this is to use the Flannel tool created by CoreOS. Flannel\\nsupports a number of different routing modes; we will use the host-gw mode. You\\ncan download an example configuration from the Flannel project page:\\n$ curl https://rawgit.com/coreos/flannel/master/Documentation/kube-flannel.yml \\\\\\n > kube-flannel.yaml\\nThe default configuration that CoreOS supplies uses vxlan mode instead, and also\\nuses the AMD64 architecture instead of ARM. To fix this, open up that configuration\\nfile in your favorite editor; replace vxlan with host-gw and replace all instances of\\namd64 with arm.\\n240 | Appendix A: Building a Raspberry Pi Kubernetes Cluster\\nYou can also do this with the sed tool in place:\\n$ curl https://rawgit.com/coreos/flannel/master/Documentation/kube-flannel.yml \\\\\\n | sed \"s/amd64/arm/g\" | sed \"s/vxlan/host-gw/g\" \\\\\\n > kube-flannel.yaml\\nOnce you have your updated kube-flannel.yaml file, you can create the Flannel net‐\\nworking setup with:\\n$ kubectl apply -f kube-flannel.yaml\\nThis will create two objects, a ConfigMap used to configure Flannel and a DaemonSet\\nthat runs the actual Flannel daemon. You can inspect these with:\\n$ kubectl describe --namespace=kube-system configmaps/kube-flannel-cfg\\n$ kubectl describe --namespace=kube-system daemonsets/kube-flannel-ds\\nSetting up the GUI\\nKubernetes ships with a rich GUI. You can install it by running:\\n$ DASHSRC=https://raw.githubusercontent.com/kubernetes/dashboard/master\\n$ curl -sSL \\\\\\n $DASHSRC/src/deploy/recommended/kubernetes-dashboard-arm-head.yaml \\\\\\n | kubectl apply -f -\\nTo access this UI, you can run kubectl proxy and then point your browser to http://\\nlocalhost:8001/ui, where localhost is local to the master node in your cluster. To view\\nthis from your laptop/desktop, you may need to set up an SSH tunnel to the root\\nnode using ssh -L8001:localhost:8001 <master-ip-address>.\\nSummary\\nAt this point you should have a working Kubernetes cluster operating on your Rasp‐\\nberry Pis. This can be great for exploring Kubernetes. Schedule some jobs, open up\\nthe UI, and try breaking your cluster by rebooting machines or disconnecting the\\nnetwork.\\nBuilding a Raspberry Pi Kubernetes Cluster | 241\\n\\nIndex\\nA\\nabstraction pattern, 205\\nadmin role, 171\\nadmission controllers, 196\\nbuilt-in and custom, 196\\ncreating SSL-based admission controller,\\n203\\nimplementing to provide defaulting, 203\\ninstalling validating admission controller,\\n200\\nvalidating, 200\\nAdmissionReview type, 203\\naggregating ClusterRoles, 173\\nAmbassador Ingress controller, 100\\nannotations, 65, 71-73\\nchange-cause, 121\\ndefining, 72\\nin deployment template for update, 119\\nkubectl annotate command, 40\\nkubernetes.io/created-by, 108\\nkubernetes.io/ingress.class, 97\\nuses of, 72\\nAPIs\\ndecoupling servers via, 6\\nKubernetes API server request flow, 196\\nKubernetes API server, versions, 31\\napplication container images, 13\\nbuilding with Docker, 16-20\\nimage security, 19\\noptimizing image size, 18\\nusing Dockerfiles, 16\\nmultistage builds, 20\\napplication containers, 16\\napplication-oriented container APIs, benefits\\nof, 9\\napplications\\neasy scaling for, 6\\norganizing Kubernetes applications,\\n221-233\\ncode review, 222\\ndeploying your application worldwide,\\n230-233\\nfeature flag gates and guards, 223\\nfilesystem layout, 224\\nfilesystems as source of truth, 222\\nguiding principles, 221\\nmanaging applications in source control,\\n224\\nmanaging releases, 225\\nparameterizing applications with tem‐\\nplates, 229\\nstructuring for development, testing, and\\ndeployment, 227-228\\nversioning with branches and tags, 225\\nversioning with directories, 226\\nreal-world, deploying, 207-219\\nGhost, 211-214\\nJupyter, 207-209\\nParse, 209-211\\nRedis, 214-219\\nin same and different Pods, 46\\nauthentication, 168\\nauthentication providers supported by\\nKubernetes, 168\\nto container registries, 22\\nauthorization, 168\\ntesting with can-i, 172\\n243\\nautocompletion for commands and resources,\\n42\\nautoscaling, 6\\nReplicaSets, 110\\nbased on CPU usage, 111\\navailability, 1\\naz CLI tool, installing, 28\\nAzure Cloud Shell, 28\\nAzure Kubernetes Service, installing Kuber‐\\nnetes with, 28\\nB\\nbase64 encoding, 162\\nbash-completion package, 42\\nbranches, versioning with, 226\\nbuild image, 21\\nC\\ncaching, using volumes for, 60\\ncert-manager project, 99\\ncertificate signing request for Kubernetes API\\nserver, 203\\ncertificates\\nTLS, 99\\ncreating secret to store, 158\\nwebhook, 201\\ncgroup technology (Linux kernel), 24\\ncontainers in Pods, 46\\nchange-cause annotation, 119, 121\\nCLI (command-line interface)\\nAzure Cloud Shell, 28\\ndeploying containers with Docker CLI, 23\\nElastic Kubernetes Service and eksctl tool,\\n29\\nclients\\nclient-go library, informers, 199\\nrolling service updates and, 124\\ncloud\\ncontainer registries for different cloud pro‐\\nviders, 22\\nDNS name for databases and other services,\\n180\\ninfrastructure provided by, drawbacks of, 9\\nIngress implementations by providers, 99\\ninstalling Kubernetes on public cloud pro‐\\nvider, 28-29\\nAzure Kubernetes Service, 28\\nGoogle Kubernetes Engine (GKE), 28\\nKubernetes services, 27\\nKubernetes-as-a-Service (KaaS) on public\\nclouds, 8\\nload-balancing capabilities by providers, 90\\nLoadBalancer type, using, 81\\nstorage in, 178\\nusing DaemonSets to install software on\\nnodes, 132\\nvolume types for providers, 182\\ncloud-native applications, 1\\ncluster IPs, 77\\nDNS address for, 77\\nenvironment variables, 85\\nkube-proxy and, 84\\ncluster-admin permissions, 91\\ncluster-admin role, 171\\nClusterRoleBindings, 170\\nbuilt-in, 171\\nClusterRoles, 170\\naggregating, 173\\nbinding a group to, 174\\nbuilt-in, 171\\nmodifications of built-in roles, 171\\nclusters\\nautoscaling, 111\\ncloud-native, 132\\ncomponents, 34-36\\nKubernetes DNS server, 34\\nKubernetes proxy, 34\\nKubernetes UI, 35\\ndeploying, 27-31\\ninstalling Kubernetes locally, 29\\ninstalling Kubernetes on public cloud\\nprovider, 28-29\\nrunning Kubernetes in Docker, 30\\nrunning Kubernetes on Raspberry Pi, 31\\neasy scaling for, 6\\nexploring with kubectl, 31-34\\nchecking cluster status, 31\\nlisting worker nodes, 32\\nlisting running Pods in, 49\\nMongoDB cluster creation, automating,\\n189-192\\nviewing with tools other than kubectl, 42\\nCNAME records (DNS), 180\\ncode review for applications, 222\\ncommand-line arguments, ConfigMap used for,\\n155, 156\\ncommunication/synchronization, using vol‐\\numes, 60\\n244 | Index\\ncompilers, 205\\ncompute costs, forecasting with Kubernetes, 6\\nConfigMaps, 153-157\\ncreating, 153\\ncreating for Ghost, 212\\nghost-config-mysql, 213\\ncreating for Redis installation, 216\\ndata values, 162\\nmanaging, 162\\ncreating ConfigMaps, 163\\nlisting all ConfigMaps in a namespace,\\n162\\nupdates, 163-165\\nviewing raw data, 163\\nMongoDB ConfigMap, 190\\nnaming constraints, key names for data\\nitems, 161\\nusing, 154\\nusing to add script to MongoDB image, 190\\nconfigurations\\ncontainer configuration file, 16\\ndeclarative configuration in Kubernetes, 4\\ndeclarative configuration in Pod manifests,\\n47\\nfor deployments, 117\\nfor rolling update, 125\\nGhost application, 211\\nIngress controller configuration, typical, 90\\nkubectl configuration file, 37\\nmanaging for load balancer, 90\\nparameterizing, 229\\nRedis installation, 215\\nconsumers job, creating for a work queue, 149\\ncontainer images, 14-16\\ncleaning up or removing, 24\\nDocker format, 15\\nmaking reusable, 153\\nmultistage builds, 20\\nstoring in remote registry, 22\\nupdating, 119\\nContainer Network Interface (CNI) plug-ins,\\n196\\ncontainer registries, 14\\n(see also registries)\\ncontainers\\nand container APIs, benefits of, 2\\napplication container images, 13\\ncopying files to and from, 53\\ncreating for Jupyter application, 208\\ndecoupling application container image\\nfrom machines, 7\\nDocker runtime, 23\\nexisting containers, adoption by ReplicaSets,\\n105\\ngrouping in Pods, 46\\ncriteria for, 47\\nimmutable container images, 3\\nquarantining, 105\\nreadiness checks, 55\\nresource requests per container, 57\\nrestart policy for MongoDB container, 191\\nrunning in a Pod, information about, 51\\nsystem and application, 16\\ncontexts, managing with kubectl, 37\\nContour Ingress controller, installing, 91\\nconfiguring DNS, 92\\nconfiguring local hosts file, 92\\ncontroller-manager, 32\\ncontrollers for custom resources, 199\\ncore-dns server, 35\\ncpu-shares functionality (Linux kernel), 58\\nCPUs\\nautoscaling based on usage, 111\\ncapping usage with resource limits, 59\\nlimiting usage with Docker, 24\\nresource requests for, 57\\ncron jobs\\ndeclaring a CronJob in Kubernetes, 150\\nsetting up to run image garbage collector, 25\\ncurl utility, using to communicate with work\\nqueue, 148\\ncustom resources, 197\\nnaming, 197\\npatterns for, 204-205\\ncompilers, 205\\njust data, 204\\noperators, 205\\nvalidation and defaulting, 200\\nCustomResourceDefinition, 91, 197\\nexample definition, 197\\nspec subobject, 197\\nD\\nDaemonSets, 34, 131-138\\ncreating, 132-134\\ndeleting, 137\\nlimiting to specific nodes, 134-136\\nadding labels to nodes, 135\\nIndex | 245\\nusing node selectors, 135\\nscheduler, 132\\nsimilarities with ReplicaSets, 131\\nupdating, 136\\nRollingUpdate strategy, 136\\ndashboards\\nfor worldwide application deployment, 233\\nKubernetes UI, 35\\ndata items, specifying for ConfigMaps or\\nsecrets, 163\\ndebugging, kubectl commands for, 40-41, 52\\ndeclarative configuration, 47\\nand undoing rollouts, 122\\ndeclarative configuration objects, 4\\ndecoupled architectures, 5\\nDaemonSets and ReplicaSets in Kubernetes,\\n132\\nin Kubernetes, 104\\ndefault-http-backend service, 95\\ndependencies, 19\\ndeployment image, 21\\nDeployment object, 72, 113\\n(see also deployments)\\ndeleting only the Deployment object, 128\\nrevision history attached to, 122\\ndeployments, 113-129\\ncreating, 116\\ncreating with kubectl run, 76\\ndeleting, 128\\ndeploying real-world applications, 207-219\\nGhost, 211-214\\nJupyter, 207-209\\nParse, 209-211\\nRedis, 214-219\\ndeploying your application around the\\nworld, 230-233\\narchitectures for worldwide deployment,\\n230\\ndashboards and monitoring, 233\\nimplementing worldwide deployment,\\n232\\nediting to add readiness check, 78\\ninternal workings of, 114-115\\nKubernetes deployment lifecycle, 128\\nmanaging, 117\\nmonitoring, 128\\nstrategies for, 123\\nRecreate strategy, 123\\nRollingUpdate strategy, 123-126\\nslowing rollouts to ensure service health,\\n126\\nupdating, 118-123\\ncontainer image, 119\\nrollout history, 120\\nscaling deployments, 118\\ndevelopment teams, scaling with microservices,\\n7\\ndevelopment, structuring your application for,\\n227\\nintroducing a development tag, 228\\ndirectories, versioning with, 226\\ndisk space on a node, getting information\\nabout, 33\\ndistributed systems, reliable and scalable, 1\\nDNS\\naddress for cluster IP, 77\\nconfiguring to external address for load bal‐\\nancer, 92\\nentries for MongoDB StatefulSet, 188\\nKubernetes DNS server, 34\\nKubernetes DNS service, 77\\nname resolution, limitations of, 75\\nnames for external services, 179\\nDocker, 14\\nbuilding application images with, 16-20\\nCLI tool, deploying container with, 23\\ncontainer runtime, 23\\nimage format, 15\\nprivate registries, storing access credentials,\\n160\\nrunning Kubernetes in, 30\\nDocker Desktop, Kubernetes installation with,\\n29\\nDocker Hub, 22\\ndocker images command, 25\\ndocker login command, 22\\ndocker rmi command, 24\\ndocker run command\\n--cpu-shares flag, 24\\n--memory and --memory-swap flags, 24\\n--publish, -d, and --name flags, 23\\ndocker system prune tool, 25\\nDocker-in-Docker clusters, 27\\nDockerfiles, 17\\nfor multistage application builds, 21\\n.dockerignore file, 17\\ndynamic volume provisioning, 185\\n246 | Index\\nE\\nEclipse, 42\\nedit role, 171\\nin ClusterRole aggregation, 173\\neditors\\nediting a deployment in, 78\\nplug-ins for integration with Kubernetes, 42\\nefficiency provided by Kubernetes, 10\\nEKS (Elastic Kubernetes Service), 29\\neksctl command-line tool, 29\\nelasticsearch tool, 53\\nendpoints, 82\\nfor default HTTP backend service, 95\\nfor external services, 180\\nwatch command on, 79\\nenvironment variables\\nfor cluster IPs, 85\\nfor parse-server, 209\\nsetting using ConfigMap, 155, 156\\nEnvoy load balancer, 91\\netcd server, 32\\nevents related to Pods, 50\\ngetting information on, 51\\nkilling a container, 55\\nexec probes, 56\\nrunning commands in your container with,\\n53\\nextending Kubernetes, 195-206\\ngetting started, using Kubebuilder project\\nlibrary, 205\\nmeaning of, 195\\npatterns for custom resources, 204-205\\npoints of extensibility, 196-204\\nExternalName type, 179\\nF\\nfeature flag gates and guards, 223\\nfilesystems\\nConfigMap defining small filesystem, 153,\\n154\\nlayout for Kubernetes application, 224\\nlayout for parameterization, 230\\nmounting host filesystem using volumes, 61\\noverlay, 15\\nsource of truth for Kubernetes applications,\\n222\\nfluentd tool, 53\\ncreating fluentd logging agent on every\\nnode in target cluster, 132-134\\nforecasting future compute costs, 6\\nG\\ngarbage collection, setting up for container\\nimages, 25\\ngcloud tool, installing, 28\\nGhost, 211-214\\nconfiguring, 211\\nusing MySQL for storage, 213\\nGitHub\\ncert-manager on, 99\\nminikube on, 30\\nparse-server page, 211\\nGloo Ingress controller, 100\\nGoogle Cloud Platform, 28\\nGoogle Container Registry, 22\\nGoogle Kubernetes Engine (GKE), 28, 81\\ngrace period for Pod termination, 51\\ngraceful shutdown, 79\\ngroups, 168\\nidentity and, 174\\nusing for role bindings, 173\\nH\\nheadless services, 188\\nhealth checks, 54-56\\nexternal services and, 181\\nfor replicated Redis cluster, 214\\nliveness probes, 54\\nreadiness probe, 55, 78, 127\\ntcpSocket and exec probes, 56\\nheapster Pod, 110\\nHelm, parameterizing configurations with, 229\\nheptio-contour namespace, 91\\nHorizontal Pod Autoscaling (HPA), 110\\nno direct link between ReplicaSets and, 111\\nhorizontal vs. vertical scaling, 111\\nhostnames\\nIngress and namespaces, 97\\nusing with Ingress, 94\\nhostPath volume, 61\\nhosts\\nconfiguring local hosts file for Contour, 92\\nmultiple paths on same host in Ingress sys‐\\ntem, 96\\nHTTP health checks, 54, 79\\nHTTP load balancing (see load balancing)\\nHTTPS, webhook access via, 201\\nhypervisors, 30\\nIndex | 247\\nI\\nidentity in Kubernetes, 168\\ngroups, 174\\nimage pull secrets, 161\\nimmutability\\ndeclarative configuration and, 4\\nvalue of, 3\\nimperative vs. declarative configuration, 4, 47\\nInformer pattern, 199\\ninfrastructure\\nabstracting, 9\\nas code, 4\\nimmutable, 3, 132\\nIngress, 7, 89-101\\nadvanced topics and gotchas, 96-99\\nIngress and namespaces, 97\\nmultiple Ingress objects, 97\\npath rewriting, 98\\nrunning multiple Ingress controllers, 97\\nserving TLS, 98\\nalternate implementations, 99\\ninstalling Contour controller, 91\\nconfiguring DNS, 92\\nconfiguring local hosts file, 92\\nresource specification vs. controller imple‐\\nmentation, 90\\ntypical software Ingress controller configu‐\\nration, 90\\nusing, 92\\nhostnames, 94\\npaths, 95\\nsimplest usage, 93\\nIntelliJ, 42\\nIP addresses\\ncluster IP, 77\\nexternal IP for Contour, 91\\nexternal services and, 179\\nfor watched service endpoints, 82\\nhosting many HTTP sites on single address,\\n89\\nkube-proxy and cluster IPs, 84\\nunique, for services of type LoadBalancer,\\n92\\niptables rules, 84\\nIstio project, 100\\nJ\\njobs, 139-151\\nCronJob, 150\\nJob object, 139\\npatterns, 140-150\\nwork queues, 146-150\\nJSON\\nfiles representing Kubernetes objects, 39\\nwork queue items, 148\\nJupyter, 207-209\\njust data pattern, 204\\nK\\nKaaS (Kubernetes-as-a-Service), 8\\nkube-apiserver, --service-cluster-ip-range flag,\\n85\\nkube-dns server, 35\\nkube-proxy, 34, 86\\ncluster IPs and, 84\\nkube-system namespace, 34, 95\\nlisting Pods in, 110\\nkubeadm, 27\\nKubebuilder project, library for Kubernetes API\\nextensions, 205\\nkubectl tool, 31-34\\nchecking cluster status, 31\\ncommands, 37-43\\napply, 49, 91, 93, 107, 110\\napply -f, 164\\nauth can-i, 172\\nauth reconcile, 172\\nautocompletion, 42\\nautoscale, 111\\ncontexts, 37\\ncp, 53\\ncreate, 114\\ncreate configmap, 163\\ncreate secret, 159, 163\\ncreate secret docker-registry, 161\\ncreate secret generic, 164\\ncreate secret tls, 98\\ncreating, updating, and destroying\\nKubernetes objects, 39\\ndebugging, 40-41\\ndelete, 48, 51, 111, 128, 137\\ndescribe, 50, 80, 93, 108, 117, 159\\nedit, 78, 80\\nedit configmap, 165\\nexec, 53\\nexpose, 76\\nget, 48, 49, 68, 93\\nget clusterrolebindings, 171\\n248 | Index\\nget clusterroles, 171\\nget configmaps, 162\\nget secrets, 162\\nlabel, 68, 135\\nlabeling and annotating objects, 40\\nlogs, 52\\nnamespaces, 37\\nport-forward, 52, 77\\nreplace --save-config, 117\\nreplace -f, 164, 164\\nrolling-update, 114\\nrollout, 118, 120, 137\\nrollout history deployment, 120\\nrollout pause deployments, 120\\nrollout resume deployments, 120\\nrollout undo deployments, 121\\nrun, 48\\nscale, 109, 115\\nviewing Kubernetes API objects, 38\\ndefault-http-backend service, 95\\ngetting more information via command-line\\nflags, 50\\ninstalling, 29, 29\\nlisting worker nodes on a cluster, 32\\nkubelet tool, 49\\nmanaging secrets volumes, 159\\nterminating containers for excess memory\\nusage, 58\\nKubernetes objects, 38, 39\\n(see also objects (Kubernetes))\\nkubernetes service, 76\\nKubernetes-as-a-Service (KaaS), 8\\nkubernetes.io/created-by annotation, 108\\nkubernetes.io/ingress.class annotation, 97\\nL\\nlabels, 65-71\\nadding to nodes, 135\\napplying, 67\\nDeployment object, 114\\nexternal services without label selectors, 179\\nfor Kubernetes objects, using kubectl, 40\\nfor Pods managed by a ReplicaSet, 108\\nfor Pods, use by ReplicaSets, 107\\nin Kubernetes architecture, 71\\nkey/value pairs, rules for, 66\\nmodifying, 68\\nmodifying on sick Pod, 105\\nmotivations for using, 66\\nnode selectors, 135\\nselectors, 68, 76\\nin API objects, 70\\nusing in service discovery, 83\\nusing to run DaemonSet Pods on specific\\nnodes, 131\\n“Let\\'s Encrypt” free certificate authority, 99\\nlibraries, external and shared, 13\\nlifecycle stages, mapping to revisions, 228\\nLinkerd project, 100\\nlive updates, 165\\nliveness probes, 54\\nload balancers\\ncloud-based, configuring with Ingress\\nobjects, 99\\ndecoupling components via, 6\\nload balancing, 7\\nfor Kubernetes dashboard server, 35\\nfor Kubernetes DNS server, 35\\nHTTP load balancing with Ingress, 89-101\\nalternate Ingress implementations, 99\\nIngress spec vs. Ingress controllers, 90\\ninstalling Contour controller, 91-92\\nusing Ingress, 92-96\\nLoadBalancer type, 81, 89\\nLoadTest custom resource, 198\\ncreating, 199\\nvalidation, 200\\nlogging agent fluentd, creating on every node in\\ntarget cluster, 132\\nlogical AND, 69\\nlogin token for Jupyter application, 208\\nlogs\\nfor Jupyter application container, 208\\ngetting information on Pods from, 52\\nlog aggregation services, 53\\ntesting with auth can-i, 172\\nM\\nmachine/operating system (OS), decoupling\\nfrom application container, 8\\nmanifests (Pod), 47\\nadding volume to, 59\\ncreated by ReplicaSets, 106\\ncreating, 48\\ndeclaring secrets volume, 160\\nusing to delete a Pod, 51\\nmaster and slave replicas, Redis installation,\\n215\\nIndex | 249\\nmaster nodes, 32\\nmaxSurge parameter (rolling updates), 126\\nmaxUnavailable parameter (rolling updates),\\n125, 137\\nmemory\\ncapping usage with resource limits, 59\\nlimiting usage for applications in container,\\n24\\non a node, getting information about, 33\\nrequests for, 58\\nmeta-resources, 197\\nmetadata\\nlabels providing metadata for objects, 65\\nmetadata section in Kubernetes objects,\\nannotation definitions in, 73\\nmetadata section (Pod manifests), 49\\nmicroservices, 6\\nbuilding with Kubernetes, advantages of, 7\\nrepresented by ReplicaSets, 105\\nminikube, 27\\nminikube tunnel command, 91\\nusing to install Kubernetes locally, 29\\nminReadySeconds parameter\\nfor DaemonSets, 137\\nfor deployments), 127\\nmobile application, accessing Kubernetes clus‐\\nter from your phone, 42\\nmodularity in Kubernetes, 104\\nMongoDB\\ncluster creation, automating, 189-192\\nmanually replicated with StatefulSets,\\n187-189\\nreadiness probes for Mongo-serving con‐\\ntainers, 193\\nuse of MongoDB cluster by Parse, 209\\nmultistage image builds, 20\\nmultitenant security, 167\\nmutable vs. immutable infrastructure, 3\\nMutatingWebhookConfiguration, 203\\nMySQL databases\\nrunning a MySQL singleton, 181-185\\nusing with Ghost application, 213\\nN\\nnamespaces, 7, 11\\ncreating for Jupyter application, 207\\ndefault, changing with kubectl, 37\\ndeployment of Kubernetes objects into, 178\\nheptio-contour, 91\\nin annotation keys, 72\\nin kubectl commands, 37\\nIngress and, 97\\nnetwork traffic, restricting in a cluster, 71\\nnetwork-based storage, 61\\nNetworkPolicy, 71\\nNewReplicaSet, 118\\nNFS persistent volume object, 182\\nNGINX ingress controller, 100\\nNodePorts, 80, 89\\nservices type for installing Contour, 91\\nnodes\\nlimiting DaemonSets to specific nodes,\\n134-136\\nadding labels to nodes, 135\\nusing node selectors, 135\\nlisting worker nodes for Kubernetes cluster,\\n32\\nnodeName field in Pod specs, 132\\nresource use by, monitoring with kubectl, 41\\n“not my monkey, not my circus” line, 8\\nO\\nobjects (Kubernetes), 38\\nannotations, 71-73\\ncreating, updating, and destroying, 39\\nlabeling and annotating with kubectl, 40\\nlabels providing metadata for, 65\\nselectors in, 70\\nOldReplicaSets, 118\\nOpenAPI, 200\\noperations teams, decoupling of, 8\\noperators\\nextensions using, 205\\nhealth detection and healing with, 5\\nOSI model, 89\\noverlay filesystems, 15\\nP\\nParse application, 209-211\\nbuilding the parse-server, 209\\ndeploying the parse-server, 209\\nprerequisites for, 209\\ntesting, 210\\npaths\\nin HTTP requests, using with Ingress, 95\\nIngress and namespaces, 97\\nrewriting with Ingress controller, 98\\nPending state, 50\\n250 | Index\\npersistent volume claim template, 193\\nPersistentVolume, 10\\nPersistentVolumeClaim, 10, 183\\nreclamation policy, and lifespan of persis‐\\ntent volumes, 186\\nreferring to a storage class, 185\\nPersistentVolumes, 51\\npersisting data with volumes, 59-61\\nPods, 7, 45-63\\naccessing, 52\\ncopying files to/from containers, 53\\ngetting more information with logs, 52\\nrunning commands in container with\\nexec, 53\\nusing port forwarding, 52\\nConfigMaps and, 153\\ncreated by ReplicaSet using Pod template,\\n106\\ncreating via kubectl run command, 48\\ncurrently running in a cluster, listing and\\nshowing labels, 69\\ncurrently running on a node, getting infor‐\\nmation about, 33\\nDaemonSets determining which node Pods\\nrun on, 132\\ndecoupling from DaemonSets, 132\\ndeleting, 51\\ndesigning, question to ask, 46\\nexample Pod with two containers and\\nshared filesystem, 45\\nfinding a ReplicaSet from, 108\\nfinding set of Pods for a ReplicaSet, 108\\ngetting details about, 50\\nhealth checks, 54-56\\nliveness probes, 54\\nother types of, 56\\nreadiness probe, 55, 78, 127\\nHorizontal Pod Autoscaling (HPA), 110\\nimagePullSecrets spec field, 161\\nin Kubernetes, 46\\nmanaged by DaemonSets, deleting (or not),\\n137\\nmanaged by ReplicaSets\\nautomatic rescheduling in failures, 103\\ndeleting, 111, 128\\nmanifests, 47\\ncreating, 48\\ndeclaring secrets volume, 160\\nmaximum number unavailable during roll‐\\ning updates, 125, 137\\nnode selectors in Pod spec when creating\\nDaemonSets, 135\\npersisting data with volumes, 59-61\\nadding volumes to Pods, 59\\nusing volumes for communication/\\nsynchronization, 60\\nPod template in deployment specs, 117\\nrelating Pods and ReplicaSets, 104\\nreplicated sets of, 103\\n(see also ReplicaSets)\\nreplicating a set of, reasons for, 131\\nresource management for, 56-59\\ncapping usage with resource limits, 58\\nmonitoring resource use with kubectl, 41\\nresource requests and minimum\\nrequired resources, 56\\nrunning, 49\\nin parallel for consumers job, 149\\nlisting running Pods in a cluster, 49\\nsingleton Pod running MySQL, 184\\nupdating in RollingUpdate strategy, 123\\nport forwarding, 77\\nfor updated deployment definition, 79\\nsetting up for Jupyter container, 208\\ntesting with auth can-i, 172\\nusing to access a Pod, 52\\nusing to connect to work queue daemon,\\n147\\nport-forward command (kubectl), 41\\nports\\nin deployment definitions, 76\\nNodePort feature, 80\\nprivate container registries, 22\\nstoring access credentials for private Docker\\nregistries, 160\\nprocess health checks, 54\\n(see also health checks)\\nprogressDeadlineSeconds parameter (deploy‐\\nments), 127, 129\\nProgressing type, 129\\nproxy (Kubernetes), 34\\n(see also kube-proxy)\\npublic container registries, 22\\nR\\nRaspberry Pi\\nbuilding a Kubernetes cluster, 235-241\\nIndex | 251\\nrunning Kubernetes on, 31\\nRBAC (see role-based access control)\\nrbac.authorization.kubernetes.io/autoupdate\\nannotation, 171\\nreadiness probes, 55, 127\\nfor Mongo-serving containers, 193\\nfor services, 78\\nreconciliation loops, 104, 132\\nRecreate strategy, 123\\nRedis, 214-219\\nconfiguring, 215\\ncreating a Redis service, 216\\ndeploying Redis cluster, 217\\nredis-sentinel, 214\\nredis-server, 214\\ntesting our Redis cluster, 218\\nregistries (container), 14\\nprivate Docker registries, storing access cre‐\\ndentials, 160\\nstoring container images in remote registry,\\n22\\nregular expressions\\nfor key names in ConfigMap or secret key,\\n161\\nfor path rewriting by Ingress controllers, 98\\nreleases, progression of, 227-228\\ndevelopment tag, 228\\nmapping stages to revisions, 228\\nreliability, 1\\nremote disks, persisting data with, 61\\nreplicas (StatefulSets), 187\\nReplicaSets, 48, 103-112\\nautoscaling\\nno direct link between HPA and Replica‐\\nSets, 111\\ncreating, 107\\ncreating for MySQL singleton Pod, 183\\ncreating to manage singleton work queue\\ndaemon, 146\\ndeleting, 111\\ndesigning with, 105\\nfor MongoDB, 189\\ninspecting, 108\\nfinding a ReplicaSet from a Pod, 108\\nfinding a set of Pods for a ReplicaSet,\\n108\\nmanaged by a deployment, 114\\ndeleting, 128\\nfields pointing to, 118\\nold and new, managed by a deployment, 120\\nreconciliation loops, 104\\nrelating Pods and ReplicaSets, 104\\nrelationship between deployments and, 115\\nscaling, 109-111\\nautoscaling, 110\\ndeclarative scaling with kubectl apply,\\n109\\nimperative scaling with kubectl scale,\\n109\\nsimilarities with DaemonSets, 131\\nspecification for, 106\\nlabels, 107\\nPod templates, 106\\nresource management, 56-59\\ncapping usage with resource limits, 58\\nresource requests and minimum required\\nresources, 56\\nrequest limit details, 57\\nresources\\ncustom, 197\\n(see also custom resources)\\nexternal, connecting to Kubernetes services,\\n86\\nisolation with containers, 46\\nlimiting applications’ usage, 24\\nmonitoring use of with kubectl top com‐\\nmand, 41\\nREST API, Kubernetes, 159\\nrestart policy for Pods, 55, 191\\nrevisionHistoryLimit property, 123\\nrevisions, 120\\nmapping to stages, 228\\nspecifying revision of 0, 122\\nRole object, 169\\nrole-based access control (RBAC), 167-175\\nadvanced topics\\naggregating ClusterRoles, 173\\nusing groups for bindings, 173\\ngeneral concept of roles and role bindings,\\n169\\nidentity in Kubernetes, 168\\nmultitenant security and, 167\\nroles and role bindings in Kubernetes, 169\\nauto-reconciliation of built-in roles, 171\\nusing built-in roles, 171\\nverbs for Kubernetes roles, 170\\ntechniques for managing, 172\\nmanaging in source control, 172\\n252 | Index\\ntesting authorization with can-i, 172\\nRoleBinding object, 169\\ncreating, 170\\nrolling updates, 114\\nRollingUpdate strategy, 123\\nconfiguring a rolling update, 125\\nmanaging multiple versions of your service,\\n124\\nusing with DaemonSets, 136\\nrollout commands for deployments, 118\\nrollouts\\ncurrent status of DaemonSet rollout, 137\\nhistory of, 120\\nand undoing a deployment, 122\\nmonitoring, 120\\npausing, 120\\nresuming, 120\\nslowing to ensure service health, 126\\nundoing last rollout, 121\\nroot path (/), 96\\nS\\nscalability, 1\\nscaling your service and your teams, 5-9\\ndecoupled architectures, 6\\neasy scaling for applications and clusters,\\n6\\nscaling development teams with micro‐\\nservices, 7\\nseparation of concerns for consistency\\nand scaling, 8\\nscaling\\ndeployments, 115, 118\\nReplicaSets, 109-111, 115\\nautoscaling, 110\\ndeclarative scaling with kubectl apply,\\n109\\nimperative scaling with kubectl scale,\\n109\\nscheduler, 32\\nDaemonSets and, 132\\nplacing Pods onto nodes, 48\\nsecrets, 158-161\\nconsuming, 159\\nsecrets volumes, 159\\nstoring access credentials for private\\nDocker registries, 160\\ncreating, 158\\ndata values, 162\\nmanaging, 162\\ncreating secrets, 163\\nlisting all secrets in current namespace,\\n162\\nupdates, 163-165\\nviewing raw data, 163\\nnaming constraints, key names for data\\nitems, 161\\nspecifying with TLS certificate and keys, 98\\nsecurity, 167\\n(see also authentication; role-based access\\ncontrol; secrets)\\nfor application container images, 19\\nselectors (label), 68, 76\\nfiltering nodes based on labels, 135\\nfinding Pods matching, 108\\nidentifying ClusterRoles to be aggregated,\\n173\\nin API objects, 70\\nin ReplicaSet spec section, 107\\nin the Kubernetes architecture, 71\\nselector operators, 70\\nself-healing systems, 5\\nseparation of concerns, 8\\nservice accounts, 168\\nservice discovery, 75-87\\nadvanced details, 82-85\\ncluster IP environment variables, 85\\nendpoints, 82\\nkube-proxy and cluster IPs, 84\\nmanual service discovery, 83\\ncloud integration, 81\\nconnecting with other environments, 86\\ndefined, 75\\nlooking beyond the cluster, 79\\nreadiness checks, 78\\nservice DNS, 77\\nService object, 76\\nService Meshes, 100\\nService object, 84\\nEndpoints object for, 82\\noperating at OSI level 4, 89\\nservices\\nbackend, creating, 92\\ncreating Redis service, 216\\ndefault-http-backend, 95\\nensuring health by slowing rollouts, 126\\nexposing Ghost service, 213\\nexposing MySQL singleton as, 184\\nIndex | 253\\nhosting multiple services on paths of a sin‐\\ngle domain, 95\\nimporting external storage services, 178-181\\nlimitation, no health checking, 181\\nservices without label selectors, 179\\nIngress and namespaces, 97\\nKubernetes, 7\\nmanaging multiple versions during rolling\\nupdates, 124\\nof type LoadBalancer, 90\\n(see also Ingress; LoadBalancer type)\\nqueue service, creating, 147\\nstateless, ReplicaSets designed for, 106\\nshutdown, graceful, 79\\nsingletons\\nrunning reliable singletons for storage,\\n181-186\\ndynamic volume provisioning, 185\\nsoftware on a node, getting information about,\\n33\\nsource control\\nmanaging RBAC in, 172\\nmanaging your application in, 224\\nstoring declarative configuration in, 4\\nspec for CustomResourceDefinition, 197\\nspec for ReplicaSets, 106\\nspec section (Pod manifests), 49\\nspec.type field, 80, 81\\nspec.volume section (Pod manifest), 59\\nSSH tunneling, 80\\nstate\\nDaemonSet management of, 132\\ndesired state of a deployment, matching, 115\\nof all replicas managed by a ReplicaSet, 108\\nreconciliation loop approach to managing,\\n104\\nstateless services, 106\\nupdating desired state of a deployment, 119\\nStatefulSets, 186-194\\nfor Redis cluster, 217\\nmanually replicated MongoDB with,\\n187-189\\nMongoDB cluster creation, automating,\\n189-192\\npersistent volumes and, 192\\nproperties of, 187\\nRedis deployment, wrapper scripts for, 215\\nstatus.conditions array (deployments), 128\\nstorage solutions, integrating with Kubernetes,\\n177-194\\nimporting external services, 178-181\\nlimitation, no health checking, 181\\nservices without selectors, 179\\nMySQL database for Ghost storage, 213\\nnative storage with StatefulSets, 186-194\\nmanually replicated MongoDB, 187-189\\npersistent volumes and StatefulSets, 192\\nreadiness probes for Mongo-serving\\ncontainers, 193\\nrunning reliable singletons, 181-186\\ndynamic volume provisioning, 185\\nMySQL singleton, 181-185\\nStorageClass objects, 185\\nstrategy object, 117\\nsystem containers, 16\\nsystem daemons, deploying, 131\\n(see also DaemonSets)\\nsystem:unauthenticated group, 168\\ncluster role allowing access to API server,\\n171\\nT\\ntab completion for commands and resources,\\n42\\ntags, source-control, 226\\nTCP, 89\\ntcpSocket health checks, 56\\ntemplates\\nannotation in template for deployment with\\nupdate information, 119\\nfor ReplicaSets and Pods, 68\\n(see also deployments)\\nparameterizing applications with, 229\\nPod template in deployment specs, 117\\nTerminating state, 51\\ntesting\\ncreating test environments with Kubernetes,\\n11\\nstructuring your application for, 227\\ntiming out a rollout, 127, 128\\nTLS key and certificate, creating secret to store,\\n158\\nTLS, serving in Ingress system, 98\\ntokens (login) for Jupyter application, 208\\nTraefik, 100\\n254 | Index\\nU\\nUDP, 89\\nUI (Kubernetes), 35\\nundoing rollouts, 121\\nupdate strategy, configuring for DaemonSets,\\n136\\nupdating ConfigMaps or secrets, 163\\nediting ConfigMap current version, 164\\nlive updates, 165\\nrecreate and update, 164\\nupdate from file, 164\\nuser accounts, 168\\nUTF-8 text, ConfigMap data values, 162\\nutilization, 56\\nV\\nValidatingWebhookConfiguration, 200\\nvalidation, adding to custom resource, 200\\nvelocity in software development, 2\\ndeclarative configuration, 4\\nimmutability, value of, 3\\nself-healing systems, 5\\nversioning in Kubernetes, 31\\nmanaging periodic versions of applications,\\n225\\nusing directories, 226\\nwith branches and tags, 225\\nvertical vs. horizontal scaling, 111\\nview role, 171\\nvirtual hosting, 89\\nvirtualbox, 30\\nVisual Studio Code, 42\\nvolume.beta.kubernetes.io/storage-class anno‐\\ntation, 186\\nvolumes, persisting data with, 59-61\\nConfigMap volume, creating, 155\\ndifferent ways to use volumes with Pods, 60\\ncaching, 60\\ncommunication/synchronization, 60\\ndynamic volume provisioning, 185\\nmongo-init ConfigMap volume, 190\\npersistent volume for MySQL singleton, 182\\npersistent volumes and StatefulSets, 192\\nsecrets volumes, 159\\nusing volumes with Pods, 59\\nW\\nwatch command, 79, 208\\nwatches, 199\\nwebhooks\\nMutatingWebhookConfiguration, 203\\nValidatingWebhookConfiguration, 200\\nwork queues, 146-150\\ncreating the consumers job, 149\\nloading up, 148\\nstarting, 146\\nworker nodes, listing for Kubernetes cluster, 32\\nY\\nYAML\\ndeployment.yaml file, 114, 116\\nfiles representing Kubernetes objects, 39\\nfor ConfigMap object, 154\\nhost-ingress.yaml file, 94\\npath-ingress.yaml file, 95\\nsimple-ingress.yaml file, 93\\ntls-secret.yaml file, 98\\nIndex | 255\\nAbout the Authors\\nBrendan Burns began his career with a brief stint in the software industry followed\\nby a PhD in robotics focused on motion planning for human-like robot arms. This\\nwas followed by a brief stint as a professor of computer science. Eventually, he\\nreturned to Seattle and joined Google, where he worked on web search infrastructure\\nwith a special focus on low-latency indexing. While at Google, he created the Kuber‐\\nnetes project with Joe Beda and Craig McLuckie. Brendan is currently a director of\\nengineering at Microsoft Azure.\\nJoe Beda started his career at Microsoft working on Internet Explorer (he was young\\nand naive). Throughout 7 years at Microsoft and 10 at Google, Joe has worked on\\nGUI frameworks, real-time voice and chat, telephony, machine learning for ads, and\\ncloud computing. Most notably, while at Google, Joe started the Google Compute\\nEngine and, along with Brendan Burns and Craig McLuckie, created Kubernetes.\\nAlong with Craig, Joe founded and sold a startup (Heptio) to VMware, where he is\\nnow a principal engineer. Joe proudly calls Seattle home.\\nKelsey Hightower is a Principal Developer Advocate at Google working on Google’s\\nCloud Platform. He has helped develop and refine many Google Cloud Products\\nincluding Google’s Kubernetes Engine, Cloud Functions, and Apigees’s API Gateway.\\nKelsey spends most of his time with executives and developers spanning the global\\nFortune 1000, helping them understand and leverage Google technologies and plat‐\\nforms to grow their businesses. Kelsey is a huge open source contributor, maintaining\\nprojects that aid software developers and operations professionals in building and\\nshipping cloud native applications. He is an accomplished author and keynote\\nspeaker, and was the inaugural winner of the CNCF Top Ambassador award for help‐\\ning bootstrap the Kubernetes community. He is a mentor and technical advisor, help‐\\ning founders turn their visions into reality.\\nColophon\\nThe animal on the cover of Kubernetes: Up and Running is an Atlantic white-sided\\ndolphin (Lagenorhynchus acutus). As its name suggests, the white-sided dolphin has\\nlight patches on its sides and a light gray strip that runs from above the eye to below\\nthe dorsal fin. It is among the largest species of oceanic dolphins, and ranges\\nthroughout the North Atlantic Ocean. It prefers open water, so it is not often seen\\nfrom the shore, but will readily approach boats and perform various acrobatic feats.\\nWhite-sided dolphins are social animals commonly found in large groups (known as\\npods) of about 60 individuals, though the size will vary depending on location and\\nthe availability of food. Dolphins often work as a team to harvest schools of fish, but\\nthey also hunt individually. They primarily search for prey using echolocation, which\\nis similar to sonar. The bulk of this marine mammal’s diet consists of herring, mack‐\\nerel, and squid.\\nThe average lifespan of the white-sided dolphin is between 22–27 years. Females only\\nmate every 2–3 years, and the gestation period is 11 months. Calves are typically born\\nin June or July, and are weaned after 18 months. Dolphins have very great intelligence\\nand display complex social behaviors like grieving, cooperation, and problem solving,\\ndue to their high brain-to-body ratio (the highest among aquatic mammals).\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world.\\nThe cover illustration is by Karen Montgomery, based on a black and white engraving\\nfrom British Quadrupeds. The cover fonts are Gilroy Semibold and Guardian Sans.\\nThe text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed;\\nand the code font is Dalton Maag’s Ubuntu Mono.\\nThere’s much more\\nwhere this came from.\\nExperience books, videos, live online\\ntraining courses, and more from O’Reilly\\nand our 200+ partners—all in one place.\\nLearn more at oreilly.com/online-learning\\n©2019 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase\n",
    "text = data.lower()\n",
    "\n",
    "# punctuation\n",
    "text = re.sub(\"\\[.*?\\]\", \"\", text)\n",
    "\n",
    "# punctuation\n",
    "text = re.sub(\"\\[%s]\" % re.escape(string.punctuation), \"\", text)\n",
    "\n",
    "# digits\n",
    "text = re.sub(\"\\w*\\d\\w*]\", \"\", text)\n",
    "\n",
    "# ticks and apostrophs\n",
    "text = re.sub(\"[´´\"\"_]\", \"\", text)\n",
    "\n",
    "# linebreaks\n",
    "text = re.sub(\"\\n\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brendan burns, joe beda & kelsey hightower kubernetes  up & running dive into the future of infrastructure second edition  brendan burns, joe beda, and kelsey hightower kubernetes: up and running dive into the future of infrastructure second edition beijing boston farnham sebastopol tokyo 978-1-492-04653-0  kubernetes: up and running by brendan burns, joe beda, and kelsey hightower copyright © 2019 brendan burns, joe beda, and kelsey hightower. all rights reserved. printed in the united states of america. published by o’reilly media, inc., 1005 gravenstein highway north, sebastopol, ca 95472. o’reilly books may be purchased for educational, business, or sales promotional use. online editions are also available for most titles (http://oreilly.com). for more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com. acquisition editor: john devins development editor: virginia wilson production editor: kristen brown copyeditor: kim cofer proofreader: rachel head indexer: ellen troutman-zaig interior designer: david futato cover designer: karen montgomery illustrator: rebecca demarest september 2017: first edition august 2019: second edition revision history for the second edition 2019-07-15: first release 2019-10-04: second release see http://oreilly.com/catalog/errata.csp?isbn=9781492046530 for release details. the o’reilly logo is a registered trademark of o’reilly media, inc. kubernetes: up and running, the cover image, and related trade dress are trademarks of o’reilly media, inc. the views expressed in this work are those of the authors, and do not represent the publisher’s views. while the publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. use of the information and instructions contained in this work is at your own risk. if any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights. this work is part of a collaboration between o’reilly and microsoft. see our statement of editorial inde‐ pendence. for robin, julia, ethan, and everyone who bought cookies to pay for that commodore 64 in my third-grade class. —brendan burns for my dad, who helped me fall in love with computers by bringing home punch cards and dot matrix banners. —joe beda for klarissa and kelis, who keep me sane. and for my mom, who taught me a strong work ethic and how to rise above all odds. —kelsey hightower  table of contents preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii 1. introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 velocity 2 the value of immutability 3 declarative configuration 4 self-healing systems 5 scaling your service and your teams 5 decoupling 6 easy scaling for applications and clusters 6 scaling development teams with microservices 7 separation of concerns for consistency and scaling 8 abstracting your infrastructure 9 efficiency 10 summary 11 2. creating and running containers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 container images 14 the docker image format 15 building application images with docker 16 dockerfiles 16 optimizing image sizes 18 image security 19 multistage image builds 20 storing images in a remote registry 22 the docker container runtime 23 running containers with docker 23 exploring the kuard application 23 v limiting resource usage 24 cleanup 24 summary 25 3. deploying a kubernetes cluster. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 installing kubernetes on a public cloud provider 28 google kubernetes engine 28 installing kubernetes with azure kubernetes service 28 installing kubernetes on amazon web services 29 installing kubernetes locally using minikube 29 running kubernetes in docker 30 running kubernetes on raspberry pi 31 the kubernetes client 31 checking cluster status 31 listing kubernetes worker nodes 32 cluster components 34 kubernetes proxy 34 kubernetes dns 34 kubernetes ui 35 summary 36 4. common kubectl commands. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 namespaces 37 contexts 37 viewing kubernetes api objects 38 creating, updating, and destroying kubernetes objects 39 labeling and annotating objects 40 debugging commands 40 command autocompletion 42 alternative ways of viewing your cluster 42 summary 43 5. pods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 pods in kubernetes 46 thinking with pods 46 the pod manifest 47 creating a pod 48 creating a pod manifest 48 running pods 49 listing pods 49 pod details 50 deleting a pod 51 vi | table of contents accessing your pod 52 using port forwarding 52 getting more info with logs 52 running commands in your container with exec 53 copying files to and from containers 53 health checks 54 liveness probe 54 readiness probe 55 types of health checks 56 resource management 56 resource requests: minimum required resources 56 capping resource usage with limits 58 persisting data with volumes 59 using volumes with pods 59 different ways of using volumes with pods 60 persisting data using remote disks 61 putting it all together 61 summary 63 6. labels and annotations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 labels 65 applying labels 67 modifying labels 68 label selectors 68 label selectors in api objects 70 labels in the kubernetes architecture 71 annotations 71 defining annotations 72 cleanup 73 summary 73 7. service discovery. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 what is service discovery? 75 the service object 76 service dns 77 readiness checks 78 looking beyond the cluster 79 cloud integration 81 advanced details 82 endpoints 82 manual service discovery 83 kube-proxy and cluster ips 84 table of contents | vii cluster ip environment variables 85 connecting with other environments 86 cleanup 86 summary 86 8. http load balancing with ingress. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 ingress spec versus ingress controllers 90 installing contour 91 configuring dns 92 configuring a local hosts file 92 using ingress 92 simplest usage 93 using hostnames 94 using paths 95 cleaning up 96 advanced ingress topics and gotchas 96 running multiple ingress controllers 97 multiple ingress objects 97 ingress and namespaces 97 path rewriting 98 serving tls 98 alternate ingress implementations 99 the future of ingress 100 summary 101 9. replicasets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 reconciliation loops 104 relating pods and replicasets 104 adopting existing containers 105 quarantining containers 105 designing with replicasets 105 replicaset spec 106 pod templates 106 labels 107 creating a replicaset 107 inspecting a replicaset 108 finding a replicaset from a pod 108 finding a set of pods for a replicaset 108 scaling replicasets 109 imperative scaling with kubectl scale 109 declaratively scaling with kubectl apply 109 autoscaling a replicaset 110 viii | table of contents deleting replicasets 111 summary 112 10. deployments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 your first deployment 114 deployment internals 114 creating deployments 116 managing deployments 117 updating deployments 118 scaling a deployment 118 updating a container image 119 rollout history 120 deployment strategies 123 recreate strategy 123 rollingupdate strategy 123 slowing rollouts to ensure service health 126 deleting a deployment 128 monitoring a deployment 128 summary 129 11. daemonsets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131 daemonset scheduler 132 creating daemonsets 132 limiting daemonsets to specific nodes 134 adding labels to nodes 135 node selectors 135 updating a daemonset 136 rolling update of a daemonset 136 deleting a daemonset 137 summary 138 12. jobs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 the job object 139 job patterns 140 one shot 140 parallelism 144 work queues 146 cronjobs 150 summary 151 13. con\\x80gmaps and secrets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 configmaps 153 table of contents | ix creating configmaps 153 using a configmap 154 secrets 157 creating secrets 158 consuming secrets 159 private docker registries 160 naming constraints 161 managing configmaps and secrets 162 listing 162 creating 163 updating 163 summary 165 14. role-based access control for kubernetes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167 role-based access control 168 identity in kubernetes 168 understanding roles and role bindings 169 roles and role bindings in kubernetes 169 techniques for managing rbac 172 testing authorization with can-i 172 managing rbac in source control 172 advanced topics 172 aggregating clusterroles 173 using groups for bindings 173 summary 175 15. integrating storage solutions and kubernetes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177 importing external services 178 services without selectors 179 limitations of external services: health checking 181 running reliable singletons 181 running a mysql singleton 181 dynamic volume provisioning 185 kubernetes-native storage with statefulsets 186 properties of statefulsets 187 manually replicated mongodb with statefulsets 187 automating mongodb cluster creation 189 persistent volumes and statefulsets 192 one final thing: readiness probes 193 summary 194 x | table of contents 16. extending kubernetes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195 what it means to extend kubernetes 195 points of extensibility 196 patterns for custom resources 204 just data 204 compilers 205 operators 205 getting started 205 summary 205 17. deploying real-world applications. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 jupyter 207 parse 209 prerequisites 209 building the parse-server 209 deploying the parse-server 209 testing parse 210 ghost 211 configuring ghost 211 redis 214 configuring redis 215 creating a redis service 216 deploying redis 217 playing with our redis cluster 218 summary 219 18. organizing your application. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221 principles to guide us 221 filesystems as the source of truth 222 the role of code review 222 feature gates and guards 223 managing your application in source control 224 filesystem layout 224 managing periodic versions 225 structuring your application for development, testing, and deployment 227 goals 227 progression of a release 227 parameterizing your application with templates 229 parameterizing with helm and templates 229 filesystem layout for parameterization 230 deploying your application around the world 230 architectures for worldwide deployment 230 table of contents | xi implementing worldwide deployment 232 dashboards and monitoring for worldwide deployments 233 summary 233 a. building a raspberry pi kubernetes cluster. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235 index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243 xii | table of contents preface kubernetes: a dedication kubernetes would like to thank every sysadmin who has woken up at 3 a.m. to restart a process. every developer who pushed code to production only to find that it didn’t run like it did on their laptop. every systems architect who mistakenly pointed a load test at the production service because of a leftover hostname that they hadn’t updated. it was the pain, the weird hours, and the weird errors that inspired the development of kubernetes. in a single sentence: kubernetes intends to radically simplify the task of building, deploying, and maintaining distributed systems. it has been inspired by decades of real-world experience building reliable systems and it has been designed from the ground up to make that experience if not euphoric, at least pleasant. we hope you enjoy the book! who should read this book whether you are new to distributed systems or have been deploying cloud-native sys‐ tems for years, containers and kubernetes can help you achieve new levels of velocity, agility, reliability, and efficiency. this book describes the kubernetes cluster orches‐ trator and how its tools and apis can be used to improve the development, delivery, and maintenance of distributed applications. though no previous experience with kubernetes is assumed, to make maximal use of the book you should be comfortable building and deploying server-based applications. familiarity with concepts like load balancers and network storage will be useful, though not required. likewise, experi‐ ence with linux, linux containers, and docker, though not essential, will help you make the most of this book. xiii why we wrote this book we have been involved with kubernetes since its very beginnings. it has been truly remarkable to watch it transform from a curiosity largely used in experiments to a crucial production-grade infrastructure that powers large-scale production applica‐ tions in varied fields, from machine learning to online services. as this transition occurred, it became increasingly clear that a book that captured both how to use the core concepts in kubernetes and the motivations behind the development of those concepts would be an important contribution to the state of cloud-native application development. we hope that in reading this book, you not only learn how to build reli‐ able, scalable applications on top of kubernetes but also receive insight into the core challenges of distributed systems that led to its development. why we updated this book in the few years that have passed since we wrote the first edition of this book, the kubernetes ecosystem has blossomed and evolved. kubernetes itself has had many releases, and many more tools and patterns for using kubernetes have become de facto standards. in updating the book we added material on http load balancing, role-based access control (rbac), extending the kubernetes api, how to organize your application in source control, and more. we also updated all of the existing chapters to reflect the changes and evolution in kubernetes since the first edition. we fully expect to revise this book again in a few years (and look forward to doing so) as kubernetes continues to evolve. a word on cloud-native applications today from the first programming languages, to object-oriented programming, to the development of virtualization and cloud infrastructure, the history of computer sci‐ ence is a history of the development of abstractions that hide complexity and empower you to build ever more sophisticated applications. despite this, the develop‐ ment of reliable, scalable applications is still dramatically more challenging than it ought to be. in recent years, containers and container orchestration apis like kuber‐ netes have proven to be an important abstraction that radically simplifies the devel‐ opment of reliable, scalable distributed systems. though containers and orchestrators are still in the process of entering the mainstream, they are already enabling develop‐ ers to build and deploy applications with a speed, agility, and reliability that would have seemed like science fiction only a few years ago. xiv | preface navigating this book this book is organized as follows. chapter 1 outlines the high-level benefits of kuber‐ netes without diving too deeply into the details. if you are new to kubernetes, this is a great place to start to understand why you should read the rest of the book. chapter 2 provides a detailed introduction to containers and containerized applica‐ tion development. if you’ve never really played around with docker before, this chap‐ ter will be a useful introduction. if you are already a docker expert, it will likely be mostly review. chapter 3 covers how to deploy kubernetes. while most of this book focuses on how to use kubernetes, you need to get a cluster up and running before you start using it. although running a cluster for production is out of the scope of this book, this chap‐ ter presents a couple of easy ways to create a cluster so that you can understand how to use kubernetes. chapter 4 covers a selection of common commands used to inter‐ act with a kubernetes cluster. starting with chapter 5, we dive into the details of deploying an application using kubernetes. we cover pods (chapter 5), labels and annotations (chapter 6), services (chapter 7), ingress (chapter 8), and replicasets (chapter 9). these form the core basics of what you need to deploy your service in kubernetes. we then cover deploy‐ ments (chapter 10), which tie together the lifecycle of a complete application. after those chapters, we cover some more specialized objects in kubernetes: dae‐ monsets (chapter 11), jobs (chapter 12), and configmaps and secrets (chapter 13). while these chapters are essential for many production applications, if you are just learning kubernetes you can skip them and return to them later, after you gain more experience and expertise. next we cover integrating storage into kubernetes (chapter 15). we discuss extend‐ ing kubernetes in chapter 16. finally, we conclude with some examples of how to develop and deploy real-world applications in kubernetes (chapter 17) and a discus‐ sion of how to organize your applications in source control (chapter 18). online resources you will want to install docker. you likely will also want to familiarize yourself with the docker documentation if you have not already done so. likewise, you will want to install the kubectl command-line tool. you may also want to join the kubernetes slack channel, where you will find a large community of users who are willing to talk and answer questions at nearly any hour of the day. finally, as you grow more advanced, you may want to engage with the open source kubernetes repository on github. preface | xv conventions used in this book the following typographical conventions are used in this book: italic indicates new terms, urls, email addresses, filenames, and file extensions. constant width used for program listings, as well as within paragraphs to refer to program ele‐ ments such as variable or function names, databases, data types, environment variables, statements, and keywords. constant width bold shows commands or other text that should be typed literally by the user. constant width italic shows text that should be replaced with user-supplied values or by values deter‐ mined by context. this icon signifies a tip, suggestion, or general note. this icon indicates a warning or caution. using code examples supplemental material (code examples, exercises, etc.) is available for download at https://github.com/kubernetes-up-and-running/examples. this book is here to help you get your job done. in general, if example code is offered with this book, you may use it in your programs and documentation. you do not need to contact us for permission unless you’re reproducing a significant portion of the code. for example, writing a program that uses several chunks of code from this book does not require permission. selling or distributing a cd-rom of examples from o’reilly books does require permission. answering a question by citing this book and quoting example code does not require permission. incorporating a signifi‐ cant amount of example code from this book into your product’s documentation does require permission. xvi | preface we appreciate, but do not require, attribution. an attribution usually includes the title, author, publisher, and isbn. for example: “kubernetes: up and running, 2nd edition, by brendan burns, joe beda, and kelsey hightower (o’reilly). copyright 2019 brendan burns, joe beda, and kelsey hightower, 978-1-492-04653-0.” if you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com. o’reilly online learning for almost 40 years, o’reilly media has provided technology and business training, knowledge, and insight to help compa‐ nies succeed. our unique network of experts and innovators share their knowledge and expertise through books, articles, conferences, and our online learning platform. o’reilly’s online learning platform gives you on-demand access to live training courses, indepth learning paths, interactive coding environments, and a vast collection of text and video from o’reilly and 200+ other publishers. for more information, please visit http://oreilly.com. how to contact us please address comments and questions concerning this book to the publisher: o’reilly media, inc. 1005 gravenstein highway north sebastopol, ca 95472 800-998-9938 (in the united states or canada) 707-829-0515 (international or local) 707-829-0104 (fax) we have a web page for this book, where we list errata, examples, and any additional information. you can access this page at http://bit.ly/kubernetesur2e. to comment or ask technical questions about this book, send email to bookques‐ tions@oreilly.com. for more information about our books, courses, conferences, and news, see our web‐ site at http://www.oreilly.com. find us on facebook: http://facebook.com/oreilly follow us on twitter: http://twitter.com/oreillymedia preface | xvii watch us on youtube: http://www.youtube.com/oreillymedia acknowledgments we would like to acknowledge everyone who helped us develop this book. this includes our editor virginia wilson and all of the great folks at o’reilly, as well as the technical reviewers who provided tremendous feedback that significantly improved the book. finally, we would like to thank all of our first edition readers who took the time to report errata that were found and fixed in this second edition. thank you all! we’re very grateful. xviii | preface 1 brendan burns et al., “borg, omega, and kubernetes: lessons learned from three container-management systems over a decade,” acm queue 14 (2016): 70–93, available at http://bit.ly/2virl4s. chapter 1 introduction kubernetes is an open source orchestrator for deploying containerized applications. it was originally developed by google, inspired by a decade of experience deploying scalable, reliable systems in containers via application-oriented apis.1 since its introduction in 2014, kubernetes has grown to be one of the largest and most popular open source projects in the world. it has become the standard api for building cloud-native applications, present in nearly every public cloud. kubernetes is a proven infrastructure for distributed systems that is suitable for cloud-native developers of all scales, from a cluster of raspberry pi computers to a warehouse full of the latest machines. it provides the software necessary to successfully build and deploy reliable, scalable distributed systems. you may be wondering what we mean when we say “reliable, scalable distributed sys‐ tems.” more and more services are delivered over the network via apis. these apis are often delivered by a distributed system, the various pieces that implement the api running on different machines, connected via the network and coordinating their actions via network communication. because we rely on these apis increasingly for all aspects of our daily lives (e.g., finding directions to the nearest hospital), these sys‐ tems must be highly reliable. they cannot fail, even if a part of the system crashes or otherwise stops working. likewise, they must maintain availability even during soft‐ ware rollouts or other maintenance events. finally, because more and more of the world is coming online and using such services, they must be highly scalable so that they can grow their capacity to keep up with ever-increasing usage without radical redesign of the distributed system that implements the services. 1 depending on when and why you have come to hold this book in your hands, you may have varying degrees of experience with containers, distributed systems, and kubernetes. you may be planning on building your application on top of public cloud infrastructure, in private data centers, or in some hybrid environment. regardless of what your experience is, we believe this book will enable you to make the most of your use of kubernetes. there are many reasons why people come to use containers and container apis like kubernetes, but we believe they can all be traced back to one of these benefits: • velocity • scaling (of both software and teams) • abstracting your infrastructure • efficiency in the following sections, we describe how kubernetes can help provide each of these features. velocity velocity is the key component in nearly all software development today. the software industry has evolved from shipping products as boxed cds or dvds to software that is delivered over the network via web-based services that are updated hourly. this changing landscape means that the difference between you and your competitors is often the speed with which you can develop and deploy new components and fea‐ tures, or the speed with which you can respond to innovations developed by others. it is important to note, however, that velocity is not defined in terms of simply raw speed. while your users are always looking for iterative improvement, they are more interested in a highly reliable service. once upon a time, it was ok for a service to be down for maintenance at midnight every night. but today, all users expect constant uptime, even if the software they are running is changing constantly. consequently, velocity is measured not in terms of the raw number of features you can ship per hour or day, but rather in terms of the number of things you can ship while maintaining a highly available service. in this way, containers and kubernetes can provide the tools that you need to move quickly, while staying available. the core concepts that enable this are: • immutability • declarative configuration • online self-healing systems 2 | chapter 1: introduction these ideas all interrelate to radically improve the speed with which you can reliably deploy software. the value of immutability containers and kubernetes encourage developers to build distributed systems that adhere to the principles of immutable infrastructure. with immutable infrastructure, once an artifact is created in the system it does not change via user modifications. traditionally, computers and software systems have been treated as mutable infra‐ structure. with mutable infrastructure, changes are applied as incremental updates to an existing system. these updates can occur all at once, or spread out across a long period of time. a system upgrade via the apt-get update tool is a good example of an update to a mutable system. running apt sequentially downloads any updated binaries, copies them on top of older binaries, and makes incremental updates to configuration files. with a mutable system, the current state of the infrastructure is not represented as a single artifact, but rather an accumulation of incremental updates and changes over time. on many systems these incremental updates come from not just system upgrades, but operator modifications as well. furthermore, in any system run by a large team, it is highly likely that these changes will have been performed by many different people, and in many cases will not have been recorded anywhere. in contrast, in an immutable system, rather than a series of incremental updates and changes, an entirely new, complete image is built, where the update simply replaces the entire image with the newer image in a single operation. there are no incremental changes. as you can imagine, this is a significant shift from the more traditional world of configuration management. to make this more concrete in the world of containers, consider two different ways to upgrade your software: 1. you can log in to a container, run a command to download your new software, kill the old server, and start the new one. 2. you can build a new container image, push it to a container registry, kill the exist‐ ing container, and start a new one. at first blush, these two approaches might seem largely indistinguishable. so what is it about the act of building a new container that improves reliability? the key differentiation is the artifact that you create, and the record of how you cre‐ ated it. these records make it easy to understand exactly the differences in some new version and, if something goes wrong, to determine what has changed and how to fix it. velocity | 3 additionally, building a new image rather than modifying an existing one means the old image is still around, and can quickly be used for a rollback if an error occurs. in contrast, once you copy your new binary over an existing binary, such a rollback is nearly impossible. immutable container images are at the core of everything that you will build in kubernetes. it is possible to imperatively change running containers, but this is an anti-pattern to be used only in extreme cases where there are no other options (e.g., if it is the only way to temporarily repair a mission-critical production system). and even then, the changes must also be recorded through a declarative configuration update at some later time, after the fire is out. declarative con\\x80guration immutability extends beyond containers running in your cluster to the way you describe your application to kubernetes. everything in kubernetes is a declarative configuration object that represents the desired state of the system. it is the job of kubernetes to ensure that the actual state of the world matches this desired state. much like mutable versus immutable infrastructure, declarative configuration is an alternative to imperative configuration, where the state of the world is defined by the execution of a series of instructions rather than a declaration of the desired state of the world. while imperative commands define actions, declarative configurations define state. to understand these two approaches, consider the task of producing three replicas of a piece of software. with an imperative approach, the configuration would say “run a, run b, and run c.” the corresponding declarative configuration would be “replicas equals three.” because it describes the state of the world, declarative configuration does not have to be executed to be understood. its impact is concretely declared. since the effects of declarative configuration can be understood before they are executed, declarative configuration is far less error-prone. further, the traditional tools of software devel‐ opment, such as source control, code review, and unit testing, can be used in declara‐ tive configuration in ways that are impossible for imperative instructions. the idea of storing declarative configuration in source control is often referred to as “infrastruc‐ ture as code.” the combination of declarative state stored in a version control system and the ability of kubernetes to make reality match this declarative state makes rollback of a change trivially easy. it is simply restating the previous declarative state of the system. this is usually impossible with imperative systems, because although the imperative instruc‐ tions describe how to get you from point a to point b, they rarely include the reverse instructions that can get you back. 4 | chapter 1: introduction self-healing systems kubernetes is an online, self-healing system. when it receives a desired state configu‐ ration, it does not simply take a set of actions to make the current state match the desired state a single time. it continuously takes actions to ensure that the current state matches the desired state. this means that not only will kubernetes initialize your system, but it will guard it against any failures or perturbations that might destabilize the system and affect reliability. a more traditional operator repair involves a manual series of mitigation steps, or human intervention performed in response to some sort of alert. imperative repair like this is more expensive (since it generally requires an on-call operator to be avail‐ able to enact the repair). it is also generally slower, since a human must often wake up and log in to respond. furthermore, it is less reliable because the imperative series of repair operations suffers from all of the problems of imperative management described in the previous section. self-healing systems like kubernetes both reduce the burden on operators and improve the overall reliability of the system by perform‐ ing reliable repairs more quickly. as a concrete example of this self-healing behavior, if you assert a desired state of three replicas to kubernetes, it does not just create three replicas—it continuously ensures that there are exactly three replicas. if you manually create a fourth replica, kubernetes will destroy one to bring the number back to three. if you manually destroy a replica, kubernetes will create one to again return you to the desired state. online self-healing systems improve developer velocity because the time and energy you might otherwise have spent on operations and maintenance can instead be spent on developing and testing new features. in a more advanced form of self-healing, there has been significant recent work in the operator paradigm for kubernetes. with operators, more advanced logic needed to maintain, scale, and heal a specific piece of software (mysql, for example) is enco‐ ded into an operator application that runs as a container in the cluster. the code in the operator is responsible for more targeted and advanced health detection and heal‐ ing than can be achieved via kubernetes’s generic self-healing. often this is packaged up as “operators,” which are discussed in a later section. scaling your service and your teams as your product grows, it’s inevitable that you will need to scale both your software and the teams that develop it. fortunately, kubernetes can help with both of these goals. kubernetes achieves scalability by favoring decoupled architectures. scaling your service and your teams | 5 decoupling in a decoupled architecture, each component is separated from other components by defined apis and service load balancers. apis and load balancers isolate each piece of the system from the others. apis provide a buffer between implementer and con‐ sumer, and load balancers provide a buffer between running instances of each service. decoupling components via load balancers makes it easy to scale the programs that make up your service, because increasing the size (and therefore the capacity) of the program can be done without adjusting or reconfiguring any of the other layers of your service. decoupling servers via apis makes it easier to scale the development teams because each team can focus on a single, smaller microservice with a comprehensible surface area. crisp apis between microservices limit the amount of cross-team communica‐ tion overhead required to build and deploy software. this communication overhead is often the major restricting factor when scaling teams. easy scaling for applications and clusters concretely, when you need to scale your service, the immutable, declarative nature of kubernetes makes this scaling trivial to implement. because your containers are immutable, and the number of replicas is merely a number in a declarative config, scaling your service upward is simply a matter of changing a number in a configura‐ tion file, asserting this new declarative state to kubernetes, and letting it take care of the rest. alternatively, you can set up autoscaling and let kubernetes take care of it for you. of course, that sort of scaling assumes that there are resources available in your clus‐ ter to consume. sometimes you actually need to scale up the cluster itself. again, kubernetes makes this task easier. because many machines in a cluster are entirely identical to other machines in that set and the applications themselves are decoupled from the details of the machine by containers, adding additional resources to the cluster is simply a matter of imaging a new machine of the same class and joining it into the cluster. this can be accomplished via a few simple commands or via a pre‐ baked machine image. one of the challenges of scaling machine resources is predicting their use. if you are running on physical infrastructure, the time to obtain a new machine is measured in days or weeks. on both physical and cloud infrastructure, predicting future costs is difficult because it is hard to predict the growth and scaling needs of specific applications. kubernetes can simplify forecasting future compute costs. to understand why this is true, consider scaling up three teams, a, b, and c. historically you have seen that 6 | chapter 1: introduction each team’s growth is highly variable and thus hard to predict. if you are provisioning individual machines for each service, you have no choice but to forecast based on the maximum expected growth for each service, since machines dedicated to one team cannot be used for another team. if instead you use kubernetes to decouple the teams from the specific machines they are using, you can forecast growth based on the aggregate growth of all three services. combining three variable growth rates into a single growth rate reduces statistical noise and produces a more reliable forecast of expected growth. furthermore, decoupling the teams from specific machines means that teams can share fractional parts of one another’s machines, reducing even further the overheads associated with forecasting growth of computing resources. scaling development teams with microservices as noted in a variety of research, the ideal team size is the “two-pizza team,” or roughly six to eight people. this group size often results in good knowledge sharing, fast decision making, and a common sense of purpose. larger teams tend to suffer from issues of hierarchy, poor visibility, and infighting, which hinder agility and success. however, many projects require significantly more resources to be successful and achieve their goals. consequently, there is a tension between the ideal team size for agility and the necessary team size for the product’s end goals. the common solution to this tension has been the development of decoupled, service-oriented teams that each build a single microservice. each small team is responsible for the design and delivery of a service that is consumed by other small teams. the aggregation of all of these services ultimately provides the implementation of the overall product’s surface area. kubernetes provides numerous abstractions and apis that make it easier to build these decoupled microservice architectures: • pods, or groups of containers, can group together container images developed by different teams into a single deployable unit. • kubernetes services provide load balancing, naming, and discovery to isolate one microservice from another. • namespaces provide isolation and access control, so that each microservice can control the degree to which other services interact with it. • ingress objects provide an easy-to-use frontend that can combine multiple micro‐ services into a single externalized api surface area. finally, decoupling the application container image and machine means that different microservices can colocate on the same machine without interfering with one another, reducing the overhead and cost of microservice architectures. the healthscaling your service and your teams | 7 checking and rollout features of kubernetes guarantee a consistent approach to appli‐ cation rollout and reliability that ensures that a proliferation of microservice teams does not also result in a proliferation of different approaches to service production lifecycle and operations. separation of concerns for consistency and scaling in addition to the consistency that kubernetes brings to operations, the decoupling and separation of concerns produced by the kubernetes stack lead to significantly greater consistency for the lower levels of your infrastructure. this enables you to scale infrastructure operations to manage many machines with a single small, focused team. we have talked at length about the decoupling of application container and machine/operating system (os), but an important aspect of this decoupling is that the container orchestration api becomes a crisp contract that separates the responsi‐ bilities of the application operator from the cluster orchestration operator. we call this the “not my monkey, not my circus” line. the application developer relies on the service-level agreement (sla) delivered by the container orchestration api, without worrying about the details of how this sla is achieved. likewise, the container orchestration api reliability engineer focuses on delivering the orchestration api’s sla without worrying about the applications that are running on top of it. this decoupling of concerns means that a small team running a kubernetes cluster can be responsible for supporting hundreds or even thousands of teams running applications within that cluster (figure 1-1). likewise, a small team can be responsi‐ ble for dozens (or more) of clusters running around the world. it’s important to note that the same decoupling of containers and os enables the os reliability engineers to focus on the sla of the individual machine’s os. this becomes another line of sepa‐ rate responsibility, with the kubernetes operators relying on the os sla, and the os operators worrying solely about delivering that sla. again, this enables you to scale a small team of os experts to a fleet of thousands of machines. of course, devoting even a small team to managing an os is beyond the scale of many organizations. in these environments, a managed kubernetes-as-a-service (kaas) provided by a public cloud provider is a great option. as kubernetes has become increasingly ubiquitous, kaas has become increasingly available as well, to the point where it is now offered on nearly every public cloud. of course, using a kaas has some limitations, since the operator makes decisions for you about how the kubernetes clusters are built and configured. for example, many kaas platforms dis‐ able alpha features because they can destabilize the managed cluster. 8 | chapter 1: introduction figure 1-1. an illustration of how different operations teams are decoupled using apis in addition to a fully managed kubernetes service, there is a thriving ecosystem of companies and projects that help to install and manage kubernetes. there is a full spectrum of solutions between doing it “the hard way” and a fully managed service. consequently, the decision of whether to use kaas or manage it yourself (or some‐ thing in between) is one each user needs to make based on the skills and demands of their situation. often for small organizations, kaas provides an easy-to-use solution that enables them to focus their time and energy on building the software to support their work rather than managing a cluster. for a larger organization that can afford a dedicated team for managing its kubernetes cluster, it may make sense to manage it yourself since it enables greater flexibility in terms of cluster capabilities and operations. abstracting your infrastructure the goal of the public cloud is to provide easy-to-use, self-service infrastructure for developers to consume. however, too often cloud apis are oriented around mirror‐ ing the infrastructure that it expects, not the concepts (e.g., “virtual machines” instead of “applications”) that developers want to consume. additionally, in many cases the cloud comes with particular details in implementation or services that are specific to the cloud provider. consuming these apis directly makes it difficult to run your application in multiple environments, or spread between cloud and physical environments. the move to application-oriented container apis like kubernetes has two concrete benefits. first, as we described previously, it separates developers from specific machines. this makes the machine-oriented it role easier, since machines can simply abstracting your infrastructure | 9 be added in aggregate to scale the cluster, and in the context of the cloud it also ena‐ bles a high degree of portability since developers are consuming a higher-level api that is implemented in terms of the specific cloud infrastructure apis. when your developers build their applications in terms of container images and deploy them in terms of portable kubernetes apis, transferring your application between environments, or even running in hybrid environments, is simply a matter of sending the declarative config to a new cluster. kubernetes has a number of plug-ins that can abstract you from a particular cloud. for example, kubernetes services know how to create load balancers on all major public clouds as well as several different pri‐ vate and physical infrastructures. likewise, kubernetes persistentvolumes and persistentvolumeclaims can be used to abstract your applications away from spe‐ cific storage implementations. of course, to achieve this portability you need to avoid cloud-managed services (e.g., amazon’s dynamodb, azure’s cosmosdb, or google’s cloud spanner), which means that you will be forced to deploy and manage open source storage solutions like cassandra, mysql, or mongodb. putting it all together, building on top of kubernetes’s application-oriented abstrac‐ tions ensures that the effort you put into building, deploying, and managing your application is truly portable across a wide variety of environments. efficiency in addition to the developer and it management benefits that containers and kuber‐ netes provide, there is also a concrete economic benefit to the abstraction. because developers no longer think in terms of machines, their applications can be colocated on the same machines without impacting the applications themselves. this means that tasks from multiple users can be packed tightly onto fewer machines. efficiency can be measured by the ratio of the useful work performed by a machine or process to the total amount of energy spent doing so. when it comes to deploying and managing applications, many of the available tools and processes (e.g., bash scripts, apt updates, or imperative configuration management) are somewhat ineffi‐ cient. when discussing efficiency it’s often helpful to think of both the cost of run‐ ning a server and the human cost required to manage it. running a server incurs a cost based on power usage, cooling requirements, datacenter space, and raw compute power. once a server is racked and powered on (or clicked and spun up), the meter literally starts running. any idle cpu time is money wasted. thus, it becomes part of the system administrator’s responsibilities to keep utilization at acceptable levels, which requires ongoing management. this is where containers and the kubernetes workflow come in. kubernetes provides tools that automate the distribution of applications across a cluster of machines, ensuring higher levels of utilization than are possible with traditional tooling. 10 | chapter 1: introduction a further increase in efficiency comes from the fact that a developer’s test environ‐ ment can be quickly and cheaply created as a set of containers running in a personal view of a shared kubernetes cluster (using a feature called namespaces). in the past, turning up a test cluster for a developer might have meant turning up three machines. with kubernetes it is simple to have all developers share a single test cluster, aggre‐ gating their usage onto a much smaller set of machines. reducing the overall number of machines used in turn drives up the efficiency of each system: since more of the resources (cpu, ram, etc.) on each individual machine are used, the overall cost of each container becomes much lower. reducing the cost of development instances in your stack enables development prac‐ tices that might previously have been cost-prohibitive. for example, with your appli‐ cation deployed via kubernetes it becomes conceivable to deploy and test every single commit contributed by every developer throughout your entire stack. when the cost of each deployment is measured in terms of a small number of con‐ tainers, rather than multiple complete virtual machines (vms), the cost you incur for such testing is dramatically lower. returning to the original value of kubernetes, this increased testing also increases velocity, since you have strong signals as to the relia‐ bility of your code as well as the granularity of detail required to quickly identify where a problem may have been introduced. summary kubernetes was built to radically change the way that applications are built and deployed in the cloud. fundamentally, it was designed to give developers more veloc‐ ity, efficiency, and agility. we hope the preceding sections have given you an idea of why you should deploy your applications using kubernetes. now that you are con‐ vinced of that, the following chapters will teach you how to deploy your application. summary | 11  chapter 2 creating and running containers kubernetes is a platform for creating, deploying, and managing distributed applica‐ tions. these applications come in many different shapes and sizes, but ultimately, they are all comprised of one or more programs that run on individual machines. these programs accept input, manipulate data, and then return the results. before we can even consider building a distributed system, we must first consider how to build the application container images that contain these programs and make up the pieces of our distributed system. application programs are typically comprised of a language runtime, libraries, and your source code. in many cases, your application relies on external shared libraries such as libc and libssl. these external libraries are generally shipped as shared components in the os that you have installed on a particular machine. this dependency on shared libraries causes problems when an application developed on a programmer’s laptop has a dependency on a shared library that isn’t available when the program is rolled out to the production os. even when the development and production environments share the exact same version of the os, problems can occur when developers forget to include dependent asset files inside a package that they deploy to production. the traditional methods of running multiple programs on a single machine require that all of these programs share the same versions of shared libraries on the system. if the different programs are developed by different teams or organizations, these shared dependencies add needless complexity and coupling between these teams. a program can only execute successfully if it can be reliably deployed onto the machine where it should run. too often the state of the art for deployment involves running imperative scripts, which inevitably have twisty and byzantine failure cases. 13 this makes the task of rolling out a new version of all or parts of a distributed system a labor-intensive and difficult task. in chapter 1, we argued strongly for the value of immutable images and infrastruc‐ ture. this immutability is exactly what the container image provides. as we will see, it easily solves all the problems of dependency management and encapsulation just described. when working with applications it’s often helpful to package them in a way that makes it easy to share them with others. docker, the default container runtime engine, makes it easy to package an executable and push it to a remote registry where it can later be pulled by others. at the time of writing, container registries are avail‐ able in all of the major public clouds, and services to build images in the cloud are also available in many of them. you can also run your own registry using open source or commercial systems. these registries make it easy for users to manage and deploy private images, while image-builder services provide easy integration with continuous delivery systems. for this chapter, and the remainder of the book, we are going to work with a simple example application that we built to help show this workflow in action. you can find the application on github. container images bundle a program and its dependencies into a single artifact under a root filesystem. the most popular container image format is the docker image for‐ mat, which has been standardized by the open container initiative to the oci image format. kubernetes supports both docker- and oci-compatible images via docker and other runtimes. docker images also include additional metadata used by a con‐ tainer runtime to start a running application instance based on the contents of the container image. this chapter covers the following topics: • how to package an application using the docker image format • how to start an application using the docker container runtime container images for nearly everyone, their first interaction with any container technology is with a container image. a container image is a binary package that encapsulates all of the files necessary to run a program inside of an os container. depending on how you first experiment with containers, you will either build a container image from your local filesystem or download a preexisting image from a container registry. in either case, once the container image is present on your computer, you can run that image to produce a running application inside an os container. 14 | chapter 2: creating and running containers the docker image format the most popular and widespread container image format is the docker image for‐ mat, which was developed by the docker open source project for packaging, distrib‐ uting, and running containers using the docker command. subsequently, work has begun by docker, inc., and others to standardize the container image format via the open container initiative (oci) project. while the oci standard achieved a 1.0 release milestone in mid-2017, adoption of these standards is proceeding slowly. the docker image format continues to be the de facto standard, and is made up of a series of filesystem layers. each layer adds, removes, or modifies files from the preceding layer in the filesystem. this is an example of an overlay filesystem. the overlay system is used both when packaging up the image and when the image is actually being used. during runtime, there are a variety of different concrete implementations of such file‐ systems, including aufs, overlay, and overlay2. container layering the phrases “docker image format” and “container images” may be a bit confusing. the image isn’t a single file but rather a specification for a manifest file that points to other files. the manifest and associated files are often treated by users as a unit. the level of indirection allows for more efficient storage and transmittal. associated with this format is an api for uploading and downloading images to an image registry. container images are constructed with a series of filesystem layers, where each layer inherits and modifies the layers that came before it. to help explain this in detail, let’s build some containers. note that for correctness the ordering of the layers should be bottom up, but for ease of understanding we take the opposite approach: . └── container a: a base operating system only, such as debian  └── container b: build upon #a, by adding ruby v2.1.10  └── container c: build upon #a, by adding golang v1.6 at this point we have three containers: a, b, and c. b and c are forked from a and share nothing besides the base container’s files. taking it further, we can build on top of b by adding rails (version 4.2.6). we may also want to support a legacy application that requires an older version of rails (e.g., version 3.2.x). we can build a container image to support that application based on b also, planning to someday migrate the app to version 4: . (continuing from above) └── container b: build upon #a, by adding ruby v2.1.10  └── container d: build upon #b, by adding rails v4.2.6  └── container e: build upon #b, by adding rails v3.2.x container images | 15 conceptually, each container image layer builds upon a previous one. each parent reference is a pointer. while the example here is a simple set of containers, other realworld containers can be part of a larger extensive directed acyclic graph. container images are typically combined with a container configuration file, which provides instructions on how to set up the container environment and execute an application entry point. the container configuration often includes information on how to set up networking, namespace isolation, resource constraints (cgroups), and what syscall restrictions should be placed on a running container instance. the container root filesystem and configuration file are typically bundled using the docker image format. containers fall into two main categories: • system containers • application containers system containers seek to mimic virtual machines and often run a full boot process. they often include a set of system services typically found in a vm, such as ssh, cron, and syslog. when docker was new, these types of containers were much more com‐ mon. over time, they have come to be seen as poor practice and application contain‐ ers have gained favor. application containers differ from system containers in that they commonly run a single program. while running a single program per container might seem like an unnecessary constraint, it provides the perfect level of granularity for composing scal‐ able applications and is a design philosophy that is leveraged heavily by pods. we will examine how pods work in detail in chapter 5. building application images with docker in general, container orchestration systems like kubernetes are focused on building and deploying distributed systems made up of application containers. consequently, we will focus on application containers for the remainder of this chapter. docker\\x80les a dockerfile can be used to automate the creation of a docker container image. let’s start by building an application image for a simple node.js program. this exam‐ ple would be very similar for many other dynamic languages, like python or ruby. 16 | chapter 2: creating and running containers the simplest of npm/node/express apps has two files: package.json (example 2-1) and server.js (example 2-2). put these in a directory and then run npm install express --save to establish a dependency on express and install it. example 2-1. package.json {  \"name\": \"simple-node\",  \"version\": \"1.0.0\",  \"description\": \"a sample simple application for kubernetes up & running\",  \"main\": \"server.js\",  \"scripts\": {  \"start\": \"node server.js\"  },  \"author\": \"\" } example 2-2. server.js var express = require(\\'express\\'); var app = express(); app.get(\\'/\\', function (req, res) {  res.send(\\'hello world!\\'); }); app.listen(3000, function () {  console.log(\\'listening on port 3000!\\');  console.log(\\' http://localhost:3000\\'); }); to package this up as a docker image we need to create two additional files: .docker‐ ignore (example 2-3) and the dockerfile (example 2-4). the dockerfile is a recipe for how to build the container image, while .dockerignore defines the set of files that should be ignored when copying files into the image. a full description of the syntax of the dockerfile is available on the docker website. example 2-3. .dockerignore nodemodules example 2-4. dockerfile # start from a node.js 10 (lts) image from node:10 # specify the directory inside the image in which all commands will run workdir /usr/src/app building application images with docker | 17 # copy package files and install dependencies copy package*.json ./ run npm install # copy all of the app files into the image copy . . # the default command to run when starting the container cmd  every dockerfile builds on other container images. this line specifies that we are starting from the node:10 image on the docker hub. this is a preconfigured image with node.js 10. this line sets the work directory, in the container image, for all following commands. these two lines initialize the dependencies for node.js. first we copy the package files into the image. this will include package.json and package-lock.json. the run command then runs the correct command in the container to install the neces‐ sary dependencies. now we copy the rest of the program files into the image. this will include every‐ thing except nodemodules, as that is excluded via the .dockerignore file. finally, we specify the command that should be run when the container is run. run the following command to create the simple-node docker image: $ docker build -t simple-node . when you want to run this image, you can do it with the following command. you can navigate to http://localhost:3000 to access the program running in the container: $ docker run --rm -p 3000:3000 simple-node at this point our simple-node image lives in the local docker registry where the image was built and is only accessible to a single machine. the true power of docker comes from the ability to share images across thousands of machines and the broader docker community. optimizing image sizes there are several gotchas that come when people begin to experiment with container images that lead to overly large images. the first thing to remember is that files that are removed by subsequent layers in the system are actually still present in the images; they’re just inaccessible. consider the following situation: 18 | chapter 2: creating and running containers . └── layer a: contains a large file named \\'bigfile\\'  └── layer b: removes \\'bigfile\\'  └── layer c: builds on b by adding a static binary you might think that bigfile is no longer present in this image. after all, when you run the image, it is no longer accessible. but in fact it is still present in layer a, which means that whenever you push or pull the image, bigfile is still transmitted through the network, even if you can no longer access it. another pitfall that people fall into revolves around image caching and building. remember that each layer is an independent delta from the layer below it. every time you change a layer, it changes every layer that comes after it. changing the preceding layers means that they need to be rebuilt, repushed, and repulled to deploy your image to development. to understand this more fully, consider two images: . └── layer a: contains a base os  └── layer b: adds source code server.js  └── layer c: installs the \\'node\\' package versus: . └── layer a: contains a base os  └── layer b: installs the \\'node\\' package  └── layer c: adds source code server.js it seems obvious that both of these images will behave identically, and indeed the first time they are pulled they do. however, consider what happens when server.js changes. in one case, it is only the change that needs to be pulled or pushed, but in the other case, both server.js and the layer providing the node package need to be pulled and pushed, since the node layer is dependent on the server.js layer. in general, you want to order your layers from least likely to change to most likely to change in order to optimize the image size for pushing and pulling. this is why, in example 2-4, we copy the package*.json files and install dependencies before copying the rest of the pro‐ gram files. a developer is going to update and change the program files much more often than the dependencies. image security when it comes to security, there are no shortcuts. when building images that will ultimately run in a production kubernetes cluster, be sure to follow best practices for packaging and distributing applications. for example, don’t build containers with passwords baked in—and this includes not just in the final layer, but any layers in the image. one of the counterintuitive problems introduced by container layers is that deleting a file in one layer doesn’t delete that file from preceding layers. it still takes building application images with docker | 19 up space, and it can be accessed by anyone with the right tools—an enterprising attacker can simply create an image that only consists of the layers that contain the password. secrets and images should never be mixed. if you do so, you will be hacked, and you will bring shame to your entire company or department. we all want to be on tv someday, but there are better ways to go about that. multistage image builds one of the most common ways to accidentally build large images is to do the actual program compilation as part of the construction of the application container image. compiling code as part of the image build feels natural, and it is the easiest way to build a container image from your program. the trouble with doing this is that it leaves all of the unnecessary development tools, which are usually quite large, lying around inside of your image and slowing down your deployments. to resolve this problem, docker introduced multistage builds. with multistage builds, rather than producing a single image, a docker file can actually produce multiple images. each image is considered a stage. artifacts can be copied from preceding stages to the current stage. to illustrate this concretely, we will look at how to build our example application, kuard. this is a somewhat complicated application that involves a react.js frontend (with its own build process) that then gets embedded into a go program. the go program runs a backend api server that the react.js frontend interacts with. a simple dockerfile might look like this: from golang:1.11-alpine # install node and npm run apk update && apk upgrade && apk add --no-cache git nodejs bash npm # get dependencies for go part of build run go get -u github.com/jteeuwen/go-bindata/... run go get github.com/tools/godep workdir /go/src/github.com/kubernetes-up-and-running/kuard # copy all sources in copy . . # this is a set of variables that the build script expects env verbose=0 env pkg=github.com/kubernetes-up-and-running/kuard env arch=amd64 env version=test 20 | chapter 2: creating and running containers # do the build. this script is part of incoming sources. run build/build.sh cmd  this dockerfile produces a container image containing a static executable, but it also contains all of the go development tools and the tools to build the react.js frontend and the source code for the application, neither of which are needed by the final application. the image, across all layers, adds up to over 500 mb. to see how we would do this with multistage builds, examine this multistage docker‐ file: # stage 1: build from golang:1.11-alpine as build # install node and npm run apk update && apk upgrade && apk add --no-cache git nodejs bash npm # get dependencies for go part of build run go get -u github.com/jteeuwen/go-bindata/... run go get github.com/tools/godep workdir /go/src/github.com/kubernetes-up-and-running/kuard # copy all sources in copy . . # this is a set of variables that the build script expects env verbose=0 env pkg=github.com/kubernetes-up-and-running/kuard env arch=amd64 env version=test # do the build. script is part of incoming sources. run build/build.sh # stage 2: deployment from alpine user nobody:nobody copy --from=build /go/bin/kuard /kuard cmd  this dockerfile produces two images. the first is the build image, which contains the go compiler, react.js toolchain, and source code for the program. the second is the deployment image, which simply contains the compiled binary. building a container image using multistage builds can reduce your final container image size by hundreds of megabytes and thus dramatically speed up your deployment times, since generally, multistage image builds | 21 deployment latency is gated on network performance. the final image produced from this dockerfile is somewhere around 20 mb. you can build and run this image with the following commands: $ docker build -t kuard . $ docker run --rm -p 8080:8080 kuard storing images in a remote registry what good is a container image if it’s only available on a single machine? kubernetes relies on the fact that images described in a pod manifest are available across every machine in the cluster. one option for getting this image to all machines in the cluster would be to export the kuard image and import it on each of them. we can’t think of anything more tedious than managing docker images this way. the process of manually importing and exporting docker images has human error writ‐ ten all over it. just say no! the standard within the docker community is to store docker images in a remote registry. there are tons of options when it comes to docker registries, and what you choose will be largely based on your needs in terms of security and collaboration features. generally speaking, the first choice you need to make regarding a registry is whether to use a private or a public registry. public registries allow anyone to download images stored in the registry, while private registries require authentication to down‐ load images. in choosing public versus private, it’s helpful to consider your use case. public registries are great for sharing images with the world, because they allow for easy, unauthenticated use of the container images. you can easily distribute your soft‐ ware as a container image and have confidence that users everywhere will have the exact same experience. in contrast, a private registry is best for storing applications that are private to your service and that you don’t want the world to use. regardless, to push an image, you need to authenticate to the registry. you can gener‐ ally do this with the docker login command, though there are some differences for certain registries. in the examples here we are pushing to the google cloud platform registry, called the google container registry (gcr); other clouds, including azure and amazon web services (aws), also have hosted container registries. for new users hosting publicly readable images, the docker hub is a great place to start. once you are logged in, you can tag the kuard image by prepending the target docker registry. you can also append another identifier that is usually used for the version or variant of that image, separated by a colon (:): 22 | chapter 2: creating and running containers $ docker tag kuard gcr.io/kuar-demo/kuard-amd64:blue then you can push the kuard image: $ docker push gcr.io/kuar-demo/kuard-amd64:blue now that the kuard image is available on a remote registry, it’s time to deploy it using docker. because we pushed it to the public docker registry, it will be available every‐ where without authentication. the docker container runtime kubernetes provides an api for describing an application deployment, but relies on a container runtime to set up an application container using the container-specific apis native to the target os. on a linux system that means configuring cgroups and namespaces. the interface to this container runtime is defined by the container run‐ time interface (cri) standard. the cri api is implemented by a number of different programs, including the containerd-cri built by docker and the cri-o implementa‐ tion contributed by red hat. running containers with docker though generally in kubernetes containers are launched by a daemon on each node called the kubelet, it’s easier to get started with containers using the docker command-line tool. the docker cli tool can be used to deploy containers. to deploy a container from the gcr.io/kuar-demo/kuard-amd64:blue image, run the following command: $ docker run -d --name kuard \\\\  --publish 8080:8080 \\\\  gcr.io/kuar-demo/kuard-amd64:blue this command starts the kuard container and maps ports 8080 on your local machine to 8080 in the container. the --publish option can be shortened to -p. this forwarding is necessary because each container gets its own ip address, so listening on localhost inside the container doesn’t cause you to listen on your machine. without the port forwarding, connections will be inaccessible to your machine. the -d option specifies that this should run in the background (daemon), while --name kuard gives the container a friendly name. exploring the kuard application kuard exposes a simple web interface, which you can load by pointing your browser at http://localhost:8080 or via the command line: $ curl http://localhost:8080 the docker container runtime | 23 kuard also exposes a number of interesting functions that we will explore later on in this book. limiting resource usage docker provides the ability to limit the amount of resources used by applications by exposing the underlying cgroup technology provided by the linux kernel. these capabilities are likewise used by kubernetes to limit the resources used by each pod. limiting memory resources one of the key benefits to running applications within a container is the ability to restrict resource utilization. this allows multiple applications to coexist on the same hardware and ensures fair usage. to limit kuard to 200 mb of memory and 1 gb of swap space, use the --memory and --memory-swap flags with the docker run command. stop and remove the current kuard container: $ docker stop kuard $ docker rm kuard then start another kuard container using the appropriate flags to limit memory usage: $ docker run -d --name kuard \\\\  --publish 8080:8080 \\\\  --memory 200m \\\\  --memory-swap 1g \\\\  gcr.io/kuar-demo/kuard-amd64:blue if the program in the container uses too much memory, it will be terminated. limiting cpu resources another critical resource on a machine is the cpu. restrict cpu utilization using the --cpu-shares flag with the docker run command: $ docker run -d --name kuard \\\\  --publish 8080:8080 \\\\  --memory 200m \\\\  --memory-swap 1g \\\\  --cpu-shares 1024 \\\\  gcr.io/kuar-demo/kuard-amd64:blue cleanup once you are done building an image, you can delete it with the docker rmi command: 24 | chapter 2: creating and running containers docker rmi <tag-name> or: docker rmi <image-id> images can either be deleted via their tag name (e.g., gcr.io/kuar-demo/kuardamd64:blue) or via their image id. as with all id values in the docker tool, the image id can be shortened as long as it remains unique. generally only three or four char‐ acters of the id are necessary. it’s important to note that unless you explicitly delete an image it will live on your system forever, even if you build a new image with an identical name. building this new image simply moves the tag to the new image; it doesn’t delete or replace the old image. consequently, as you iterate while you are creating a new image, you will often create many, many different images that end up taking up unnecessary space on your computer. to see the images currently on your machine, you can use the docker images com‐ mand. you can then delete tags you are no longer using. docker provides a tool called docker system prune for doing general cleanup. this will remove all stopped containers, all untagged images, and all unused image layers cached as part of the build process. use it carefully. a slightly more sophisticated approach is to set up a cron job to run an image garbage collector. for example, the docker-gc tool is a commonly used image garbage collector that can easily run as a recurring cron job, once per day or once per hour, depending on how many images you are creating. summary application containers provide a clean abstraction for applications, and when pack‐ aged in the docker image format, applications become easy to build, deploy, and dis‐ tribute. containers also provide isolation between applications running on the same machine, which helps avoid dependency conflicts. in future chapters we’ll see how the ability to mount external directories means we can run not only stateless applications in a container, but also applications like mysql and others that generate lots of data. summary | 25  chapter 3 deploying a kubernetes cluster now that you have successfully built an application container, the next step is to learn how to transform it into a complete, reliable, scalable distributed system. to do that, you need a working kubernetes cluster. at this point, there are cloud-based kuber‐ netes services in most public clouds that make it easy to create a cluster with a few command-line instructions. we highly recommend this approach if you are just get‐ ting started with kubernetes. even if you are ultimately planning on running kuber‐ netes on bare metal, it’s a good way to quickly get started with kubernetes, learn about kubernetes itself, and then learn how to install it on physical machines. fur‐ thermore, managing a kubernetes cluster is a complicated task in itself, and for most people it makes sense to defer this management to the cloud—especially when in most clouds the management service is free. of course, using a cloud-based solution requires paying for those cloud-based resour‐ ces as well as having an active network connection to the cloud. for these reasons, local development can be more attractive, and in that case the minikube tool provides an easy-to-use way to get a local kubernetes cluster up running in a vm on your local laptop or desktop. though this is a nice option, minikube only creates a single-node cluster, which doesn’t quite demonstrate all of the aspects of a complete kubernetes cluster. for that reason, we recommend people start with a cloud-based solution, unless it really doesn’t work for their situation. a more recent alternative is to run a docker-in-docker cluster, which can spin up a multi-node cluster on a single machine. this project is still in beta, though, so keep in mind that you may encounter unexpected issues. if you truly insist on starting on bare metal, appendix a at the end of this book gives instructions for building a cluster from a collection of raspberry pi single-board computers. these instructions use the kubeadm tool and can be adapted to other machines beyond raspberry pis. 27 installing kubernetes on a public cloud provider this chapter covers installing kubernetes on the three major cloud providers: ama‐ zon web services, microsoft azure, and the google cloud platform. if you choose to use a cloud provider to manage kubernetes, you only need to install one of these options; once you have a cluster configured and ready to go you can skip to “the kubernetes client” on page 31, unless you would prefer to install kubernetes elsewhere. google kubernetes engine the google cloud platform offers a hosted kubernetes-as-a-service called google kubernetes engine (gke). to get started with gke, you need a google cloud plat‐ form account with billing enabled and the gcloud tool installed. once you have gcloud installed, first set a default zone: $ gcloud config set compute/zone us-west1-a then you can create a cluster: $ gcloud container clusters create kuar-cluster this will take a few minutes. when the cluster is ready you can get credentials for the cluster using: $ gcloud auth application-default login if you run into trouble, you can find the complete instructions for creating a gke cluster in the google cloud platform documentation. installing kubernetes with azure kubernetes service microsoft azure offers a hosted kubernetes-as-a-service as part of the azure con‐ tainer service. the easiest way to get started with azure container service is to use the built-in azure cloud shell in the azure portal. you can activate the shell by click‐ ing the shell icon in the upper-right toolbar: the shell has the az tool automatically installed and configured to work with your azure environment. alternatively, you can install the az command-line interface (cli) on your local machine. when you have the shell up and working, you can run: 28 | chapter 3: deploying a kubernetes cluster $ az group create --name=kuar --location=westus once the resource group is created, you can create a cluster using: $ az aks create --resource-group=kuar --name=kuar-cluster this will take a few minutes. once the cluster is created, you can get credentials for the cluster with: $ az aks get-credentials --resource-group=kuar --name=kuar-cluster if you don’t already have the kubectl tool installed, you can install it using: $ az aks install-cli you can find complete instructions for installing kubernetes on azure in the azure documentation. installing kubernetes on amazon web services amazon offers a managed kubernetes service called elastic kubernetes service (eks). the easiest way to create an eks cluster is via the open source eksctl command-line tool.. once you have eksctl installed and in your path, you can run the following com‐ mand to create a cluster: $ eksctl create cluster --name kuar-cluster ... for more details on installation options (such as node size and more), view the help using this command: $ eksctl create cluster --help the cluster installation includes the right configuration for the kubectl commandline tool. if you don’t already have kubectl installed, you can follow the instructions in the documentation. installing kubernetes locally using minikube if you need a local development experience, or you don’t want to pay for cloud resources, you can install a simple single-node cluster using minikube. alternatively, if you have already installed docker desktop, it comes bundled with a single-machine installation of kubernetes. while minikube (or docker desktop) is a good simulation of a kubernetes cluster, it’s really intended for local development, learning, and experimentation. because it only runs in a vm on a single node, it doesn’t provide the reliability of a distributed kubernetes cluster. installing kubernetes locally using minikube | 29 in addition, certain features described in this book require integration with a cloud provider. these features are either not available or work in a limited way with minikube. you need to have a hypervisor installed on your machine to use minikube. for linux and macos, this is generally virtualbox. on windows, the hyper-v hypervisor is the default option. make sure you install the hypervisor before using minikube. you can find the minikube tool on github. there are binaries for linux, macos, and windows that you can download. once you have the minikube tool installed, you can create a local cluster using: $ minikube start this will create a local vm, provision kubernetes, and create a local kubectl configu‐ ration that points to that cluster. when you are done with your cluster, you can stop the vm with: $ minikube stop if you want to remove the cluster, you can run: $ minikube delete running kubernetes in docker a different approach to running a kubernetes cluster has been developed more recently, which uses docker containers to simulate multiple kubernetes nodes instead of running everything in a virtual machine. the kind project provides a great experi‐ ence for launching and managing test clusters in docker. (kind stands for kubernetes in docker.) kind is still a work in progress (pre 1.0), but is widely used by those building kubernetes for fast and easy testing. installation instructions for your platform can be found at the kind site. once you get it installed, creating a cluster is as easy as: $ kind create cluster --wait 5m \\\\ $ export kubeconfig=\"$(kind get kubeconfig-path)\" $ kubectl cluster-info $ kind delete cluster 30 | chapter 3: deploying a kubernetes cluster running kubernetes on raspberry pi if you want to experiment with a realistic kubernetes cluster but don’t want to pay a lot, a very nice kubernetes cluster can be built on top of raspberry pi computers for a relatively small cost. the details of building such a cluster are out of scope for this chapter, but they are given in appendix a at the end of this book. the kubernetes client the official kubernetes client is kubectl: a command-line tool for interacting with the kubernetes api. kubectl can be used to manage most kubernetes objects, such as pods, replicasets, and services. kubectl can also be used to explore and verify the overall health of the cluster. we’ll use the kubectl tool to explore the cluster you just created. checking cluster status the first thing you can do is check the version of the cluster that you are running: $ kubectl version this will display two different versions: the version of the local kubectl tool, as well as the version of the kubernetes api server. don’t worry if these versions are different. the kubernetes tools are backward- and forward-compatible with different versions of the kubernetes api, so long as you stay within two minor versions for both the tools and the cluster and don’t try to use newer fea‐ tures on an older cluster. kubernetes follows the semantic version‐ ing specification, where the minor version is the middle number (e.g., the 5 in 1.5.2). now that we’ve established that you can communicate with your kubernetes cluster, we’ll explore the cluster in more depth. first, you can get a simple diagnostic for the cluster. this is a good way to verify that your cluster is generally healthy: $ kubectl get componentstatuses the output should look like this: name status message error scheduler healthy ok controller-manager healthy ok etcd-0 healthy {\"health\": \"true\"} running kubernetes on raspberry pi | 31 as kubernetes changes and improves over time, the output of the kubectl command sometimes changes. don’t worry if the output doesn’t look exactly identical to what is shown in the examples in this book. you can see here the components that make up the kubernetes cluster. the controller-manager is responsible for running various controllers that regulate behavior in the cluster; for example, ensuring that all of the replicas of a service are available and healthy. the scheduler is responsible for placing different pods onto different nodes in the cluster. finally, the etcd server is the storage for the cluster where all of the api objects are stored. listing kubernetes worker nodes next, you can list out all of the nodes in your cluster: $ kubectl get nodes name status age version kubernetes ready,master 45d v1.12.1 node-1 ready 45d v1.12.1 node-2 ready 45d v1.12.1 node-3 ready 45d v1.12.1 you can see this is a four-node cluster that’s been up for 45 days. in kubernetes, nodes are separated into master nodes that contain containers like the api server, schedu‐ ler, etc., which manage the cluster, and worker nodes where your containers will run. kubernetes won’t generally schedule work onto master nodes to ensure that user workloads don’t harm the overall operation of the cluster. you can use the kubectl describe command to get more information about a spe‐ cific node, such as node-1: $ kubectl describe nodes node-1 first, you see basic information about the node: name: node-1 role: labels: beta.kubernetes.io/arch=arm  beta.kubernetes.io/os=linux  kubernetes.io/hostname=node-1 you can see that this node is running the linux os and is running on an arm pro‐ cessor. next, you see information about the operation of node-1 itself: 32 | chapter 3: deploying a kubernetes cluster conditions:  type status lastheartbeattime reason message  ---- ------ ----------------- ------ -------  outofdisk false sun, 05 feb 2017… kubelethassufficientdisk kubelet…  memorypressure false sun, 05 feb 2017… kubelethassufficientmemory kubelet…  diskpressure false sun, 05 feb 2017… kubelethasnodiskpressure kubelet…  ready true sun, 05 feb 2017… kubeletready kubelet… these statuses show that the node has sufficient disk and memory space and is reporting that it is healthy to the kubernetes master. next, there is information about the capacity of the machine: capacity:  alpha.kubernetes.io/nvidia-gpu: 0  cpu: 4  memory: 882636ki  pods: 110 allocatable:  alpha.kubernetes.io/nvidia-gpu: 0  cpu: 4  memory: 882636ki  pods: 110 then there is information about the software on the node, including the version of docker that is running, the versions of kubernetes and the linux kernel, and more: system info:  machine id: 9122895d0d494e3f97dda1e8f969c85c  system uuid: a7dbf2ce-db1e-e34a-969a-3355c36a2149  boot id: ba53d5ee-27d2-4b6a-8f19-e5f702993ec6  kernel version: 4.15.0-1037-azure  os image: ubuntu 16.04.5 lts  operating system: linux  architecture: amd64  container runtime version: docker://3.0.4  kubelet version: v1.12.6  kube-proxy version: v1.12.6 podcidr: 10.244.1.0/24 finally, there is information about the pods that are currently running on this node: non-terminated pods: (3 in total)  namespace name cpu requests cpu limits memory requests memory limits  --------- ---- ------------ ---------- --------------- -------------  kube-system kube-dns... 260m (6%) 0 (0%) 140mi (16%) 220mi (25%)  kube-system kube-fla... 0 (0%) 0 (0%) 0 (0%) 0 (0%)  kube-system kube-pro... 0 (0%) 0 (0%) 0 (0%) 0 (0%) allocated resources:  (total limits may be over 100 percent, i.e., overcommitted.  cpu requests cpu limits memory requests memory limits  ------------ ---------- --------------- -------------  260m (6%) 0 (0%) 140mi (16%) 220mi (25%) no events. the kubernetes client | 33 1 as you’ll learn in the next chapter, a namespace in kubernetes is an entity for organizing kubernetes resour‐ ces. you can think of it like a folder in a filesystem. from this output you can see the pods on the node (e.g., the kube-dns pod that sup‐ plies dns services for the cluster), the cpu and memory that each pod is requesting from the node, as well as the total resources requested. it’s worth noting here that kubernetes tracks both the requests and upper limits for resources for each pod that runs on a machine. the difference between requests and limits is described in detail in chapter 5, but in a nutshell, resources requested by a pod are guaranteed to be present on the node, while a pod’s limit is the maximum amount of a given resource that a pod can consume. a pod’s limit can be higher than its request, in which case the extra resources are supplied on a best-effort basis. they are not guaranteed to be present on the node. cluster components one of the interesting aspects of kubernetes is that many of the components that make up the kubernetes cluster are actually deployed using kubernetes itself. we’ll take a look at a few of these. these components use a number of the concepts that we’ll introduce in later chapters. all of these components run in the kube-system namespace.1 kubernetes proxy the kubernetes proxy is responsible for routing network traffic to load-balanced services in the kubernetes cluster. to do its job, the proxy must be present on every node in the cluster. kubernetes has an api object named daemonset, which you will learn about later in the book, that is used in many clusters to accomplish this. if your cluster runs the kubernetes proxy with a daemonset, you can see the proxies by running: $ kubectl get daemonsets --namespace=kube-system kube-proxy name desired current ready node-selector age kube-proxy 4 4 4 <none> 45d depending on how your cluster is set up, the daemonset for the kube-proxy may be named something else, or its possible that it won’t use a daemonset at all. regardless, the kube-proxy container should be running on all nodes in a cluster. kubernetes dns kubernetes also runs a dns server, which provides naming and discovery for the services that are defined in the cluster. this dns server also runs as a replicated ser‐ vice on the cluster. depending on the size of your cluster, you may see one or more 34 | chapter 3: deploying a kubernetes cluster dns servers running in your cluster. the dns service is run as a kubernetes deploy‐ ment, which manages these replicas: $ kubectl get deployments --namespace=kube-system core-dns name desired current up-to-date available age core-dns 1 1 1 1 45d there is also a kubernetes service that performs load balancing for the dns server: $ kubectl get services --namespace=kube-system core-dns name cluster-ip external-ip port(s) age core-dns 10.96.0.10 <none> 53/udp,53/tcp 45d this shows that the dns service for the cluster has the address 10.96.0.10. if you log in to a container in the cluster, you’ll see that this has been populated into the /etc/ resolv.conf file for the container. with kubernetes 1.12, kubernetes transitioned from the kube-dns dns server to the core-dns dns server. because of this, if you are running an older kubernetes cluster, you may see kube-dns instead. kubernetes ui the final kubernetes component is a gui. the ui is run as a single replica, but it is still managed by a kubernetes deployment for reliability and upgrades. you can see this ui server using: $ kubectl get deployments --namespace=kube-system kubernetes-dashboard name desired current up-to-date available age kubernetes-dashboard 1 1 1 1 45d the dashboard also has a service that performs load balancing for the dashboard: $ kubectl get services --namespace=kube-system kubernetes-dashboard name cluster-ip external-ip port(s) age kubernetes-dashboard 10.99.104.174 <nodes> 80:32551/tcp 45d you can use kubectl proxy to access this ui. launch the kubernetes proxy using: $ kubectl proxy this starts up a server running on localhost:8001. if you visit http://localhost: 8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/ proxy/ in your web browser, you should see the kubernetes web ui. you can use this interface to explore your cluster, as well as create new containers. the full details of this interface are outside of the scope of this book, and it is changing rapidly as the dashboard is improved. cluster components | 35 some providers don’t install the kubernetes dashboard by default, so don’t worry if you don’t see it in your cluster. documentation on how to install the dashboard for these clusters is available at https://kubernetes.io/docs/tasks/access-application-cluster/ web-ui-dashboard/. summary hopefully at this point you have a kubernetes cluster (or three) up and running and you’ve used a few commands to explore the cluster you have created. next, we’ll spend some more time exploring the command-line interface to that kubernetes cluster and teach you how to master the kubectl tool. throughout the rest of the book, you’ll be using kubectl and your test cluster to explore the various objects in the kubernetes api. 36 | chapter 3: deploying a kubernetes cluster chapter 4 common kubectl commands the kubectl command-line utility is a powerful tool, and in the following chapters you will use it to create objects and interact with the kubernetes api. before that, however, it makes sense to go over the basic kubectl commands that apply to all kubernetes objects. namespaces kubernetes uses namespaces to organize objects in the cluster. you can think of each namespace as a folder that holds a set of objects. by default, the kubectl commandline tool interacts with the default namespace. if you want to use a different name‐ space, you can pass kubectl the --namespace flag. for example, kubectl --namespace=mystuff references objects in the mystuff namespace. if you want to interact with all namespaces—for example, to list all pods in your cluster— you can pass the --all-namespaces flag. contexts if you want to change the default namespace more permanently, you can use a con‐ text. this gets recorded in a kubectl configuration file, usually located at $home/.kube/config. this configuration file also stores how to both find and authen‐ ticate to your cluster. for example, you can create a context with a different default namespace for your kubectl commands using: $ kubectl config set-context my-context --namespace=mystuff this creates a new context, but it doesn’t actually start using it yet. to use this newly created context, you can run: $ kubectl config use-context my-context 37 contexts can also be used to manage different clusters or different users for authenti‐ cating to those clusters using the --users or --clusters flags with the set-context command. viewing kubernetes api objects everything contained in kubernetes is represented by a restful resource. through‐ out this book, we refer to these resources as kubernetes objects. each kubernetes object exists at a unique http path; for example, https://your-k8s.com/api/v1/name‐ spaces/default/pods/my-pod leads to the representation of a pod in the default name‐ space named my-pod. the kubectl command makes http requests to these urls to access the kubernetes objects that reside at these paths. the most basic command for viewing kubernetes objects via kubectl is get. if you run kubectl get <resource-name> you will get a listing of all resources in the cur‐ rent namespace. if you want to get a specific resource, you can use kubectl get <resource-name> <obj-name>. by default, kubectl uses a human-readable printer for viewing the responses from the api server, but this human-readable printer removes many of the details of the objects to fit each object on one terminal line. one way to get slightly more informa‐ tion is to add the -o wide flag, which gives more details, on a longer line. if you want to view the complete object, you can also view the objects as raw json or yaml using the -o json or -o yaml flags, respectively. a common option for manipulating the output of kubectl is to remove the headers, which is often useful when combining kubectl with unix pipes (e.g., kubectl ... | awk ...). if you specify the --no-headers flag, kubectl will skip the headers at the top of the human-readable table. another common task is extracting specific fields from the object. kubectl uses the jsonpath query language to select fields in the returned object. the complete details of jsonpath are beyond the scope of this chapter, but as an example, this command will extract and print the ip address of the specified pod: $ kubectl get pods my-pod -o jsonpath --template={.status.podip} if you are interested in more detailed information about a particular object, use the describe command: $ kubectl describe <resource-name> <obj-name> this will provide a rich multiline human-readable description of the object as well as any other relevant, related objects and events in the kubernetes cluster. 38 | chapter 4: common kubectl commands creating, updating, and destroying kubernetes objects objects in the kubernetes api are represented as json or yaml files. these files are either returned by the server in response to a query or posted to the server as part of an api request. you can use these yaml or json files to create, update, or delete objects on the kubernetes server. let’s assume that you have a simple object stored in obj.yaml. you can use kubectl to create this object in kubernetes by running: $ kubectl apply -f obj.yaml notice that you don’t need to specify the resource type of the object; it’s obtained from the object file itself. similarly, after you make changes to the object, you can use the apply command again to update the object: $ kubectl apply -f obj.yaml the apply tool will only modify objects that are different from the current objects in the cluster. if the objects you are creating already exist in the cluster, it will simply exit successfully without making any changes. this makes it useful for loops where you want to ensure the state of the cluster matches the state of the filesystem. you can repeatedly use apply to reconcile state. if you want to see what the apply command will do without actually making the changes, you can use the --dry-run flag to print the objects to the terminal without actually sending them to the server. if you feel like making interactive edits instead of editing a local file, you can instead use the edit command, which will download the latest object state and then launch an editor that contains the definition: $ kubectl edit <resource-name> <obj-name> after you save the file, it will be automatically uploaded back to the kubernetes cluster. the apply command also records the history of previous configurations in an anno‐ tation within the object. you can manipulate these records with the edit-lastapplied, set-last-applied, and view-last-applied commands. for example: $ kubectl apply -f myobj.yaml view-last-applied will show you the last state that was applied to the object. creating, updating, and destroying kubernetes objects | 39 when you want to delete an object, you can simply run: $ kubectl delete -f obj.yaml it is important to note that kubectl will not prompt you to confirm the deletion. once you issue the command, the object will be deleted. likewise, you can delete an object using the resource type and name: $ kubectl delete <resource-name> <obj-name> labeling and annotating objects labels and annotations are tags for your objects. we’ll discuss the differences in chapter 6, but for now, you can update the labels and annotations on any kubernetes object using the annotate and label commands. for example, to add the color=red label to a pod named bar, you can run: $ kubectl label pods bar color=red the syntax for annotations is identical. by default, label and annotate will not let you overwrite an existing label. to do this, you need to add the --overwrite flag. if you want to remove a label, you can use the <label-name>- syntax: $ kubectl label pods bar colorthis will remove the color label from the pod named bar. debugging commands kubectl also makes a number of commands available for debugging your containers. you can use the following to see the logs for a running container: $ kubectl logs <pod-name> if you have multiple containers in your pod, you can choose the container to view using the -c flag. by default, kubectl logs lists the current logs and exits. if you instead want to con‐ tinuously stream the logs back to the terminal without exiting, you can add the -f (follow) command-line flag. you can also use the exec command to execute a command in a running container: $ kubectl exec -it <pod-name> -- bash this will provide you with an interactive shell inside the running container so that you can perform more debugging. 40 | chapter 4: common kubectl commands if you don’t have bash or some other terminal available within your container, you can always attach to the running process: $ kubectl attach -it <pod-name> this will attach to the running process. it is similar to kubectl logs but will allow you to send input to the running process, assuming that process is set up to read from standard input. you can also copy files to and from a container using the cp command: $ kubectl cp <pod-name>:</path/to/remote/file> </path/to/local/file> this will copy a file from a running container to your local machine. you can also specify directories, or reverse the syntax to copy a file from your local machine back out into the container. if you want to access your pod via the network, you can use the port-forward com‐ mand to forward network traffic from the local machine to the pod. this enables you to securely tunnel network traffic through to containers that might not be exposed anywhere on the public network. for example, the following command: $ kubectl port-forward <pod-name> 8080:80 opens up a connection that forwards traffic from the local machine on port 8080 to the remote container on port 80. you can also use the port-forward command with services by specifying services/<service-name> instead of <pod-name>, but note that if you do port-forward to a service, the requests will only ever be forwarded to a single pod in that service. they will not go through the service load balancer. finally, if you are interested in how your cluster is using resources, you can use the top command to see the list of resources in use by either nodes or pods. this command: kubectl top nodes will display the total cpu and memory in use by the nodes in terms of both absolute units (e.g., cores) and percentage of available resources (e.g., total number of cores). similarly, this command: kubectl top pods will show all pods and their resource usage. by default it only displays pods in the current namespace, but you can add the --all-namespaces flag to see resource usage by all pods in the cluster. debugging commands | 41 command autocompletion kubectl supports integration with your shell to enable tab completion for both com‐ mands and resources. depending on your environment, you may need to install the bash-completion package before you activate command autocompletion. you can do this using the appropriate package manager: # macos brew install bash-completion # centos/red hat yum install bash-completion # debian/ubuntu apt-get install bash-completion when installing on macos, make sure to follow the instructions from brew about how to activate tab completion using your ${home}/.bashprofile. once bash-completion is installed, you can temporarily activate it for your terminal using: source <(kubectl completion bash) to make this automatic for every terminal, you can add it to your ${home}/.bashrc file: echo \"source <(kubectl completion bash)\" >> ${home}/.bashrc if you use zsh you can find similar instructions online. alternative ways of viewing your cluster in addition to kubectl, there are other tools for interacting with your kubernetes cluster. for example, there are plug-ins for several editors that integrate kubernetes and the editor environment, including: • visual studio code • intellij • eclipse additionally, there is an open source mobile application that allows you to access your cluster from your phone. 42 | chapter 4: common kubectl commands summary kubectl is a powerful tool for managing your applications in your kubernetes cluster. this chapter has illustrated many of the common uses for the tool, but kubectl has a great deal of built-in help available. you can start viewing this help with: $ kubectl help or: $ kubectl help <command-name> summary | 43  chapter 5 pods in earlier chapters we discussed how you might go about containerizing your applica‐ tion, but in real-world deployments of containerized applications you will often want to colocate multiple applications into a single atomic unit, scheduled onto a single machine. a canonical example of such a deployment is illustrated in figure 5-1, which consists of a container serving web requests and a container synchronizing the filesystem with a remote git repository. figure 5-1. an example pod with two containers and a shared filesystem at first, it might seem tempting to wrap up both the web server and the git syn‐ chronizer into a single container. after closer inspection, however, the reasons for the separation become clear. first, the two different containers have significantly different requirements in terms of resource usage. take, for example, memory. because the web server is serving user requests, we want to ensure that it is always available and responsive. on the other hand, the git synchronizer isn’t really user-facing and has a “best effort” quality of service. 45 suppose that our git synchronizer has a memory leak. we need to ensure that the git synchronizer cannot use up memory that we want to use for our web server, since this can affect web server performance or even crash the server. this sort of resource isolation is exactly the sort of thing that containers are designed to accomplish. by separating the two applications into two separate containers, we can ensure reliable web server operation. of course, the two containers are quite symbiotic; it makes no sense to schedule the web server on one machine and the git synchronizer on another. consequently, kubernetes groups multiple containers into a single atomic unit called a pod. (the name goes with the whale theme of docker containers, since a pod is also a group of whales.) though the concept of such sidecars seemed controversial or con‐ fusing when it was first introduced in kubernetes, it has subse‐ quently been adopted by a variety of different applications to deploy their infrastructure. for example, several service mesh implementations use sidecars to inject network management into an application’s pod. pods in kubernetes a pod represents a collection of application containers and volumes running in the same execution environment. pods, not containers, are the smallest deployable arti‐ fact in a kubernetes cluster. this means all of the containers in a pod always land on the same machine. each container within a pod runs in its own cgroup, but they share a number of linux namespaces. applications running in the same pod share the same ip address and port space (net‐ work namespace), have the same hostname (uts namespace), and can communicate using native interprocess communication channels over system v ipc or posix message queues (ipc namespace). however, applications in different pods are iso‐ lated from each other; they have different ip addresses, different hostnames, and more. containers in different pods running on the same node might as well be on different servers. thinking with pods one of the most common questions that occurs in the adoption of kubernetes is “what should i put in a pod?” 46 | chapter 5: pods sometimes people see pods and think, “aha! a wordpress container and a mysql database container should be in the same pod.” however, this kind of pod is actually an example of an anti-pattern for pod construction. there are two reasons for this. first, wordpress and its database are not truly symbiotic. if the wordpress container and the database container land on different machines, they still can work together quite effectively, since they communicate over a network connection. secondly, you don’t necessarily want to scale wordpress and the database as a unit. wordpress itself is mostly stateless, and thus you may want to scale your wordpress frontends in response to frontend load by creating more wordpress pods. scaling a mysql data‐ base is much trickier, and you would be much more likely to increase the resources dedicated to a single mysql pod. if you group the wordpress and mysql containers together in a single pod, you are forced to use the same scaling strategy for both con‐ tainers, which doesn’t fit well. in general, the right question to ask yourself when designing pods is, “will these con‐ tainers work correctly if they land on different machines?” if the answer is “no,” a pod is the correct grouping for the containers. if the answer is “yes,” multiple pods is probably the correct solution. in the example at the beginning of this chapter, the two containers interact via a local filesystem. it would be impossible for them to operate correctly if the containers were scheduled on different machines. in the remaining sections of this chapter, we will describe how to create, introspect, manage, and delete pods in kubernetes. the pod manifest pods are described in a pod manifest. the pod manifest is just a text-file representa‐ tion of the kubernetes api object. kubernetes strongly believes in declarative configu‐ ration. declarative configuration means that you write down the desired state of the world in a configuration and then submit that configuration to a service that takes actions to ensure the desired state becomes the actual state. declarative configuration is different from imperative configura‐ tion, where you simply take a series of actions (e.g., apt-get install foo) to modify the world. years of production experience have taught us that maintaining a written record of the system’s desired state leads to a more manageable, reliable system. declara‐ tive configuration enables numerous advantages, including code review for configurations as well as documenting the current state of the world for distributed teams. additionally, it is the basis for all of the self-healing behaviors in kubernetes that keep applica‐ tions running without user action. the pod manifest | 47 the kubernetes api server accepts and processes pod manifests before storing them in persistent storage (etcd). the scheduler also uses the kubernetes api to find pods that haven’t been scheduled to a node. the scheduler then places the pods onto nodes depending on the resources and other constraints expressed in the pod manifests. multiple pods can be placed on the same machine as long as there are sufficient resources. however, scheduling multiple replicas of the same application onto the same machine is worse for reliability, since the machine is a single failure domain. consequently, the kubernetes scheduler tries to ensure that pods from the same application are distributed onto different machines for reliability in the presence of such failures. once scheduled to a node, pods don’t move and must be explicitly destroyed and rescheduled. multiple instances of a pod can be deployed by repeating the workflow described here. however, replicasets (chapter 9) are better suited for running multiple instan‐ ces of a pod. (it turns out they’re also better at running a single pod, but we’ll get into that later.) creating a pod the simplest way to create a pod is via the imperative kubectl run command. for example, to run our same kuard server, use: $ kubectl run kuard --generator=run-pod/v1 \\\\  --image=gcr.io/kuar-demo/kuard-amd64:blue you can see the status of this pod by running: $ kubectl get pods you may initially see the container as pending, but eventually you will see it transition to running, which means that the pod and its containers have been successfully created. for now, you can delete this pod by running: $ kubectl delete pods/kuard we will now move on to writing a complete pod manifest by hand. creating a pod manifest pod manifests can be written using yaml or json, but yaml is generally preferred because it is slightly more human-editable and has the ability to add comments. pod manifests (and other kubernetes api objects) should really be treated in the same way as source code, and things like comments help explain the pod to new team members who are looking at them for the first time. 48 | chapter 5: pods pod manifests include a couple of key fields and attributes: namely a metadata sec‐ tion for describing the pod and its labels, a spec section for describing volumes, and a list of containers that will run in the pod. in chapter 2 we deployed kuard using the following docker command: $ docker run -d --name kuard \\\\  --publish 8080:8080 \\\\  gcr.io/kuar-demo/kuard-amd64:blue a similar result can be achieved by instead writing example 5-1 to a file named kuard-pod.yaml and then using kubectl commands to load that manifest to kubernetes. example 5-1. kuard-pod.yaml apiversion: v1 kind: pod metadata:  name: kuard spec:  containers:  - image: gcr.io/kuar-demo/kuard-amd64:blue  name: kuard  ports:  - containerport: 8080  name: http  protocol: tcp running pods in the previous section we created a pod manifest that can be used to start a pod run‐ ning kuard. use the kubectl apply command to launch a single instance of kuard: $ kubectl apply -f kuard-pod.yaml the pod manifest will be submitted to the kubernetes api server. the kubernetes system will then schedule that pod to run on a healthy node in the cluster, where it will be monitored by the kubelet daemon process. don’t worry if you don’t under‐ stand all the moving parts of kubernetes right now; we’ll get into more details throughout the book. listing pods now that we have a pod running, let’s go find out some more about it. using the kubectl command-line tool, we can list all pods running in the cluster. for now, this should only be the single pod that we created in the previous step: running pods | 49 $ kubectl get pods name ready status restarts age kuard 1/1 running 0 44s you can see the name of the pod (kuard) that we gave it in the previous yaml file. in addition to the number of ready containers (1/1), the output also shows the status, the number of times the pod was restarted, as well as the age of the pod. if you ran this command immediately after the pod was created, you might see: name ready status restarts age kuard 0/1 pending 0 1s the pending state indicates that the pod has been submitted but hasn’t been sched‐ uled yet. if a more significant error occurs (e.g., an attempt to create a pod with a container image that doesn’t exist), it will also be listed in the status field. by default, the kubectl command-line tool tries to be concise in the information it reports, but you can get more information via command-line flags. adding -o wide to any kubectl command will print out slightly more information (while still trying to keep the information to a single line). adding -o json or -o yaml will print out the complete objects in json or yaml, respectively. pod details sometimes, the single-line view is insufficient because it is too terse. additionally, kubernetes maintains numerous events about pods that are present in the event stream, not attached to the pod object. to find out more information about a pod (or any kubernetes object) you can use the kubectl describe command. for example, to describe the pod we previously cre‐ ated, you can run: $ kubectl describe pods kuard this outputs a bunch of information about the pod in different sections. at the top is basic information about the pod: name: kuard namespace: default node: node1/10.0.15.185 start time: sun, 02 jul 2017 15:00:38 -0700 labels: <none> annotations: <none> status: running ip: 192.168.199.238 controllers: <none> 50 | chapter 5: pods then there is information about the containers running in the pod: containers:  kuard:  container id: docker://055095…  image: gcr.io/kuar-demo/kuard-amd64:blue  image id: docker-pullable://gcr.io/kuar-demo/kuard-amd64@sha256:a580…  port: 8080/tcp  state: running  started: sun, 02 jul 2017 15:00:41 -0700  ready: true  restart count: 0  environment: <none>  mounts:  /var/run/secrets/kubernetes.io/serviceaccount from default-token-cg5f5 (ro) finally, there are events related to the pod, such as when it was scheduled, when its image was pulled, and if/when it had to be restarted because of failing health checks: events:  seen from subobjectpath type reason message  ---- ---- ------------- -------- ------ -------  50s default-scheduler normal scheduled success…  49s kubelet, node1 spec.containers{kuard} normal pulling pulling…  47s kubelet, node1 spec.containers{kuard} normal pulled success…  47s kubelet, node1 spec.containers{kuard} normal created created…  47s kubelet, node1 spec.containers{kuard} normal started started… deleting a pod when it is time to delete a pod, you can delete it either by name: $ kubectl delete pods/kuard or using the same file that you used to create it: $ kubectl delete -f kuard-pod.yaml when a pod is deleted, it is not immediately killed. instead, if you run kubectl get pods you will see that the pod is in the terminating state. all pods have a termina‐ tion grace period. by default, this is 30 seconds. when a pod is transitioned to terminating it no longer receives new requests. in a serving scenario, the grace period is important for reliability because it allows the pod to finish any active requests that it may be in the middle of processing before it is terminated. it’s important to note that when you delete a pod, any data stored in the containers associated with that pod will be deleted as well. if you want to persist data across mul‐ tiple instances of a pod, you need to use persistentvolumes, described at the end of this chapter. running pods | 51 accessing your pod now that your pod is running, you’re going to want to access it for a variety of rea‐ sons. you may want to load the web service that is running in the pod. you may want to view its logs to debug a problem that you are seeing, or even execute other com‐ mands in the context of the pod to help debug. the following sections detail various ways that you can interact with the code and data running inside your pod. using port forwarding later in the book, we’ll show how to expose a service to the world or other containers using load balancers—but oftentimes you simply want to access a specific pod, even if it’s not serving traffic on the internet. to achieve this, you can use the port-forwarding support built into the kubernetes api and command-line tools. when you run: $ kubectl port-forward kuard 8080:8080 a secure tunnel is created from your local machine, through the kubernetes master, to the instance of the pod running on one of the worker nodes. as long as the port-forward command is still running, you can access the pod (in this case the kuard web interface) at http://localhost:8080. getting more info with logs when your application needs debugging, it’s helpful to be able to dig deeper than describe to understand what the application is doing. kubernetes provides two com‐ mands for debugging running containers. the kubectl logs command downloads the current logs from the running instance: $ kubectl logs kuard adding the -f flag will cause you to continuously stream logs. the kubectl logs command always tries to get logs from the currently running con‐ tainer. adding the --previous flag will get logs from a previous instance of the con‐ tainer. this is useful, for example, if your containers are continuously restarting due to a problem at container startup. 52 | chapter 5: pods while using kubectl logs is useful for one-off debugging of con‐ tainers in production environments, it’s generally useful to use a log aggregation service. there are several open source log aggregation tools, like fluentd and elasticsearch, as well as numerous cloud logging providers. log aggregation services provide greater capacity for storing a longer duration of logs, as well as rich log searching and filtering capabilities. finally, they often provide the ability to aggregate logs from multiple pods into a single view. running commands in your container with exec sometimes logs are insufficient, and to truly determine what’s going on you need to execute commands in the context of the container itself. to do this you can use: $ kubectl exec kuard date you can also get an interactive session by adding the -it flags: $ kubectl exec -it kuard ash copying files to and from containers at times you may need to copy files from a remote container to a local machine for more in-depth exploration. for example, you can use a tool like wireshark to visual‐ ize tcpdump packet captures. suppose you had a file called /captures/capture3.txt inside a container in your pod. you could securely copy that file to your local machine by running: $ kubectl cp <pod-name>:/captures/capture3.txt ./capture3.txt other times you may need to copy files from your local machine into a container. let’s say you want to copy $home/config.txt to a remote container. in this case, you can run: $ kubectl cp $home/config.txt <pod-name>:/config.txt generally speaking, copying files into a container is an anti-pattern. you really should treat the contents of a container as immutable. but occasionally it’s the most immedi‐ ate way to stop the bleeding and restore your service to health, since it is quicker than building, pushing, and rolling out a new image. once the bleeding is stopped, how‐ ever, it is critically important that you immediately go and do the image build and rollout, or you are guaranteed to forget the local change that you made to your con‐ tainer and overwrite it in the subsequent regularly scheduled rollout. accessing your pod | 53 health checks when you run your application as a container in kubernetes, it is automatically kept alive for you using a process health check. this health check simply ensures that the main process of your application is always running. if it isn’t, kubernetes restarts it. however, in most cases, a simple process check is insufficient. for example, if your process has deadlocked and is unable to serve requests, a process health check will still believe that your application is healthy since its process is still running. to address this, kubernetes introduced health checks for application liveness. liveness health checks run application-specific logic (e.g., loading a web page) to ver‐ ify that the application is not just still running, but is functioning properly. since these liveness health checks are application-specific, you have to define them in your pod manifest. liveness probe once the kuard process is up and running, we need a way to confirm that it is actually healthy and shouldn’t be restarted. liveness probes are defined per container, which means each container inside a pod is health-checked separately. in example 5-2, we add a liveness probe to our kuard container, which runs an http request against the /healthy path on our container. example 5-2. kuard-pod-health.yaml apiversion: v1 kind: pod metadata:  name: kuard spec:  containers:  - image: gcr.io/kuar-demo/kuard-amd64:blue  name: kuard  livenessprobe:  httpget:  path: /healthy  port: 8080  initialdelayseconds: 5  timeoutseconds: 1  periodseconds: 10  failurethreshold: 3  ports:  - containerport: 8080  name: http  protocol: tcp 54 | chapter 5: pods the preceding pod manifest uses an httpget probe to perform an http get request against the /healthy endpoint on port 8080 of the kuard container. the probe sets an initialdelayseconds of 5, and thus will not be called until 5 seconds after all the containers in the pod are created. the probe must respond within the 1-second time‐ out, and the http status code must be equal to or greater than 200 and less than 400 to be considered successful. kubernetes will call the probe every 10 seconds. if more than three consecutive probes fail, the container will fail and restart. you can see this in action by looking at the kuard status page. create a pod using this manifest and then port-forward to that pod: $ kubectl apply -f kuard-pod-health.yaml $ kubectl port-forward kuard 8080:8080 point your browser to http://localhost:8080. click the “liveness probe” tab. you should see a table that lists all of the probes that this instance of kuard has received. if you click the “fail” link on that page, kuard will start to fail health checks. wait long enough and kubernetes will restart the container. at that point the display will reset and start over again. details of the restart can be found with kubectl describe pods kuard. the “events” section will have text similar to the following: killing container with id docker://2ac946...:pod \"kuarddefault(9ee84...)\" container \"kuard\" is unhealthy, it will be killed and re-created. while the default response to a failed liveness check is to restart the pod, the actual behavior is governed by the pod’s restartpolicy. there are three options for the restart policy: always (the default), onfailure (restart only on liveness failure or nonzero process exit code), or never. readiness probe of course, liveness isn’t the only kind of health check we want to perform. kubernetes makes a distinction between liveness and readiness. liveness determines if an applica‐ tion is running properly. containers that fail liveness checks are restarted. readiness describes when a container is ready to serve user requests. containers that fail readi‐ ness checks are removed from service load balancers. readiness probes are config‐ ured similarly to liveness probes. we explore kubernetes services in detail in chapter 7. combining the readiness and liveness probes helps ensure only healthy containers are running within the cluster. health checks | 55 types of health checks in addition to http checks, kubernetes also supports tcpsocket health checks that open a tcp socket; if the connection is successful, the probe succeeds. this style of probe is useful for non-http applications; for example, databases or other non– http-based apis. finally, kubernetes allows exec probes. these execute a script or program in the con‐ text of the container. following typical convention, if this script returns a zero exit code, the probe succeeds; otherwise, it fails. exec scripts are often useful for custom application validation logic that doesn’t fit neatly into an http call. resource management most people move into containers and orchestrators like kubernetes because of the radical improvements in image packaging and reliable deployment they provide. in addition to application-oriented primitives that simplify distributed system develop‐ ment, equally important is the ability to increase the overall utilization of the com‐ pute nodes that make up the cluster. the basic cost of operating a machine, either virtual or physical, is basically constant regardless of whether it is idle or fully loaded. consequently, ensuring that these machines are maximally active increases the effi‐ ciency of every dollar spent on infrastructure. generally speaking, we measure this efficiency with the utilization metric. utilization is defined as the amount of a resource actively being used divided by the amount of a resource that has been purchased. for example, if you purchase a one-core machine, and your application uses one-tenth of a core, then your utilization is 10%. with scheduling systems like kubernetes managing resource packing, you can drive your utilization to greater than 50%. to achieve this, you have to tell kubernetes about the resources your application requires, so that kubernetes can find the optimal packing of containers onto pur‐ chased machines. kubernetes allows users to specify two different resource metrics. resource requests specify the minimum amount of a resource required to run the application. resource limits specify the maximum amount of a resource that an application can consume. both of these resource definitions are described in greater detail in the following sections. resource requests: minimum required resources with kubernetes, a pod requests the resources required to run its containers. kuber‐ netes guarantees that these resources are available to the pod. the most commonly 56 | chapter 5: pods requested resources are cpu and memory, but kubernetes has support for other resource types as well, such as gpus and more. for example, to request that the kuard container lands on a machine with half a cpu free and gets 128 mb of memory allocated to it, we define the pod as shown in example 5-3. example 5-3. kuard-pod-resreq.yaml apiversion: v1 kind: pod metadata:  name: kuard spec:  containers:  - image: gcr.io/kuar-demo/kuard-amd64:blue  name: kuard  resources:  requests:  cpu: \"500m\"  memory: \"128mi\"  ports:  - containerport: 8080  name: http  protocol: tcp resources are requested per container, not per pod. the total resources requested by the pod is the sum of all resources reques‐ ted by all containers in the pod. the reason for this is that in many cases the different containers have very different cpu require‐ ments. for example, in the web server and data synchronizer pod, the web server is user-facing and likely needs a great deal of cpu, while the data synchronizer can make do with very little. request limit details requests are used when scheduling pods to nodes. the kubernetes scheduler will ensure that the sum of all requests of all pods on a node does not exceed the capacity of the node. therefore, a pod is guaranteed to have at least the requested resources when running on the node. importantly, “request” specifies a minimum. it does not specify a maximum cap on the resources a pod may use. to explore what this means, let’s look at an example. imagine that we have container whose code attempts to use all available cpu cores. suppose that we create a pod with this container that requests 0.5 cpu. kubernetes schedules this pod onto a machine with a total of 2 cpu cores. resource management | 57 as long as it is the only pod on the machine, it will consume all 2.0 of the available cores, despite only requesting 0.5 cpu. if a second pod with the same container and the same request of 0.5 cpu lands on the machine, then each pod will receive 1.0 cores. if a third identical pod is scheduled, each pod will receive 0.66 cores. finally, if a fourth identical pod is scheduled, each pod will receive the 0.5 core it requested, and the node will be at capacity. cpu requests are implemented using the cpu-shares functionality in the linux kernel. memory requests are handled similarly to cpu, but there is an important difference. if a container is over its memory request, the os can’t just remove memory from the process, because it’s been allocated. consequently, when the system runs out of memory, the kubelet terminates containers whose memory usage is greater than their requested memory. these containers are automatically restarted, but with less available memory on the machine for the container to consume. since resource requests guarantee resource availability to a pod, they are critical to ensuring that containers have sufficient resources in high-load situations. capping resource usage with limits in addition to setting the resources required by a pod, which establishes the mini‐ mum resources available to the pod, you can also set a maximum on a pod’s resource usage via resource limits. in our previous example we created a kuard pod that requested a minimum of 0.5 of a core and 128 mb of memory. in the pod manifest in example 5-4, we extend this configuration to add a limit of 1.0 cpu and 256 mb of memory. example 5-4. kuard-pod-reslim.yaml apiversion: v1 kind: pod metadata:  name: kuard spec:  containers:  - image: gcr.io/kuar-demo/kuard-amd64:blue  name: kuard  resources:  requests: 58 | chapter 5: pods  cpu: \"500m\"  memory: \"128mi\"  limits:  cpu: \"1000m\"  memory: \"256mi\"  ports:  - containerport: 8080  name: http  protocol: tcp when you establish limits on a container, the kernel is configured to ensure that con‐ sumption cannot exceed these limits. a container with a cpu limit of 0.5 cores will only ever get 0.5 cores, even if the cpu is otherwise idle. a container with a memory limit of 256 mb will not be allowed additional memory (e.g., malloc will fail) if its memory usage exceeds 256 mb. persisting data with volumes when a pod is deleted or a container restarts, any and all data in the container’s file‐ system is also deleted. this is often a good thing, since you don’t want to leave around cruft that happened to be written by your stateless web application. in other cases, having access to persistent disk storage is an important part of a healthy application. kubernetes models such persistent storage. using volumes with pods to add a volume to a pod manifest, there are two new stanzas to add to our configu‐ ration. the first is a new spec.volumes section. this array defines all of the volumes that may be accessed by containers in the pod manifest. it’s important to note that not all containers are required to mount all volumes defined in the pod. the second addi‐ tion is the volumemounts array in the container definition. this array defines the vol‐ umes that are mounted into a particular container, and the path where each volume should be mounted. note that two different containers in a pod can mount the same volume at different mount paths. the manifest in example 5-5 defines a single new volume named kuard-data, which the kuard container mounts to the /data path. example 5-5. kuard-pod-vol.yaml apiversion: v1 kind: pod metadata:  name: kuard spec:  volumes: persisting data with volumes | 59  - name: \"kuard-data\"  hostpath:  path: \"/var/lib/kuard\"  containers:  - image: gcr.io/kuar-demo/kuard-amd64:blue  name: kuard  volumemounts:  - mountpath: \"/data\"  name: \"kuard-data\"  ports:  - containerport: 8080  name: http  protocol: tcp different ways of using volumes with pods there are a variety of ways you can use data in your application. the following are a few, and the recommended patterns for kubernetes. communication/synchronization in the first example of a pod, we saw how two containers used a shared volume to serve a site while keeping it synchronized to a remote git location. to achieve this, the pod uses an emptydir volume. such a volume is scoped to the pod’s lifespan, but it can be shared between two containers, forming the basis for communication between our git sync and web serving containers. cache an application may use a volume that is valuable for performance, but not required for correct operation of the application. for example, perhaps the application keeps prerendered thumbnails of larger images. of course, they can be reconstructed from the original images, but that makes serving the thumbnails more expensive. you want such a cache to survive a container restart due to a health-check failure, and thus emptydir works well for the cache use case as well. persistent data sometimes you will use a volume for truly persistent data—data that is independent of the lifespan of a particular pod, and should move between nodes in the cluster if a node fails or a pod moves to a different machine for some reason. to achieve this, kubernetes supports a wide variety of remote network storage volumes, including widely supported protocols like nfs and iscsi as well as cloud provider network storage like amazon’s elastic block store, azure’s files and disk storage, as well as google’s persistent disk. 60 | chapter 5: pods mounting the host \\x80lesystem other applications don’t actually need a persistent volume, but they do need some access to the underlying host filesystem. for example, they may need access to the /dev filesystem in order to perform raw block-level access to a device on the sys‐ tem. for these cases, kubernetes supports the hostpath volume, which can mount arbitrary locations on the worker node into the container. the previous example uses the hostpath volume type. the volume created is /var/lib/ kuard on the host. persisting data using remote disks oftentimes, you want the data a pod is using to stay with the pod, even if it is restar‐ ted on a different host machine. to achieve this, you can mount a remote network storage volume into your pod. when using network-based storage, kubernetes automatically mounts and unmounts the appropriate storage whenever a pod using that volume is scheduled onto a partic‐ ular machine. there are numerous methods for mounting volumes over the network. kubernetes includes support for standard protocols such as nfs and iscsi as well as cloud pro‐ vider–based storage apis for the major cloud providers (both public and private). in many cases, the cloud providers will also create the disk for you if it doesn’t already exist. here is an example of using an nfs server: ... # rest of pod definition above here volumes:  - name: \"kuard-data\"  nfs:  server: my.nfs.server.local  path: \"/exports\" persistent volumes are a deep topic that has many different details: in particular, the manner in which persistent volumes, persistent volume claims, and dynamic volume provisioning work together. there is a more in-depth examination of the subject in chapter 15. putting it all together many applications are stateful, and as such we must preserve any data and ensure access to the underlying storage volume regardless of what machine the application runs on. as we saw earlier, this can be achieved using a persistent volume backed by network-attached storage. we also want to ensure that a healthy instance of the putting it all together | 61 application is running at all times, which means we want to make sure the container running kuard is ready before we expose it to clients. through a combination of persistent volumes, readiness and liveness probes, and resource restrictions, kubernetes provides everything needed to run stateful applica‐ tions reliably. example 5-6 pulls this all together into one manifest. example 5-6. kuard-pod-full.yaml apiversion: v1 kind: pod metadata:  name: kuard spec:  volumes:  - name: \"kuard-data\"  nfs:  server: my.nfs.server.local  path: \"/exports\"  containers:  - image: gcr.io/kuar-demo/kuard-amd64:blue  name: kuard  ports:  - containerport: 8080  name: http  protocol: tcp  resources:  requests:  cpu: \"500m\"  memory: \"128mi\"  limits:  cpu: \"1000m\"  memory: \"256mi\"  volumemounts:  - mountpath: \"/data\"  name: \"kuard-data\"  livenessprobe:  httpget:  path: /healthy  port: 8080  initialdelayseconds: 5  timeoutseconds: 1  periodseconds: 10  failurethreshold: 3  readinessprobe:  httpget:  path: /ready  port: 8080  initialdelayseconds: 30  timeoutseconds: 1 62 | chapter 5: pods  periodseconds: 10  failurethreshold: 3 summary pods represent the atomic unit of work in a kubernetes cluster. pods are comprised of one or more containers working together symbiotically. to create a pod, you write a pod manifest and submit it to the kubernetes api server by using the command-line tool or (less frequently) by making http and json calls to the server directly. once you’ve submitted the manifest to the api server, the kubernetes scheduler finds a machine where the pod can fit and schedules the pod to that machine. once sched‐ uled, the kubelet daemon on that machine is responsible for creating the containers that correspond to the pod, as well as performing any health checks defined in the pod manifest. once a pod is scheduled to a node, no rescheduling occurs if that node fails. addi‐ tionally, to create multiple replicas of the same pod you have to create and name them manually. in a later chapter we introduce the replicaset object and show how you can automate the creation of multiple identical pods and ensure that they are recreated in the event of a node machine failure. summary | 63  chapter 6 labels and annotations kubernetes was made to grow with you as your application scales in both size and complexity. with this in mind, labels and annotations were added as foundational concepts. labels and annotations let you work in sets of things that map to how you think about your application. you can organize, mark, and cross-index all of your resources to represent the groups that make the most sense for your application. labels are key/value pairs that can be attached to kubernetes objects such as pods and replicasets. they can be arbitrary, and are useful for attaching identifying informa‐ tion to kubernetes objects. labels provide the foundation for grouping objects. annotations, on the other hand, provide a storage mechanism that resembles labels: annotations are key/value pairs designed to hold nonidentifying information that can be leveraged by tools and libraries. labels labels provide identifying metadata for objects. these are fundamental qualities of the object that will be used for grouping, viewing, and operating. 65 the motivations for labels grew out of google’s experience in run‐ ning large and complex applications. there were a couple of les‐ sons that emerged from this experience. see the great site reliability book site reliability engineering by betsy beyer et al. (o’reilly) for some deeper background on how google approaches production systems. the first lesson is that production abhors a singleton. when deploying software, users will often start with a single instance. however, as the application matures, these singletons often multi‐ ply and become sets of objects. with this in mind, kubernetes uses labels to deal with sets of objects instead of single instances. the second lesson is that any hierarchy imposed by the system will fall short for many users. in addition, user groupings and hierar‐ chies are subject to change over time. for instance, a user may start out with the idea that all apps are made up of many services. how‐ ever, over time, a service may be shared across multiple apps. kubernetes labels are flexible enough to adapt to these situations and more. labels have simple syntax. they are key/value pairs, where both the key and value are represented by strings. label keys can be broken down into two parts: an optional prefix and a name, separated by a slash. the prefix, if specified, must be a dns sub‐ domain with a 253-character limit. the key name is required and must be shorter than 63 characters. names must also start and end with an alphanumeric character and permit the use of dashes (-), underscores (), and dots (.) between characters. label values are strings with a maximum length of 63 characters. the contents of the label values follow the same rules as for label keys. table 6-1 shows some valid label keys and values. table 6-1. label examples key value acme.com/app-version 1.0.0 appversion 1.0.0 app.version 1.0.0 kubernetes.io/cluster-service true when domain names are used in labels and annotations they are expected to be aligned to that particular entity in some way. for example, a project might define a canonical set of labels used to identify the various stages of application deployment (e.g., staging, canary, production). 66 | chapter 6: labels and annotations or a cloud provider might define provider-specific annotations that extend kuber‐ netes objects to activate features specific to their service. applying labels here we create a few deployments (a way to create an array of pods) with some inter‐ esting labels. we’ll take two apps (called alpaca and bandicoot) and have two envi‐ ronments for each. we will also have two different versions: 1. first, create the alpaca-prod deployment and set the ver, app, and env labels: $ kubectl run alpaca-prod \\\\  --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\  --replicas=2 \\\\  --labels=\"ver=1,app=alpaca,env=prod\" 2. next, create the alpaca-test deployment and set the ver, app, and env labels with the appropriate values: $ kubectl run alpaca-test \\\\  --image=gcr.io/kuar-demo/kuard-amd64:green \\\\  --replicas=1 \\\\  --labels=\"ver=2,app=alpaca,env=test\" 3. finally, create two deployments for bandicoot. here we name the environments prod and staging: $ kubectl run bandicoot-prod \\\\  --image=gcr.io/kuar-demo/kuard-amd64:green \\\\  --replicas=2 \\\\  --labels=\"ver=2,app=bandicoot,env=prod\" $ kubectl run bandicoot-staging \\\\  --image=gcr.io/kuar-demo/kuard-amd64:green \\\\  --replicas=1 \\\\  --labels=\"ver=2,app=bandicoot,env=staging\" at this point you should have four deployments—alpaca-prod, alpaca-test, bandicoot-prod, and bandicoot-staging: $ kubectl get deployments --show-labels name ... labels alpaca-prod ... app=alpaca,env=prod,ver=1 alpaca-test ... app=alpaca,env=test,ver=2 bandicoot-prod ... app=bandicoot,env=prod,ver=2 bandicoot-staging ... app=bandicoot,env=staging,ver=2 we can visualize this as a venn diagram based on the labels (figure 6-1). labels | 67 figure 6-1. visualization of labels applied to our deployments modifying labels labels can also be applied (or updated) on objects after they are created: $ kubectl label deployments alpaca-test \"canary=true\" there is a caveat to be aware of here. in this example, the kubectl label command will only change the label on the deployment itself; it won’t affect the objects (replicasets and pods) the deploy‐ ment creates. to change those, you’ll need to change the template embedded in the deployment (see chapter 10). you can also use the -l option to kubectl get to show a label value as a column: $ kubectl get deployments -l canary name desired current ... canary alpaca-prod 2 2 ... <none> alpaca-test 1 1 ... true bandicoot-prod 2 2 ... <none> bandicoot-staging 1 1 ... <none> you can remove a label by applying a dash suffix: $ kubectl label deployments alpaca-test \"canary-\" label selectors label selectors are used to filter kubernetes objects based on a set of labels. selectors use a simple boolean language. they are used both by end users (via tools like kubectl) and by different types of objects (such as how a replicaset relates to its pods). 68 | chapter 6: labels and annotations each deployment (via a replicaset) creates a set of pods using the labels specified in the template embedded in the deployment. this is configured by the kubectl run command. running the kubectl get pods command should return all the pods currently run‐ ning in the cluster. we should have a total of six kuard pods across our three environments: $ kubectl get pods --show-labels name ... labels alpaca-prod-3408831585-4nzfb ... app=alpaca,env=prod,ver=1,... alpaca-prod-3408831585-kga0a ... app=alpaca,env=prod,ver=1,... alpaca-test-1004512375-3r1m5 ... app=alpaca,env=test,ver=2,... bandicoot-prod-373860099-0t1gp ... app=bandicoot,env=prod,ver=2,... bandicoot-prod-373860099-k2wcf ... app=bandicoot,env=prod,ver=2,... bandicoot-staging-1839769971-3ndv ... app=bandicoot,env=staging,ver=2,... you may see a new label that we haven’t seen yet: pod-templatehash. this label is applied by the deployment so it can keep track of which pods were generated from which template versions. this allows the deployment to manage updates in a clean way, as will be covered in depth in chapter 10. if we only wanted to list pods that had the ver label set to 2, we could use the --selector flag: $ kubectl get pods --selector=\"ver=2\" name ready status restarts age alpaca-test-1004512375-3r1m5 1/1 running 0 3m bandicoot-prod-373860099-0t1gp 1/1 running 0 3m bandicoot-prod-373860099-k2wcf 1/1 running 0 3m bandicoot-staging-1839769971-3ndv5 1/1 running 0 3m if we specify two selectors separated by a comma, only the objects that satisfy both will be returned. this is a logical and operation: $ kubectl get pods --selector=\"app=bandicoot,ver=2\" name ready status restarts age bandicoot-prod-373860099-0t1gp 1/1 running 0 4m bandicoot-prod-373860099-k2wcf 1/1 running 0 4m bandicoot-staging-1839769971-3ndv5 1/1 running 0 4m we can also ask if a label is one of a set of values. here we ask for all pods where the app label is set to alpaca or bandicoot (which will be all six pods): $ kubectl get pods --selector=\"app in (alpaca,bandicoot)\" labels | 69 name ready status restarts age alpaca-prod-3408831585-4nzfb 1/1 running 0 6m alpaca-prod-3408831585-kga0a 1/1 running 0 6m alpaca-test-1004512375-3r1m5 1/1 running 0 6m bandicoot-prod-373860099-0t1gp 1/1 running 0 6m bandicoot-prod-373860099-k2wcf 1/1 running 0 6m bandicoot-staging-1839769971-3ndv5 1/1 running 0 6m finally, we can ask if a label is set at all. here we are asking for all of the deployments with the canary label set to anything: $ kubectl get deployments --selector=\"canary\" name desired current up-to-date available age alpaca-test 1 1 1 1 7m there are also “negative” versions of each of these, as shown in table 6-2. table 6-2. selector operators operator description key=value key is set to value key!=value key is not set to value key in (value1, value2) key is one of value1 or value2 key notin (value1, value2) key is not one of value1 or value2 key key is set !key key is not set for example, asking if a key, in this case canary, is not set can look like: $ kubectl get deployments --selector=\\'!canary\\' similarly, you can combine positive and negative selectors together as follows: $ kubectl get pods -l \\'ver=2,!canary\\' label selectors in api objects when a kubernetes object refers to a set of other kubernetes objects, a label selector is used. instead of a simple string as described in the previous section, we use a parsed structure. for historical reasons (kubernetes doesn’t break api compatibility!), there are two forms. most objects support a newer, more powerful set of selector operators. a selector of app=alpaca,ver in (1, 2) would be converted to this: selector:  matchlabels:  app: alpaca 70 | chapter 6: labels and annotations  matchexpressions:  - {key: ver, operator: in, values: } compact yaml syntax. this is an item in a list (matchexpressions) that is a map with three entries. the last entry (values) has a value that is a list with two items. all of the terms are evaluated as a logical and. the only way to represent the != operator is to convert it to a notin expression with a single value. the older form of specifying selectors (used in replicationcontrollers and serv‐ ices) only supports the = operator. this is a simple set of key/value pairs that must all match a target object to be selected. the selector app=alpaca,ver=1 would be represented like this: selector:  app: alpaca  ver: 1 labels in the kubernetes architecture in addition to enabling users to organize their infrastructure, labels play a critical role in linking various related kubernetes objects. kubernetes is a purposefully decoupled system. there is no hierarchy and all components operate independently. however, in many cases objects need to relate to one another, and these relationships are defined by labels and label selectors. for example, replicasets, which create and maintain multiple replicas of a pod, find the pods that they are managing via a selector. likewise, a service load balancer finds the pods it should bring traffic to via a selector query. when a pod is created, it can use a node selector to identify a particular set of nodes that it can be scheduled onto. when people want to restrict network traffic in their cluster, they use networkpolicy in conjunction with specific labels to identify pods that should or should not be allowed to communicate with each other. labels are a powerful and ubiquitous glue that holds a kubernetes application together. though your application will likely start out with a simple set of labels and queries, you should expect it to grow in size and sophistication with time. annotations annotations provide a place to store additional metadata for kubernetes objects with the sole purpose of assisting tools and libraries. they are a way for other programs driving kubernetes via an api to store some opaque data with an object. annotations can be used for the tool itself or to pass configuration information between external systems. annotations | 71 while labels are used to identify and group objects, annotations are used to provide extra information about where an object came from, how to use it, or policy around that object. there is overlap, and it is a matter of taste as to when to use an annotation or a label. when in doubt, add information to an object as an annotation and pro‐ mote it to a label if you find yourself wanting to use it in a selector. annotations are used to: • keep track of a “reason” for the latest update to an object. • communicate a specialized scheduling policy to a specialized scheduler. • extend data about the last tool to update the resource and how it was updated (used for detecting changes by other tools and doing a smart merge). • attach build, release, or image information that isn’t appropriate for labels (may include a git hash, timestamp, pr number, etc.). • enable the deployment object (chapter 10) to keep track of replicasets that it is managing for rollouts. • provide extra data to enhance the visual quality or usability of a ui. for example, objects could include a link to an icon (or a base64-encoded version of an icon). • prototype alpha functionality in kubernetes (instead of creating a first-class api field, the parameters for that functionality are encoded in an annotation). annotations are used in various places in kubernetes, with the primary use case being rolling deployments. during rolling deployments, annotations are used to track rollout status and provide the necessary information required to roll back a deploy‐ ment to a previous state. users should avoid using the kubernetes api server as a general-purpose database. annotations are good for small bits of data that are highly associated with a specific resource. if you want to store data in kubernetes but you don’t have an obvious object to associate it with, consider storing that data in some other, more appropriate database. de\\x80ning annotations annotation keys use the same format as label keys. however, because they are often used to communicate information between tools, the “namespace” part of the key is more important. example keys include deployment.kubernetes.io/revision or kubernetes.io/change-cause. the value component of an annotation is a free-form string field. while this allows maximum flexibility as users can store arbitrary data, because this is arbitrary text, there is no validation of any format. for example, it is not uncommon for a json document to be encoded as a string and stored in an annotation. it is important to 72 | chapter 6: labels and annotations note that the kubernetes server has no knowledge of the required format of annota‐ tion values. if annotations are used to pass or store data, there is no guarantee the data is valid. this can make tracking down errors more difficult. annotations are defined in the common metadata section in every kubernetes object: ... metadata:  annotations:  example.com/icon-url: \"https://example.com/icon.png\" ... annotations are very convenient and provide powerful loose cou‐ pling. however, they should be used judiciously to avoid an unty‐ ped mess of data. cleanup it is easy to clean up all of the deployments that we started in this chapter: $ kubectl delete deployments --all if you want to be more selective, you can use the --selector flag to choose which deployments to delete. summary labels are used to identify and optionally group objects in a kubernetes cluster. labels are also used in selector queries to provide flexible runtime grouping of objects such as pods. annotations provide object-scoped key/value storage of metadata that can be used by automation tooling and client libraries. annotations can also be used to hold configu‐ ration data for external tools such as third-party schedulers and monitoring tools. labels and annotations are vital to understanding how key components in a kuber‐ netes cluster work together to ensure the desired cluster state. using labels and anno‐ tations properly unlocks the true power of kubernetes’s flexibility and provides the starting point for building automation tools and deployment workflows. cleanup | 73  chapter 7 service discovery kubernetes is a very dynamic system. the system is involved in placing pods on nodes, making sure they are up and running, and rescheduling them as needed. there are ways to automatically change the number of pods based on load (such as horizontal pod autoscaling ). the apidriven nature of the system encourages others to create higher and higher levels of automation. while the dynamic nature of kubernetes makes it easy to run a lot of things, it creates problems when it comes to finding those things. most of the traditional network infrastructure wasn’t built for the level of dynamism that kubernetes presents. what is service discovery? the general name for this class of problems and solutions is service discovery. servicediscovery tools help solve the problem of finding which processes are listening at which addresses for which services. a good service-discovery system will enable users to resolve this information quickly and reliably. a good system is also low-latency; clients are updated soon after the information associated with a service changes. finally, a good service-discovery system can store a richer definition of what that ser‐ vice is. for example, perhaps there are multiple ports associated with the service. the domain name system (dns) is the traditional system of service discovery on the internet. dns is designed for relatively stable name resolution with wide and effi‐ cient caching. it is a great system for the internet but falls short in the dynamic world of kubernetes. unfortunately, many systems (for example, java, by default) look up a name in dns directly and never re-resolve. this can lead to clients caching stale mappings and talking to the wrong ip. even with short ttls and well-behaved clients, there is a 75 natural delay between when a name resolution changes and when the client notices. there are natural limits to the amount and type of information that can be returned in a typical dns query, too. things start to break past 20–30 a records for a single name. srv records solve some problems, but are often very hard to use. finally, the way that clients handle multiple ips in a dns record is usually to take the first ip address and rely on the dns server to randomize or round-robin the order of records. this is no substitute for more purpose-built load balancing. the service object real service discovery in kubernetes starts with a service object. a service object is a way to create a named label selector. as we will see, the service object does some other nice things for us, too. just as the kubectl run command is an easy way to create a kubernetes deployment, we can use kubectl expose to create a service. let’s create some deployments and services so we can see how they work: $ kubectl run alpaca-prod \\\\  --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\  --replicas=3 \\\\  --port=8080 \\\\  --labels=\"ver=1,app=alpaca,env=prod\" $ kubectl expose deployment alpaca-prod $ kubectl run bandicoot-prod \\\\  --image=gcr.io/kuar-demo/kuard-amd64:green \\\\  --replicas=2 \\\\  --port=8080 \\\\  --labels=\"ver=2,app=bandicoot,env=prod\" $ kubectl expose deployment bandicoot-prod $ kubectl get services -o wide name cluster-ip ... port(s) ... selector alpaca-prod 10.115.245.13 ... 8080/tcp ... app=alpaca,env=prod,ver=1 bandicoot-prod 10.115.242.3 ... 8080/tcp ... app=bandicoot,env=prod,ver=2 kubernetes 10.115.240.1 ... 443/tcp ... <none> after running these commands, we have three services. the ones we just created are alpaca-prod and bandicoot-prod. the kubernetes service is automatically created for you so that you can find and talk to the kubernetes api from within the app. if we look at the selector column, we see that the alpaca-prod service simply gives a name to a selector and specifies which ports to talk to for that service. the kubectl expose command will conveniently pull both the label selector and the relevant ports (8080, in this case) from the deployment definition. 76 | chapter 7: service discovery furthermore, that service is assigned a new type of virtual ip called a cluster ip. this is a special ip address the system will load-balance across all of the pods that are iden‐ tified by the selector. to interact with services, we are going to port forward to one of the alpaca pods. start and leave this command running in a terminal window. you can see the port forward working by accessing the alpaca pod at http://localhost:48858: $ alpacapod=$(kubectl get pods -l app=alpaca \\\\  -o jsonpath=\\'{.items.metadata.name}\\') $ kubectl port-forward $alpacapod 48858:8080 service dns because the cluster ip is virtual, it is stable, and it is appropriate to give it a dns address. all of the issues around clients caching dns results no longer apply. within a namespace, it is as easy as just using the service name to connect to one of the pods identified by a service. kubernetes provides a dns service exposed to pods running in the cluster. this kubernetes dns service was installed as a system component when the cluster was first created. the dns service is, itself, managed by kubernetes and is a great exam‐ ple of kubernetes building on kubernetes. the kubernetes dns service provides dns names for cluster ips. you can try this out by expanding the “dns query” section on the kuard server sta‐ tus page. query the a record for alpaca-prod. the output should look something like this: ;; opcode: query, status: noerror, id: 12071 ;; flags: qr aa rd ra; query: 1, answer: 1, authority: 0, additional: 0 ;; question section: ;alpaca-prod.default.svc.cluster.local. in a ;; answer section: alpaca-prod.default.svc.cluster.local. 30 in a 10.115.245.13 the full dns name here is alpaca-prod.default.svc.cluster.local.. let’s break this down: alpaca-prod the name of the service in question. default the namespace that this service is in. the service object | 77 svc recognizing that this is a service. this allows kubernetes to expose other types of things as dns in the future. cluster.local. the base domain name for the cluster. this is the default and what you will see for most clusters. administrators may change this to allow unique dns names across multiple clusters. when referring to a service in your own namespace you can just use the service name (alpaca-prod). you can also refer to a service in another namespace with alpacaprod.default. and, of course, you can use the fully qualified service name (alpacaprod.default.svc.cluster.local.). try each of these out in the “dns query” section of kuard. readiness checks often, when an application first starts up it isn’t ready to handle requests. there is usually some amount of initialization that can take anywhere from under a second to several minutes. one nice thing the service object does is track which of your pods are ready via a readiness check. let’s modify our deployment to add a readiness check that is attached to a pod, as we discussed in chapter 5: $ kubectl edit deployment/alpaca-prod this command will fetch the current version of the alpaca-prod deployment and bring it up in an editor. after you save and quit your editor, it’ll then write the object back to kubernetes. this is a quick way to edit an object without saving it to a yaml file. add the following section: spec:  ...  template:  ...  spec:  containers:  ...  name: alpaca-prod  readinessprobe:  httpget:  path: /ready  port: 8080  periodseconds: 2  initialdelayseconds: 0  failurethreshold: 3  successthreshold: 1 78 | chapter 7: service discovery this sets up the pods this deployment will create so that they will be checked for readiness via an http get to /ready on port 8080. this check is done every 2 sec‐ onds starting as soon as the pod comes up. if three successive checks fail, then the pod will be considered not ready. however, if only one check succeeds, the pod will again be considered ready. only ready pods are sent traffic. updating the deployment definition like this will delete and recreate the alpaca pods. as such, we need to restart our port-forward command from earlier: $ alpacapod=$(kubectl get pods -l app=alpaca \\\\  -o jsonpath=\\'{.items.metadata.name}\\') $ kubectl port-forward $alpacapod 48858:8080 point your browser to http://localhost:48858 and you should see the debug page for that instance of kuard. expand the “readiness probe” section. you should see this page update every time there is a new readiness check from the system, which should happen every 2 seconds. in another terminal window, start a watch command on the endpoints for the alpaca-prod service. endpoints are a lower-level way of finding what a service is sending traffic to and are covered later in this chapter. the --watch option here causes the kubectl command to hang around and output any updates. this is an easy way to see how a kubernetes object changes over time: $ kubectl get endpoints alpaca-prod --watch now go back to your browser and hit the “fail” link for the readiness check. you should see that the server is now returning 500s. after three of these, this server is removed from the list of endpoints for the service. hit the “succeed” link and notice that after a single readiness check the endpoint is added back. this readiness check is a way for an overloaded or sick server to signal to the system that it doesn’t want to receive traffic anymore. this is a great way to implement grace‐ ful shutdown. the server can signal that it no longer wants traffic, wait until existing connections are closed, and then cleanly exit. press ctrl-c to exit out of both the port-forward and watch commands in your terminals. looking beyond the cluster so far, everything we’ve covered in this chapter has been about exposing services inside of a cluster. oftentimes, the ips for pods are only reachable from within the cluster. at some point, we have to allow new traffic in! looking beyond the cluster | 79 the most portable way to do this is to use a feature called nodeports, which enhance a service even further. in addition to a cluster ip, the system picks a port (or the user can specify one), and every node in the cluster then forwards traffic to that port to the service. with this feature, if you can reach any node in the cluster you can contact a service. you use the nodeport without knowing where any of the pods for that service are running. this can be integrated with hardware or software load balancers to expose the service further. try this out by modifying the alpaca-prod service: $ kubectl edit service alpaca-prod change the spec.type field to nodeport. you can also do this when creating the ser‐ vice via kubectl expose by specifying --type=nodeport. the system will assign a new nodeport: $ kubectl describe service alpaca-prod name: alpaca-prod namespace: default labels: app=alpaca  env=prod  ver=1 annotations: <none> selector: app=alpaca,env=prod,ver=1 type: nodeport ip: 10.115.245.13 port: <unset> 8080/tcp nodeport: <unset> 32711/tcp endpoints: 10.112.1.66:8080,10.112.2.104:8080,10.112.2.105:8080 session affinity: none no events. here we see that the system assigned port 32711 to this service. now we can hit any of our cluster nodes on that port to access the service. if you are sitting on the same network, you can access it directly. if your cluster is in the cloud someplace, you can use ssh tunneling with something like this: $ ssh <node> -l 8080:localhost:32711 now if you point your browser to http://localhost:8080 you will be connected to that service. each request that you send to the service will be randomly directed to one of the pods that implements the service. reload the page a few times and you will see that you are randomly assigned to different pods. when you are done, exit out of the ssh session. 80 | chapter 7: service discovery cloud integration finally, if you have support from the cloud that you are running on (and your cluster is configured to take advantage of it), you can use the loadbalancer type. this builds on the nodeport type by additionally configuring the cloud to create a new load bal‐ ancer and direct it at nodes in your cluster. edit the alpaca-prod service again (kubectl edit service alpaca-prod) and change spec.type to loadbalancer. if you do a kubectl get services right away you’ll see that the external-ip col‐ umn for alpaca-prod now says <pending>. wait a bit and you should see a public address assigned by your cloud. you can look in the console for your cloud account and see the configuration work that kubernetes did for you: $ kubectl describe service alpaca-prod name: alpaca-prod namespace: default labels: app=alpaca  env=prod  ver=1 selector: app=alpaca,env=prod,ver=1 type: loadbalancer ip: 10.115.245.13 loadbalancer ingress: 104.196.248.204 port: <unset> 8080/tcp nodeport: <unset> 32711/tcp endpoints: 10.112.1.66:8080,10.112.2.104:8080,10.112.2.105:8080 session affinity: none events:  firstseen ... reason message  --------- ... ------ -------  3m ... type nodeport -> loadbalancer  3m ... creatingloadbalancer creating load balancer  2m ... createdloadbalancer created load balancer here we see that we have an address of 104.196.248.204 now assigned to the alpacaprod service. open up your browser and try! this example is from a cluster launched and managed on the goo‐ gle cloud platform via gke. however, the way a load balancer is configured is specific to a cloud. in addition, some clouds have dns-based load balancers (e.g., aws elb). in this case you’ll see a hostname here instead of an ip. also, depending on the cloud pro‐ vider, it may still take a little while for the load balancer to be fully operational. cloud integration | 81 creating a cloud-based load balancer can take some time. don’t be surprised if it takes a few minutes on most cloud providers. advanced details kubernetes is built to be an extensible system. as such, there are layers that allow for more advanced integrations. understanding the details of how a sophisticated con‐ cept like services is implemented may help you troubleshoot or create more advanced integrations. this section goes a bit below the surface. endpoints some applications (and the system itself) want to be able to use services without using a cluster ip. this is done with another type of object called an endpoints object. for every service object, kubernetes creates a buddy endpoints object that contains the ip addresses for that service: $ kubectl describe endpoints alpaca-prod name: alpaca-prod namespace: default labels: app=alpaca  env=prod  ver=1 subsets:  addresses: 10.112.1.54,10.112.2.84,10.112.2.85  notreadyaddresses: <none>  ports:  name port protocol  ---- ---- --------  <unset> 8080 tcp no events. to use a service, an advanced application can talk to the kubernetes api directly to look up endpoints and call them. the kubernetes api even has the capability to “watch” objects and be notified as soon as they change. in this way, a client can react immediately as soon as the ips associated with a service change. let’s demonstrate this. in a terminal window, start the following command and leave it running: $ kubectl get endpoints alpaca-prod --watch it will output the current state of the endpoint and then “hang”: name endpoints age alpaca-prod 10.112.1.54:8080,10.112.2.84:8080,10.112.2.85:8080 1m 82 | chapter 7: service discovery now open up another terminal window and delete and recreate the deployment back‐ ing alpaca-prod: $ kubectl delete deployment alpaca-prod $ kubectl run alpaca-prod \\\\  --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\  --replicas=3 \\\\  --port=8080 \\\\  --labels=\"ver=1,app=alpaca,env=prod\" if you look back at the output from the watched endpoint, you will see that as you deleted and re-created these pods, the output of the command reflected the most upto-date set of ip addresses associated with the service. your output will look some‐ thing like this: name endpoints age alpaca-prod 10.112.1.54:8080,10.112.2.84:8080,10.112.2.85:8080 1m alpaca-prod 10.112.1.54:8080,10.112.2.84:8080 1m alpaca-prod <none> 1m alpaca-prod 10.112.2.90:8080 1m alpaca-prod 10.112.1.57:8080,10.112.2.90:8080 1m alpaca-prod 10.112.0.28:8080,10.112.1.57:8080,10.112.2.90:8080 1m the endpoints object is great if you are writing new code that is built to run on kubernetes from the start. but most projects aren’t in this position! most existing sys‐ tems are built to work with regular old ip addresses that don’t change that often. manual service discovery kubernetes services are built on top of label selectors over pods. that means that you can use the kubernetes api to do rudimentary service discovery without using a ser‐ vice object at all! let’s demonstrate. with kubectl (and via the api) we can easily see what ips are assigned to each pod in our example deployments: $ kubectl get pods -o wide --show-labels name ... ip ... labels alpaca-prod-12334-87f8h ... 10.112.1.54 ... app=alpaca,env=prod,ver=1 alpaca-prod-12334-jssmh ... 10.112.2.84 ... app=alpaca,env=prod,ver=1 alpaca-prod-12334-tjp56 ... 10.112.2.85 ... app=alpaca,env=prod,ver=1 bandicoot-prod-5678-sbxzl ... 10.112.1.55 ... app=bandicoot,env=prod,ver=2 bandicoot-prod-5678-x0dh8 ... 10.112.2.86 ... app=bandicoot,env=prod,ver=2 this is great, but what if you have a ton of pods? you’ll probably want to filter this based on the labels applied as part of the deployment. let’s do that for just the alpaca app: advanced details | 83 $ kubectl get pods -o wide --selector=app=alpaca,env=prod name ... ip ... alpaca-prod-3408831585-bpzdz ... 10.112.1.54 ... alpaca-prod-3408831585-kncwt ... 10.112.2.84 ... alpaca-prod-3408831585-l9fsq ... 10.112.2.85 ... at this point you have the basics of service discovery! you can always use labels to identify the set of pods you are interested in, get all of the pods for those labels, and dig out the ip address. but keeping the correct set of labels to use in sync can be tricky. this is why the service object was created. kube-proxy and cluster ips cluster ips are stable virtual ips that load-balance traffic across all of the endpoints in a service. this magic is performed by a component running on every node in the cluster called the kube-proxy (figure 7-1). figure 7-1. configuring and using a cluster ip in figure 7-1, the kube-proxy watches for new services in the cluster via the api server. it then programs a set of iptables rules in the kernel of that host to rewrite the destinations of packets so they are directed at one of the endpoints for that ser‐ vice. if the set of endpoints for a service changes (due to pods coming and going or due to a failed readiness check), the set of iptables rules is rewritten. the cluster ip itself is usually assigned by the api server as the service is created. however, when creating the service, the user can specify a specific cluster ip. once set, the cluster ip cannot be modified without deleting and recreating the service object. 84 | chapter 7: service discovery the kubernetes service address range is configured using the --service-cluster-ip-range flag on the kube-apiserver binary. the service address range should not overlap with the ip subnets and ranges assigned to each docker bridge or kubernetes node. in addition, any explicit cluster ip requested must come from that range and not already be in use. cluster ip environment variables while most users should be using the dns services to find cluster ips, there are some older mechanisms that may still be in use. one of these is injecting a set of environ‐ ment variables into pods as they start up. to see this in action, let’s look at the console for the bandicoot instance of kuard. enter the following commands in your terminal: $ bandicootpod=$(kubectl get pods -l app=bandicoot \\\\  -o jsonpath=\\'{.items.metadata.name}\\') $ kubectl port-forward $bandicootpod 48858:8080 now point your browser to http://localhost:48858 to see the status page for this server. expand the “server env” section and note the set of environment variables for the alpaca service. the status page should show a table similar to table 7-1. table 7-1. service environment variables key value alpacaprodport tcp://10.115.245.13:8080 alpacaprodport8080tcp tcp://10.115.245.13:8080 alpacaprodport8080tcpaddr 10.115.245.13 alpacaprodport8080tcpport 8080 alpacaprodport8080tcpproto tcp alpacaprodservicehost 10.115.245.13 alpacaprodserviceport 8080 the two main environment variables to use are alpacaprodservicehost and alpacaprodserviceport. the other environment variables are created to be com‐ patible with (now deprecated) docker link variables. a problem with the environment variable approach is that it requires resources to be created in a specific order. the services must be created before the pods that reference them. this can introduce quite a bit of complexity when deploying a set of services that make up a larger application. in addition, using just environment variables seems strange to many users. for this reason, dns is probably a better option. advanced details | 85 connecting with other environments while it is great to have service discovery within your own cluster, many real-world applications actually require that you integrate more cloud-native applications deployed in kubernetes with applications deployed to more legacy environments. additionally, you may need to integrate a kubernetes cluster in the cloud with infra‐ structure that has been deployed on-premise. this is an area of kubernetes that is still undergoing a fair amount of exploration and development of solutions. when you are connecting kubernetes to legacy resources outside of the cluster, you can use selector-less services to declare a kubernetes ser‐ vice with a manually assigned ip address that is outside of the cluster. that way, kubernetes service discovery via dns works as expected, but the network traffic itself flows to an external resource. connecting external resources to kubernetes services is somewhat trickier. if your cloud provider supports it, the easiest thing to do is to create an “internal” load bal‐ ancer that lives in your virtual private network and can deliver traffic from a fixed ip address into the cluster. you can then use traditional dns to make this ip address available to the external resource. another option is to run the full kube-proxy on an external resource and program that machine to use the dns server in the kubernetes cluster. such a setup is significantly more difficult to get right and should really only be used in on-premise environments. there are also a variety of open source projects (for example, hashicorp’s consul) that can be used to manage connectivity between in-cluster and out-of-cluster resources. cleanup run the following command to clean up all of the objects created in this chapter: $ kubectl delete services,deployments -l app summary kubernetes is a dynamic system that challenges traditional methods of naming and connecting services over the network. the service object provides a flexible and pow‐ erful way to expose services both within the cluster and beyond. with the techniques covered here you can connect services to each other and expose them outside the cluster. 86 | chapter 7: service discovery while using the dynamic service discovery mechanisms in kubernetes introduces some new concepts and may, at first, seem complex, understanding and adapting these techniques is key to unlocking the power of kubernetes. once your application can dynamically find services and react to the dynamic placement of those applica‐ tions, you are free to stop worrying about where things are running and when they move. it is a critical piece of the puzzle to start to think about services in a logical way and let kubernetes take care of the details of container placement. summary | 87  1 the open systems interconnection (osi) model is a standard way to describe how different networking lay‐ ers build on each other. tcp and udp are considered to be layer 4, while http is layer 7. chapter 8 http load balancing with ingress a critical part of any application is getting network traffic to and from that applica‐ tion. as described in chapter 7, kubernetes has a set of capabilities to enable services to be exposed outside of the cluster. for many users and simple use cases these capa‐ bilities are sufficient. but the service object operates at layer 4 (according to the osi model1 ). this means that it only forwards tcp and udp connections and doesn’t look inside of those con‐ nections. because of this, hosting many applications on a cluster uses many different exposed services. in the case where these services are type: nodeport, you’ll have to have clients connect to a unique port per service. in the case where these services are type: loadbalancer, you’ll be allocating (often expensive or scarce) cloud resources for each service. but for http (layer 7)-based services, we can do better. when solving a similar problem in non-kubernetes situations, users often turn to the idea of “virtual hosting.” this is a mechanism to host many http sites on a single ip address. typically, the user uses a load balancer or reverse proxy to accept incoming connections on http (80) and https (443) ports. that program then parses the http connection and, based on the host header and the url path that is requested, proxies the http call to some other program. in this way, that load balancer or reverse proxy plays “traffic cop” for decoding and directing incoming connections to the right “upstream” server. kubernetes calls its http-based load-balancing system ingress. ingress is a kubernetes-native way to implement the “virtual hosting” pattern we just discussed. one of the more complex aspects of the pattern is that the user has to manage the 89 load balancer configuration file. in a dynamic environment and as the set of virtual hosts expands, this can be very complex. the kubernetes ingress system works to simplify this by (a) standardizing that configuration, (b) moving it to a standard kubernetes object, and (c) merging multiple ingress objects into a single config for the load balancer. the typical software base implementation looks something like what is depicted in figure 8-1. the ingress controller is a software system exposed outside the cluster using a service of type: loadbalancer. it then proxies requests to “upstream” servers. the configuration for how it does this is the result of reading and monitoring ingress objects. figure 8-1. the typical software ingress controller configuration ingress spec versus ingress controllers while conceptually simple, at an implementation level ingress is very different from pretty much every other regular resource object in kubernetes. specifically, it is split into a common resource specification and a controller implementation. there is no “standard” ingress controller that is built into kubernetes, so the user must install one of many optional implementations. users can create and modify ingress objects just like every other object. but, by default, there is no code running to actually act on those objects. it is up to the users (or the distribution they are using) to install and manage an outside controller. in this way, the controller is pluggable. there are multiple reasons that ingress ended up like this. first of all, there is no one single http load balancer that can universally be used. in addition to many software load balancers (both open source and proprietary), there are also load-balancing capabilities provided by cloud providers (e.g., elb on aws), and hardware-based load balancers. the second reason is that the ingress object was added to kubernetes before any of the common extensibility capabilities were added (see chapter 16). as ingress progresses, it is likely that it will evolve to use these mechanisms. 90 | chapter 8: http load balancing with ingress 2 heptio was recently acquired by vmware, so there is a chance that this url could change. however, github will forward to the new destination. installing contour while there are many available ingress controllers, for the examples here we use an ingress controller called contour. this is a controller built to configure the open source (and cncf project) load balancer called envoy. envoy is built to be dynami‐ cally configured via an api. the contour ingress controller takes care of translating the ingress objects into something that envoy can understand. the contour project is hosted at https://github.com/heptio/contour. it was created by heptio2  in collaboration with real-world custom‐ ers and is used in production settings. you can install contour with a simple one-line invocation: $ kubectl apply -f https://j.hept.io/contour-deployment-rbac note that this requires execution by a user who has cluster-admin permissions. this one line works for most configurations. it creates a namespace called heptiocontour. inside of that namespace it creates a deployment (with two replicas) and an external-facing service of type: loadbalancer. in addition, it sets up the correct per‐ missions via a service account and installs a customresourcedefinition (see chap‐ ter 16) for some extended capabilities discussed in “the future of ingress” on page 100. because it is a global install, you need to ensure that you have wide admin permis‐ sions on the cluster you are installing into. after you install it, you can fetch the external address of contour via: $ kubectl get -n heptio-contour service contour -o wide name cluster-ip external-ip port(s) ... contour 10.106.53.14 a477...amazonaws.com 80:30274/tcp ... look at the external-ip column. this can be either an ip address (for gcp and azure) or a hostname (for aws). other clouds and environments may differ. if your kubernetes cluster doesn’t support services of type: loadbalancer, you’ll have to change the yaml for installing contour to simply use type: nodeport and route traffic to machines on the cluster via a mechanism that works in your configuration. if you are using minikube, you probably won’t have anything listed for external-ip. to fix this, you need to open a separate terminal window and run minikube tunnel. installing contour | 91 this configures networking routes such that you have unique ip addresses assigned to every service of type: loadbalancer. con\\x80guring dns to make ingress work well, you need to configure dns entries to the external address for your load balancer. you can map multiple hostnames to a single external endpoint and the ingress controller will play traffic cop and direct incoming requests to the appropriate upstream service based on that hostname. for this chapter, we assume that you have a domain called example.com. you need to configure two dns entries: alpaca.example.com and bandicoot.example.com. if you have an ip address for your external load balancer, you’ll want to create a records. if you have a hostname, you’ll want to configure cname records. con\\x80guring a local hosts file if you don’t have a domain or if you are using a local solution such as minikube, you can set up a local configuration by editing your /etc/hosts file to add an ip address. you need admin/root privileges on your workstation. the location of the file may dif‐ fer on your platform, and making it take effect may require extra steps. for example, on windows the file is usually at c:\\\\windows\\\\system32\\\\drivers\\\\etc\\\\hosts, and for recent versions of macos you need to run sudo killall -hup mdnsresponder after changing the file. edit the file to add a line like the following: <ip-address> alpaca.example.com bandicoot.example.com for <ip-address>, fill in the external ip address for contour. if all you have is a host‐ name (like from aws), you can get an ip address (that may change in the future) by executing host -t a <address>. don’t forget to undo these changes when you are done! using ingress now that we have an ingress controller configured, let’s put it through its paces. first we’ll create a few upstream (also sometimes referred to as “backend”) services to play with by executing the following commands: $ kubectl run be-default \\\\  --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\  --replicas=3 \\\\  --port=8080 $ kubectl expose deployment be-default $ kubectl run alpaca \\\\ 92 | chapter 8: http load balancing with ingress  --image=gcr.io/kuar-demo/kuard-amd64:green \\\\  --replicas=3 \\\\  --port=8080 $ kubectl expose deployment alpaca $ kubectl run bandicoot \\\\  --image=gcr.io/kuar-demo/kuard-amd64:purple \\\\  --replicas=3 \\\\  --port=8080 $ kubectl expose deployment bandicoot $ kubectl get services -o wide name cluster-ip ... port(s) ... selector alpaca-prod 10.115.245.13 ... 8080/tcp ... run=alpaca bandicoot-prod 10.115.242.3 ... 8080/tcp ... run=bandicoot be-default 10.115.246.6 ... 8080/tcp ... run=be-default kubernetes 10.115.240.1 ... 443/tcp ... <none> simplest usage the simplest way to use ingress is to have it just blindly pass everything that it sees through to an upstream service. there is limited support for imperative commands to work with ingress in kubectl, so we’ll start with a yaml file (see example 8-1). example 8-1. simple-ingress.yaml apiversion: extensions/v1beta1 kind: ingress metadata:  name: simple-ingress spec:  backend:  servicename: alpaca  serviceport: 8080 create this ingress with kubectl apply: $ kubectl apply -f simple-ingress.yaml ingress.extensions/simple-ingress created you can verify that it was set up correctly using kubectl get and kubectl describe: $ kubectl get ingress name hosts address ports age simple-ingress * 80 13m $ kubectl describe ingress simple-ingress name: simple-ingress namespace: default address: default backend: be-default:8080 (172.17.0.6:8080,172.17.0.7:8080,172.17.0.8:8080) using ingress | 93 rules:  host path backends  ---- ---- --------  * * be-default:8080 (172.17.0.6:8080,172.17.0.7:8080,172.17.0.8:8080) annotations:  ... events: <none> this sets things up so that any http request that hits the ingress controller is for‐ warded on to the alpaca service. you can now access the alpaca instance of kuard on any of the raw ips/cnames of the service; in this case, either alpaca.example.com or bandicoot.example.com. this doesn’t, at this point, add much value above a simple service of type: loadba lancer. we experiment with more complex configurations in the following sections. using hostnames things start to get interesting when we start to direct traffic based on properties of the request. the most common example of this is to have the ingress system look at the http host header (which is set to the dns domain in the original url) and direct traffic based on that header. let’s add another ingress object for directing traffic to the alpaca service for any traffic directed to alpaca.example.com (see example 8-2). example 8-2. host-ingress.yaml apiversion: extensions/v1beta1 kind: ingress metadata:  name: host-ingress spec:  rules:  - host: alpaca.example.com  http:  paths:  - backend:  servicename: alpaca  serviceport: 8080 94 | chapter 8: http load balancing with ingress create this ingress with kubectl apply: $ kubectl apply -f host-ingress.yaml ingress.extensions/host-ingress created we can verify that things are set up correctly as follows: $ kubectl get ingress name hosts address ports age host-ingress alpaca.example.com 80 54s simple-ingress * 80 13m $ kubectl describe ingress host-ingress name: host-ingress namespace: default address: default backend: default-http-backend:80 (<none>) rules:  host path backends  ---- ---- --------  alpaca.example.com  / alpaca:8080 (<none>) annotations:  ... events: <none> there are a couple of things that are a bit confusing here. first, there is a reference to the default-http-backend. this is a convention that only some ingress controllers use to handle requests that aren’t handled in any other way. these controllers send those requests to a service called default-http-backend in the kube-system name‐ space. this convention is surfaced client-side in kubectl. next, there are no endpoints listed for the alpaca backend service. this is a bug in kubectl that is fixed in kubernetes v1.14. regardless, you should now be able to address the alpaca service via http:// alpaca.example.com. if instead you reach the service endpoint via other methods, you should get the default service. using paths the next interesting scenario is to direct traffic based on not just the hostname, but also the path in the http request. we can do this easily by specifying a path in the paths entry (see example 8-3). in this example we direct everything coming into http://bandicoot.example.com to the bandicoot service, but we also send http://bandi‐ coot.example.com/a to the alpaca service. this type of scenario can be used to host multiple services on different paths of a single domain. using ingress | 95 example 8-3. path-ingress.yaml apiversion: extensions/v1beta1 kind: ingress metadata:  name: path-ingress spec:  rules:  - host: bandicoot.example.com  http:  paths:  - path: \"/\"  backend:  servicename: bandicoot  serviceport: 8080  - path: \"/a/\"  backend:  servicename: alpaca  serviceport: 8080 when there are multiple paths on the same host listed in the ingress system, the longest prefix matches. so, in this example, traffic starting with /a/ is forwarded to the alpaca service, while all other traffic (starting with /) is directed to the bandicoot service. as requests get proxied to the upstream service, the path remains unmodified. that means a request to bandicoot.example.com/a/ shows up to the upstream server that is configured for that request hostname and path. the upstream service needs to be ready to serve traffic on that subpath. in this case, kuard has special code for testing, where it responds on the root path (/) along with a predefined set of subpaths (/ a/, /b/, and /c/). cleaning up to clean up, execute the following: $ kubectl delete ingress host-ingress path-ingress simple-ingress $ kubectl delete service alpaca bandicoot be-default $ kubectl delete deployment alpaca bandicoot be-default advanced ingress topics and gotchas there are some other fancy features that are supported by ingress. the level of sup‐ port for these features differs based on the ingress controller implementation, and two controllers may implement a feature in slightly different ways. many of the extended features are exposed via annotations on the ingress object. be careful, as these annotations can be hard to validate and are easy to get wrong. many 96 | chapter 8: http load balancing with ingress of these annotations apply to the entire ingress object and so can be more general than you might like. to scope the annotations down you can always split a single ingress object into multiple ingress objects. the ingress controller should read them and merge them together. running multiple ingress controllers oftentimes, you may want to run multiple ingress controllers on a single cluster. in that case, you specify which ingress object is meant for which ingress controller using the kubernetes.io/ingress.class annotation. the value should be a string that specifies which ingress controller should look at this object. the ingress controllers themselves, then, should be configured with that same string and should only respect those ingress objects with the correct annotation. if the kubernetes.io/ingress.class annotation is missing, behavior is undefined. it is likely that multiple controllers will fight to satisfy the ingress and write the status field of the ingress objects. multiple ingress objects if you specify multiple ingress objects, the ingress controllers should read them all and try to merge them into a coherent configuration. however, if you specify dupli‐ cate and conflicting configurations, the behavior is undefined. it is likely that differ‐ ent ingress controllers will behave differently. even a single implementation may do different things depending on nonobvious factors. ingress and namespaces ingress interacts with namespaces in some nonobvious ways. first, due to an abundance of security caution, an ingress object can only refer to an upstream service in the same namespace. this means that you can’t use an ingress object to point a subpath to a service in another namespace. however, multiple ingress objects in different namespaces can specify subpaths for the same host. these ingress objects are then merged together to come up with the final config for the ingress controller. this cross-namespace behavior means that it is necessary that ingress be coordinated globally across the cluster. if not coordinated carefully, an ingress object in one namespace could cause problems (and undefined behavior) in other namespaces. typically there are no restrictions built into the ingress controller around what name‐ spaces are allowed to specify what hostnames and paths. advanced users may try to enforce a policy for this using a custom admission controller. there are also advanced ingress topics and gotchas | 97 evolutions of ingress described in “the future of ingress” on page 100 that address this problem. path rewriting some ingress controller implementations support, optionally, doing path rewriting. this can be used to modify the path in the http request as it gets proxied. this is usually specified by an annotation on the ingress object and applies to all requests that are specified by that object. for example, if we were using the nginx ingress controller, we could specify an annotation of nginx.ingress.kubernetes.io/ rewrite-target: /. this can sometimes make upstream services work on a subpath even if they weren’t built to do so. there are multiple implementations that not only implement path rewriting, but also support regular expressions when specifying the path. for example, the nginx con‐ troller allows regular expressions to capture parts of the path and then use that cap‐ tured content when doing rewriting. how this is done (and what variant of regular expressions is used) is implementation-specific. path rewriting isn’t a silver bullet, though, and can often lead to bugs. many web applications assume that they can link within themselves using absolute paths. in that case, the app in question may be hosted on /subpath but have requests show up to it on /. it may then send a user to /app-path. there is then the question of whether that is an “internal” link for the app (in which case it should instead be /subpath/apppath) or a link to some other app. for this reason, it is probably best to avoid sub‐ paths if you can help it for any complicated applications. serving tls when serving websites, it is becoming increasingly necessary to do so securely using tls and https. ingress supports this (as do most ingress controllers). first, users need to specify a secret with their tls certificate and keys—something like what is outlined in example 8-4. you can also create a secret imperatively with kubectl create secret tls <secret-name> --cert <certificate-pem-file> -- key <private-key-pem-file>. example 8-4. tls-secret.yaml apiversion: v1 kind: secret metadata:  creationtimestamp: null  name: tls-secret-name type: kubernetes.io/tls data: 98 | chapter 8: http load balancing with ingress  tls.crt: <base64 encoded certificate>  tls.key: <base64 encoded private key> once you have the certificate uploaded, you can reference it in an ingress object. this specifies a list of certificates along with the hostnames that those certificates should be used for (see example 8-5). again, if multiple ingress objects specify certificates for the same hostname, the behavior is undefined. example 8-5. tls-ingress.yaml apiversion: extensions/v1beta1 kind: ingress metadata:  name: tls-ingress spec:  tls:  - hosts:  - alpaca.example.com  secretname: tls-secret-name  rules:  - host: alpaca.example.com  http:  paths:  - backend:  servicename: alpaca  serviceport: 8080 uploading and managing tls secrets can be difficult. in addition, certificates can often come at a significant cost. to help solve this problem, there is a non-profit called “let’s encrypt” running a free certificate authority that is api-driven. since it is api-driven, it is possible to set up a kubernetes cluster that automatically fetches and installs tls certificates for you. it can be tricky to set up, but when working it’s very simple to use. the missing piece is an open source project called cert-manager initiated and supported by jetstack, a uk startup. visit its github page for instruc‐ tions on installing and using cert-manager. alternate ingress implementations there are many different implementations of ingress controllers, each building on the base ingress object with unique features. it is a vibrant ecosystem. first, each cloud provider has an ingress implementation that exposes the specific cloud-based l7 load balancer for that cloud. instead of configuring a software load balancer running in a pod, these controllers take ingress objects and use them to con‐ figure, via an api, the cloud-based load balancers. this reduces the load on the clus‐ ter and management burden for the operators, but can often come at a cost. alternate ingress implementations | 99 the most popular generic ingress controller is probably the open source nginx ingress controller. be aware that there is also a commercial controller based on the proprietary nginx plus. the open source controller essentially reads ingress objects and merges them into an nginx configuration file. it then signals to the nginx process to restart with the new configuration (while responsibly serving existing inflight connections). the open nginx controller has an enormous number of fea‐ tures and options exposed via annotations. ambassador and gloo are two other envoy-based ingress controllers that are focused on being api gateways. traefik is a reverse proxy implemented in go that also can function as an ingress controller. it has a set of features and dashboards that are very developer-friendly. this just scratches the surface. the ingress ecosystem is very active and there are many new projects and commercial offerings that build on the humble ingress object in unique ways. the future of ingress as you have seen, the ingress object provides a very useful abstraction for configur‐ ing l7 load balancers—but it hasn’t scaled to all the features that users want and vari‐ ous implementations are looking to offer. many of the features in ingress are underdefined. implementations can surface these features in different ways, reducing the portability of configurations between imple‐ mentations. another problem is that it is easy to misconfigure ingress. the way that multiple objects compose together opens the door for conflicts that are resolved differently by different implementations. in addition, the way that these are merged across name‐ spaces breaks the idea of namespace isolation. ingress was also created before the idea of a service mesh (exemplified by projects such as istio and linkerd) was well known. the intersection of ingress and service meshes is still being defined. there are a lot of great ideas floating around the community. for example, istio implements the idea of a gateway that overlaps with ingress in some ways. contour introduces a new type called ingressroute that is a better-defined and more explicit version of ingress inspired by other network protocols like dns. defining the next step for ingress is a hard problem. it involves creating a common language that applies to most load balancers, while at the same time leaving room for innovation of new features and allowing different implementations to specialize in other directions. this is an active area of innovation in the kubernetes community through the net‐ work special interest group. 100 | chapter 8: http load balancing with ingress summary ingress is a unique system in kubernetes. it is simply a schema, and the implementa‐ tions of a controller for that schema must be installed and managed separately. but it is also a critical system for exposing services to users in a practical and cost-efficient way. as kubernetes continues to mature, expect to see ingress become more and more relevant. summary | 101  chapter 9 replicasets previously, we covered how to run individual containers as pods. but these pods are essentially one-off singletons. more often than not, you want multiple replicas of a container running at a particular time. there are a variety of reasons for this type of replication: redundancy multiple running instances mean failure can be tolerated. scale multiple running instances mean that more requests can be handled. sharding different replicas can handle different parts of a computation in parallel. of course, you could manually create multiple copies of a pod using multiple differ‐ ent (though largely similar) pod manifests, but doing so is both tedious and errorprone. logically, a user managing a replicated set of pods considers them as a single entity to be defined and managed. this is precisely what a replicaset is. a replicaset acts as a cluster-wide pod manager, ensuring that the right types and number of pods are running at all times. because replicasets make it easy to create and manage replicated sets of pods, they are the building blocks used to describe common application deployment patterns and provide the underpinnings of self-healing for our applications at the infrastruc‐ ture level. pods managed by replicasets are automatically rescheduled under certain failure conditions, such as node failures and network partitions. the easiest way to think of a replicaset is that it combines a cookie cutter and a desired number of cookies into a single api object. when we define a replicaset, we define a specification for the pods we want to create (the “cookie cutter”), and a 103 desired number of replicas. additionally, we need to define a way of finding pods that the replicaset should control. the actual act of managing the replicated pods is an example of a reconciliation loop. such loops are fundamental to most of the design and implementation of kubernetes. the decision to embed the definition of a pod inside a replicaset (and a deployment, and a job, and…) is one of the more interesting ones in kubernetes. in retrospect, it probably would have been a better decision to use a reference to the podtemplate object rather than embedding it directly. reconciliation loops the central concept behind a reconciliation loop is the notion of desired state versus observed or current state. desired state is the state you want. with a replicaset, it is the desired number of replicas and the definition of the pod to replicate. for example, “the desired state is that there are three replicas of a pod running the kuard server.” in contrast, the current state is the currently observed state of the system. for exam‐ ple, “there are only two kuard pods currently running.” the reconciliation loop is constantly running, observing the current state of the world and taking action to try to make the observed state match the desired state. for instance, with the previous examples, the reconciliation loop would create a new kuard pod in an effort to make the observed state match the desired state of three replicas. there are many benefits to the reconciliation loop approach to managing state. it is an inherently goal-driven, self-healing system, yet it can often be easily expressed in a few lines of code. as a concrete example of this, note that the reconciliation loop for replicasets is a single loop, yet it handles user actions to scale up or scale down the replicaset as well as node failures or nodes rejoining the cluster after being absent. we’ll see numerous examples of reconciliation loops in action throughout the rest of the book. relating pods and replicasets one of the key themes that runs through kubernetes is decoupling. in particular, it’s important that all of the core concepts of kubernetes are modular with respect to each other and that they are swappable and replaceable with other components. in this spirit, the relationship between replicasets and pods is loosely coupled. though replicasets create and manage pods, they do not own the pods they create. replica‐ 104 | chapter 9: replicasets sets use label queries to identify the set of pods they should be managing. they then use the exact same pod api that you used directly in chapter 5 to create the pods that they are managing. this notion of “coming in the front door” is another central design concept in kubernetes. in a similar decoupling, replicasets that create multi‐ ple pods and the services that load-balance to those pods are also totally separate, decoupled api objects. in addition to supporting modularity, the decoupling of pods and replicasets enables several important behaviors, discussed in the following sections. adopting existing containers despite the value placed on declarative configuration of software, there are times when it is easier to build something up imperatively. in particular, early on you may be simply deploying a single pod with a container image without a replicaset manag‐ ing it. but at some point you may want to expand your singleton container into a replicated service and create and manage an array of similar containers. you may have even defined a load balancer that is serving traffic to that single pod. if replica‐ sets owned the pods they created, then the only way to start replicating your pod would be to delete it and then relaunch it via a replicaset. this might be disruptive, as there would be a moment in time when there would be no copies of your container running. however, because replicasets are decoupled from the pods they manage, you can simply create a replicaset that will “adopt” the existing pod, and scale out additional copies of those containers. in this way, you can seamlessly move from a single imperative pod to a replicated set of pods managed by a replicaset. quarantining containers oftentimes, when a server misbehaves, pod-level health checks will automatically restart that pod. but if your health checks are incomplete, a pod can be misbehaving but still be part of the replicated set. in these situations, while it would work to simply kill the pod, that would leave your developers with only logs to debug the problem. instead, you can modify the set of labels on the sick pod. doing so will disassociate it from the replicaset (and service) so that you can debug the pod. the replicaset con‐ troller will notice that a pod is missing and create a new copy, but because the pod is still running it is available to developers for interactive debugging, which is signifi‐ cantly more valuable than debugging from logs. designing with replicasets replicasets are designed to represent a single, scalable microservice inside your architecture. the key characteristic of replicasets is that every pod that is created by the replicaset controller is entirely homogeneous. typically, these pods are then fronted by a kubernetes service load balancer, which spreads traffic across the pods designing with replicasets | 105 that make up the service. generally speaking, replicasets are designed for stateless (or nearly stateless) services. the elements created by the replicaset are interchange‐ able; when a replicaset is scaled down, an arbitrary pod is selected for deletion. your application’s behavior shouldn’t change because of such a scale-down operation. replicaset spec like all objects in kubernetes, replicasets are defined using a specification. all replicasets must have a unique name (defined using the metadata.name field), a spec section that describes the number of pods (replicas) that should be running clusterwide at any given time, and a pod template that describes the pod to be created when the defined number of replicas is not met. example 9-1 shows a minimal replicaset definition. example 9-1. kuard-rs.yaml apiversion: extensions/v1beta1 kind: replicaset metadata:  name: kuard spec:  replicas: 1  template:  metadata:  labels:  app: kuard  version: \"2\"  spec:  containers:  - name: kuard  image: \"gcr.io/kuar-demo/kuard-amd64:green\" pod templates as mentioned previously, when the number of pods in the current state is less than the number of pods in the desired state, the replicaset controller will create new pods. the pods are created using a pod template that is contained in the replicaset specification. the pods are created in exactly the same manner as when you created a pod from a yaml file in previous chapters, but instead of using a file, the kubernetes replicaset controller creates and submits a pod manifest based on the pod template directly to the api server. the following shows an example of a pod template in a replicaset: template:  metadata:  labels: 106 | chapter 9: replicasets  app: helloworld  version: v1  spec:  containers:  - name: helloworld  image: kelseyhightower/helloworld:v1  ports:  - containerport: 80 labels in any cluster of reasonable size, there are many different pods running at any given time—so how does the replicaset reconciliation loop discover the set of pods for a particular replicaset? replicasets monitor cluster state using a set of pod labels. labels are used to filter pod listings and track pods running within a cluster. when initially created, a replicaset fetches a pod listing from the kubernetes api and filters the results by labels. based on the number of pods returned by the query, the replica‐ set deletes or creates pods to meet the desired number of replicas. the labels used for filtering are defined in the replicaset spec section and are the key to understanding how replicasets work. the selector in the replicaset spec should be a proper subset of the labels in the pod template. creating a replicaset replicasets are created by submitting a replicaset object to the kubernetes api. in this section we will create a replicaset using a configuration file and the kubectl apply command. the replicaset configuration file in example 9-1 will ensure one copy of the gcr.io/ kuar-demo/kuard-amd64:green container is running at any given time. use the kubectl apply command to submit the kuard replicaset to the kubernetes api: $ kubectl apply -f kuard-rs.yaml replicaset \"kuard\" created once the kuard replicaset has been accepted, the replicaset controller will detect that there are no kuard pods running that match the desired state, and a new kuard pod will be created based on the contents of the pod template: creating a replicaset | 107 $ kubectl get pods name ready status restarts age kuard-yvzgd 1/1 running 0 11s inspecting a replicaset as with pods and other kubernetes api objects, if you are interested in further details about a replicaset, the describe command will provide much more information about its state. here is an example of using describe to obtain the details of the replicaset we previously created: $ kubectl describe rs kuard name: kuard namespace: default image(s): kuard:1.9.15 selector: app=kuard,version=2 labels: app=kuard,version=2 replicas: 1 current / 1 desired pods status: 1 running / 0 waiting / 0 succeeded / 0 failed no volumes. you can see the label selector for the replicaset, as well as the state of all of the repli‐ cas managed by the replicaset. finding a replicaset from a pod sometimes you may wonder if a pod is being managed by a replicaset, and if it is, which replicaset. to enable this kind of discovery, the replicaset controller adds an annotation to every pod that it creates. the key for the annotation is kubernetes.io/created-by. if you run the following, look for the kubernetes.io/created-by entry in the annota‐ tions section: $ kubectl get pods <pod-name> -o yaml if applicable, this will list the name of the replicaset that is managing this pod. note that such annotations are best-effort; they are only created when the pod is created by the replicaset, and can be removed by a kubernetes user at any time. finding a set of pods for a replicaset you can also determine the set of pods managed by a replicaset. first, you can get the set of labels using the kubectl describe command. in the previous example, the label selector was app=kuard,version=2. to find the pods that match this selector, use the --selector flag or the shorthand -l: $ kubectl get pods -l app=kuard,version=2 108 | chapter 9: replicasets this is exactly the same query that the replicaset executes to determine the current number of pods. scaling replicasets replicasets are scaled up or down by updating the spec.replicas key on the replicaset object stored in kubernetes. when a replicaset is scaled up, new pods are submitted to the kubernetes api using the pod template defined on the replicaset. imperative scaling with kubectl scale the easiest way to achieve this is using the scale command in kubectl. for example, to scale up to four replicas you could run: $ kubectl scale replicasets kuard --replicas=4 while such imperative commands are useful for demonstrations and quick reactions to emergency situations (e.g., in response to a sudden increase in load), it is impor‐ tant to also update any text-file configurations to match the number of replicas that you set via the imperative scale command. the reason for this becomes obvious when you consider the following scenario: alice is on call, when suddenly there is a large increase in load on the service she is managing. alice uses the scale command to increase the number of servers respond‐ ing to requests to 10, and the situation is resolved. however, alice forgets to update the replicaset configurations checked into source control. several days later, bob is pre‐ paring the weekly rollouts. bob edits the replicaset configurations stored in version control to use the new container image, but he doesn’t notice that the number of repli‐ cas in the file is currently 5, not the 10 that alice set in response to the increased load. bob proceeds with the rollout, which both updates the container image and reduces the number of replicas by half, causing an immediate overload or outage. hopefully, this illustrates the need to ensure that any imperative changes are immedi‐ ately followed by a declarative change in source control. indeed, if the need is not acute, we generally recommend only making declarative changes as described in the following section. declaratively scaling with kubectl apply in a declarative world, we make changes by editing the configuration file in version control and then applying those changes to our cluster. to scale the kuard replicaset, edit the kuard-rs.yaml configuration file and set the replicas count to 3: ... spec:  replicas: 3 ... scaling replicasets | 109 in a multiuser setting, you would like to have a documented code review of this change and eventually check the changes into version control. either way, you can then use the kubectl apply command to submit the updated kuard replicaset to the api server: $ kubectl apply -f kuard-rs.yaml replicaset \"kuard\" configured now that the updated kuard replicaset is in place, the replicaset controller will detect that the number of desired pods has changed and that it needs to take action to realize that desired state. if you used the imperative scale command in the previous section, the replicaset controller will destroy one pod to get the number to three. otherwise, it will submit two new pods to the kubernetes api using the pod template defined on the kuard replicaset. regardless, use the kubectl get pods command to list the running kuard pods. you should see output like the following: $ kubectl get pods name ready status restarts age kuard-3a2sb 1/1 running 0 26s kuard-wuq9v 1/1 running 0 26s kuard-yvzgd 1/1 running 0 2m autoscaling a replicaset while there will be times when you want to have explicit control over the number of replicas in a replicaset, often you simply want to have “enough” replicas. the defini‐ tion varies depending on the needs of the containers in the replicaset. for example, with a web server like nginx, you may want to scale due to cpu usage. for an inmemory cache, you may want to scale with memory consumption. in some cases you may want to scale in response to custom application metrics. kubernetes can handle all of these scenarios via horizontal pod autoscaling (hpa). hpa requires the presence of the heapster pod on your cluster. heapster keeps track of metrics and provides an api for consum‐ ing metrics that hpa uses when making scaling decisions. most installations of kubernetes include heapster by default. you can validate its presence by listing the pods in the kube-system name‐ space: $ kubectl get pods --namespace=kube-system you should see a pod named heapster somewhere in that list. if you do not see it, autoscaling will not work correctly. “horizontal pod autoscaling” is kind of a mouthful, and you might wonder why it is not simply called “autoscaling.” kubernetes makes a distinction between horizontal scaling, which involves creating additional replicas of a pod, and vertical scaling, 110 | chapter 9: replicasets which involves increasing the resources required for a particular pod (e.g., increasing the cpu required for the pod). vertical scaling is not currently implemented in kubernetes, but it is planned. additionally, many solutions also enable cluster autoscaling, where the number of machines in the cluster is scaled in response to resource needs, but this solution is not covered here. autoscaling based on cpu scaling based on cpu usage is the most common use case for pod autoscaling. gen‐ erally it is most useful for request-based systems that consume cpu proportionally to the number of requests they are receiving, while using a relatively static amount of memory. to scale a replicaset, you can run a command like the following: $ kubectl autoscale rs kuard --min=2 --max=5 --cpu-percent=80 this command creates an autoscaler that scales between two and five replicas with a cpu threshold of 80%. to view, modify, or delete this resource you can use the stan‐ dard kubectl commands and the horizontalpodautoscalers resource. horizontal podautoscalers is quite a bit to type, but it can be shortened to hpa: $ kubectl get hpa because of the decoupled nature of kubernetes, there is no direct link between the hpa and the replicaset. while this is great for modularity and composition, it also enables some anti-patterns. in particular, it’s a bad idea to combine both autoscaling and impera‐ tive or declarative management of the number of replicas. if both you and an autoscaler are attempting to modify the number of rep‐ licas, it’s highly likely that you will clash, resulting in unexpected behavior. deleting replicasets when a replicaset is no longer required it can be deleted using the kubectl delete command. by default, this also deletes the pods that are managed by the replicaset: $ kubectl delete rs kuard replicaset \"kuard\" deleted running the kubectl get pods command shows that all the kuard pods created by the kuard replicaset have also been deleted: $ kubectl get pods deleting replicasets | 111 if you don’t want to delete the pods that are being managed by the replicaset, you can set the --cascade flag to false to ensure only the replicaset object is deleted and not the pods: $ kubectl delete rs kuard --cascade=false summary composing pods with replicasets provides the foundation for building robust appli‐ cations with automatic failover, and makes deploying those applications a breeze by enabling scalable and sane deployment patterns. replicasets should be used for any pod you care about, even if it is a single pod! some people even default to using rep‐ licasets instead of pods. a typical cluster will have many replicasets, so apply liber‐ ally to the affected area. 112 | chapter 9: replicasets chapter 10 deployments so far, you have seen how to package your applications as containers, create replica‐ ted sets of containers, and use ingress controllers to load-balance traffic to your serv‐ ices. all of these objects (pods, replicasets, and services) are used to build a single instance of your application. however, they do little to help you manage the daily or weekly cadence of releasing new versions of your application. indeed, both pods and replicasets are expected to be tied to specific container images that don’t change. the deployment object exists to manage the release of new versions. deployments represent deployed applications in a way that transcends any particular version. additionally, deployments enable you to easily move from one version of your code to the next. this “rollout” process is specifiable and careful. it waits for a userconfigurable amount of time between upgrading individual pods. it also uses health checks to ensure that the new version of the application is operating correctly, and stops the deployment if too many failures occur. using deployments you can simply and reliably roll out new software versions without downtime or errors. the actual mechanics of the software rollout performed by a deployment is controlled by a deployment controller that runs in the kubernetes cluster itself. this means you can let a deployment proceed unattended and it will still operate correctly and safely. this makes it easy to integrate deployments with numer‐ ous continuous delivery tools and services. further, running server-side makes it safe to perform a rollout from places with poor or intermittent internet connectivity. imagine rolling out a new version of your software from your phone while riding on the subway. deployments make this possible and safe! 113 when kubernetes was first released, one of the most popular dem‐ onstrations of its power was the “rolling update,” which showed how you could use a single command to seamlessly update a run‐ ning application without any downtime and without losing requests. this original demo was based on the kubectl rollingupdate command, which is still available in the command-line tool, although its functionality has largely been subsumed by the deployment object. your first deployment like all objects in kubernetes, a deployment can be represented as a declarative yaml object that provides the details about what you want to run. in the following case, the deployment is requesting a single instance of the kuard application: apiversion: extensions/v1beta1 kind: deployment metadata:  name: kuard spec:  selector:  matchlabels:  run: kuard  replicas: 1  template:  metadata:  labels:  run: kuard  spec:  containers:  - name: kuard  image: gcr.io/kuar-demo/kuard-amd64:blue save this yaml file as kuard-deployment.yaml, then you can create it using: $ kubectl create -f kuard-deployment.yaml deployment internals let’s explore how deployments actually work. just as we learned that replicasets manage pods, deployments manage replicasets. as with all relationships in kuber‐ netes, this relationship is defined by labels and a label selector. you can see the label selector by looking at the deployment object: $ kubectl get deployments kuard \\\\  -o jsonpath --template {.spec.selector.matchlabels} map 114 | chapter 10: deployments from this you can see that the deployment is managing a replicaset with the label run=kuard. we can use this in a label selector query across replicasets to find that specific replicaset: $ kubectl get replicasets --selector=run=kuard name desired current ready age kuard-1128242161 1 1 1 13m now let’s see the relationship between a deployment and a replicaset in action. we can resize the deployment using the imperative scale command: $ kubectl scale deployments kuard --replicas=2 deployment.extensions/kuard scaled now if we list that replicaset again, we should see: $ kubectl get replicasets --selector=run=kuard name desired current ready age kuard-1128242161 2 2 2 13m scaling the deployment has also scaled the replicaset it controls. now let’s try the opposite, scaling the replicaset: $ kubectl scale replicasets kuard-1128242161 --replicas=1 replicaset \"kuard-1128242161\" scaled now get that replicaset again: $ kubectl get replicasets --selector=run=kuard name desired current ready age kuard-1128242161 2 2 2 13m that’s odd. despite our scaling the replicaset to one replica, it still has two replicas as its desired state. what’s going on? remember, kubernetes is an online, self-healing system. the top-level deployment object is managing this replicaset. when you adjust the number of replicas to one, it no longer matches the desired state of the deployment, which has replicas set to 2. the deployment controller notices this and takes action to ensure the observed state matches the desired state, in this case read‐ justing the number of replicas back to two. if you ever want to manage that replicaset directly, you need to delete the deploy‐ ment (remember to set --cascade to false, or else it will delete the replicaset and pods as well!). your first deployment | 115 creating deployments of course, as has been stated elsewhere, you should have a preference for declarative management of your kubernetes configurations. this means maintaining the state of your deployments in yaml or json files on disk. as a starting point, download this deployment into a yaml file: $ kubectl get deployments kuard --export -o yaml > kuard-deployment.yaml $ kubectl replace -f kuard-deployment.yaml --save-config if you look in the file, you will see something like this: apiversion: extensions/v1beta1 kind: deployment metadata:  annotations:  deployment.kubernetes.io/revision: \"1\"  creationtimestamp: null  generation: 1  labels:  run: kuard  name: kuard  selflink: /apis/extensions/v1beta1/namespaces/default/deployments/kuard spec:  progressdeadlineseconds: 2147483647  replicas: 2  revisionhistorylimit: 10  selector:  matchlabels:  run: kuard  strategy:  rollingupdate:  maxsurge: 1  maxunavailable: 1  type: rollingupdate  template:  metadata:  creationtimestamp: null  labels:  run: kuard  spec:  containers:  - image: gcr.io/kuar-demo/kuard-amd64:blue  imagepullpolicy: ifnotpresent  name: kuard  resources: {}  terminationmessagepath: /dev/termination-log  terminationmessagepolicy: file  dnspolicy: clusterfirst  restartpolicy: always  schedulername: default-scheduler 116 | chapter 10: deployments  securitycontext: {}  terminationgraceperiodseconds: 30 status: {} a lot of read-only and default fields were removed in the preceding listing for brevity. you also need to run kubectl replace --saveconfig. this adds an annotation so that, when applying changes in the future, kubectl will know what the last applied configuration was for smarter merging of configs. if you always use kubectl apply, this step is only required after the first time you create a deployment using kubectl create -f. the deployment spec has a very similar structure to the replicaset spec. there is a pod template, which contains a number of containers that are created for each replica managed by the deployment. in addition to the pod specification, there is also a strategy object: ...  strategy:  rollingupdate:  maxsurge: 1  maxunavailable: 1  type: rollingupdate ... the strategy object dictates the different ways in which a rollout of new software can proceed. there are two different strategies supported by deployments: recreate and rollingupdate. these are discussed in detail later in this chapter. managing deployments as with all kubernetes objects, you can get detailed information about your deploy‐ ment via the kubectl describe command: $ kubectl describe deployments kuard name: kuard namespace: default creationtimestamp: tue, 16 apr 2019 21:43:25 -0700 labels: run=kuard annotations: deployment.kubernetes.io/revision: 1 selector: run=kuard replicas: 2 desired | 2 updated | 2 total | 2 available | 0 ... strategytype: rollingupdate minreadyseconds: 0 rollingupdatestrategy: 1 max unavailable, 1 max surge managing deployments | 117 pod template:  labels: run=kuard  containers:  kuard:  image: gcr.io/kuar-demo/kuard-amd64:blue  port: <none>  host port: <none>  environment: <none>  mounts: <none>  volumes: <none> conditions:  type status reason  ---- ------ ------  available true minimumreplicasavailable oldreplicasets: <none> newreplicaset: kuard-6d69d9fc5c (2/2 replicas created) events:  type reason age from message  ---- ------ ---- ---- -------  normal scalingreplicaset 4m6s deployment-con... ...  normal scalingreplicaset 113s (x2 over 3m20s) deployment-con... ... in the output of describe there is a great deal of important information. two of the most important pieces of information in the output are oldreplicasets and newreplicaset. these fields point to the replicaset objects this deployment is currently managing. if a deployment is in the middle of a rollout, both fields will be set to a value. if a rollout is complete, oldreplicasets will be set to <none>. in addition to the describe command, there is also the kubectl rollout command for deployments. we will go into this command in more detail later on, but for now, know that you can use kubectl rollout history to obtain the history of rollouts associated with a particular deployment. if you have a current deployment in pro‐ gress, you can use kubectl rollout status to obtain the current status of a rollout. updating deployments deployments are declarative objects that describe a deployed application. the two most common operations on a deployment are scaling and application updates. scaling a deployment although we previously showed how you could imperatively scale a deployment using the kubectl scale command, the best practice is to manage your deployments declaratively via the yaml files, and then use those files to update your deployment. to scale up a deployment, you would edit your yaml file to increase the number of replicas: 118 | chapter 10: deployments ... spec:  replicas: 3 ... once you have saved and committed this change, you can update the deployment using the kubectl apply command: $ kubectl apply -f kuard-deployment.yaml this will update the desired state of the deployment, causing it to increase the size of the replicaset it manages, and eventually create a new pod managed by the deploy‐ ment: $ kubectl get deployments kuard name desired current up-to-date available age kuard 3 3 3 3 4m updating a container image the other common use case for updating a deployment is to roll out a new version of the software running in one or more containers. to do this, you should likewise edit the deployment yaml file, though in this case you are updating the container image, rather than the number of replicas: ...  containers:  - image: gcr.io/kuar-demo/kuard-amd64:green  imagepullpolicy: always ... we are also going to put an annotation in the template for the deployment to record some information about the update: ... spec:  ...  template:  metadata:  annotations:  kubernetes.io/change-cause: \"update to green kuard\" ... make sure you add this annotation to the template and not the deployment itself, since the kubectl apply command uses this field in the deployment object. also, do not update the changecause annotation when doing simple scaling operations. a modifi‐ cation of change-cause is a significant change to the template and will trigger a new rollout. updating deployments | 119 again, you can use kubectl apply to update the deployment: $ kubectl apply -f kuard-deployment.yaml after you update the deployment it will trigger a rollout, which you can then monitor via the kubectl rollout command: $ kubectl rollout status deployments kuard deployment kuard successfully rolled out you can see the old and new replicasets managed by the deployment along with the images being used. both the old and new replicasets are kept around in case you want to roll back: $ kubectl get replicasets -o wide name desired current ready ... image(s) ... kuard-1128242161 0 0 0 ... gcr.io/kuar-demo/ ... kuard-1128635377 3 3 3 ... gcr.io/kuar-demo/ ... if you are in the middle of a rollout and you want to temporarily pause it for some reason (e.g., if you start seeing weird behavior in your system and you want to inves‐ tigate), you can use the pause command: $ kubectl rollout pause deployments kuard deployment \"kuard\" paused if, after investigation, you believe the rollout can safely proceed, you can use the resume command to start up where you left off: $ kubectl rollout resume deployments kuard deployment \"kuard\" resumed rollout history kubernetes deployments maintain a history of rollouts, which can be useful both for understanding the previous state of the deployment and to roll back to a specific version. you can see the deployment history by running: $ kubectl rollout history deployment kuard deployment.extensions/kuard revision change-cause 1 <none> 2 update to green kuard the revision history is given in oldest to newest order. a unique revision number is incremented for each new rollout. so far we have two: the initial deployment, and the update of the image to kuard:1.9.10. 120 | chapter 10: deployments if you are interested in more details about a particular revision, you can add the --revision flag to view details about that specific revision: $ kubectl rollout history deployment kuard --revision=2 deployment.extensions/kuard with revision #2 pod template:  labels: pod-template-hash=54b74ddcd4  run=kuard  annotations: kubernetes.io/change-cause: update to green kuard  containers:  kuard:  image: gcr.io/kuar-demo/kuard-amd64:green  port: <none>  host port: <none>  environment: <none>  mounts: <none>  volumes: <none> let’s do one more update for this example. update the kuard version back to blue by modifying the container version number and updating the change-cause annotation. apply it with kubectl apply. our history should now have three entries: $ kubectl rollout history deployment kuard deployment.extensions/kuard revision change-cause 1 <none> 2 update to green kuard 3 update to blue kuard let’s say there is an issue with the latest release and you want to roll back while you investigate. you can simply undo the last rollout: $ kubectl rollout undo deployments kuard deployment \"kuard\" rolled back the undo command works regardless of the stage of the rollout. you can undo both partially completed and fully completed rollouts. an undo of a rollout is actually sim‐ ply a rollout in reverse (e.g., from v2 to v1, instead of from v1 to v2), and all of the same policies that control the rollout strategy apply to the undo strategy as well. you can see the deployment object simply adjusts the desired replica counts in the man‐ aged replicasets: $ kubectl get replicasets -o wide name desired current ready ... image(s) ... kuard-1128242161 0 0 0 ... gcr.io/kuar-demo/ ... kuard-1570155864 0 0 0 ... gcr.io/kuar-demo/ ... kuard-2738859366 3 3 3 ... gcr.io/kuar-demo/ ... updating deployments | 121 when using declarative files to control your production systems, you want to, as much as possible, ensure that the checked-in mani‐ fests match what is actually running in your cluster. when you do a kubectl rollout undo you are updating the production state in a way that isn’t reflected in your source control. an alternative (and perhaps preferred) way to undo a rollout is to revert your yaml file and kubectl apply the previous version. in this way, your “change tracked configuration” more closely tracks what is really running in your cluster. let’s look at our deployment history again: $ kubectl rollout history deployment kuard deployment.extensions/kuard revision change-cause 1 <none> 3 update to blue kuard 4 update to green kuard revision 2 is missing! it turns out that when you roll back to a previous revision, the deployment simply reuses the template and renumbers it so that it is the latest revi‐ sion. what was revision 2 before is now reordered into revision 4. we previously saw that you can use the kubectl rollout undo command to roll back to a previous version of a deployment. additionally, you can roll back to a spe‐ cific revision in the history using the --to-revision flag: $ kubectl rollout undo deployments kuard --to-revision=3 deployment \"kuard\" rolled back $ kubectl rollout history deployment kuard deployment.extensions/kuard revision change-cause 1 <none> 4 update to green kuard 5 update to blue kuard again, the undo took revision 3, applied it, and renumbered it as revision 5. specifying a revision of 0 is a shorthand way of specifying the previous revision. in this way, kubectl rollout undo is equivalent to kubectl rollout undo --torevision=0. by default, the complete revision history of a deployment is kept attached to the deployment object itself. over time (e.g., years) this history can grow fairly large, so it is recommended that if you have deployments that you expect to keep around for a long time you set a maximum history size for the deployment revision history, to limit the total size of the deployment object. for example, if you do a daily update 122 | chapter 10: deployments you may limit your revision history to 14, to keep a maximum of 2 weeks’ worth of revisions (if you don’t expect to need to roll back beyond 2 weeks). to accomplish this, use the revisionhistorylimit property in the deployment specification: ... spec:  # we do daily rollouts, limit the revision history to two weeks of  # releases as we don\\'t expect to roll back beyond that.  revisionhistorylimit: 14 ... deployment strategies when it comes time to change the version of software implementing your service, a kubernetes deployment supports two different rollout strategies: • recreate • rollingupdate recreate strategy the recreate strategy is the simpler of the two rollout strategies. it simply updates the replicaset it manages to use the new image and terminates all of the pods associ‐ ated with the deployment. the replicaset notices that it no longer has any replicas, and re-creates all pods using the new image. once the pods are re-created, they are running the new version. while this strategy is fast and simple, it has one major drawback—it is potentially catastrophic, and will almost certainly result in some site downtime. because of this, the recreate strategy should only be used for test deployments where a service is not user-facing and a small amount of downtime is acceptable. rollingupdate strategy the rollingupdate strategy is the generally preferable strategy for any user-facing service. while it is slower than recreate, it is also significantly more sophisticated and robust. using rollingupdate, you can roll out a new version of your service while it is still receiving user traffic, without any downtime. as you might infer from the name, the rollingupdate strategy works by updating a few pods at a time, moving incrementally until all of the pods are running the new version of your software. deployment strategies | 123 managing multiple versions of your service importantly, this means that for a period of time, both the new and the old version of your service will be receiving requests and serving traffic. this has important impli‐ cations for how you build your software. namely, it is critically important that each version of your software, and all of its clients, is capable of talking interchangeably with both a slightly older and a slightly newer version of your software. as an example of why this is important, consider the following scenario: you are in the middle of rolling out your frontend software; half of your servers are running version 1 and half are running version 2. a user makes an initial request to your service and downloads a client-side javascript library that implements your ui. this request is serviced by a version 1 server and thus the user receives the version 1 client library. this client library runs in the user’s browser and makes subsequent api requests to your service. these api requests happen to be routed to a version 2 server; thus, version 1 of your javascript client library is talking to version 2 of your api server. if you haven’t ensured compatibility between these versions, your application won’t function correctly. at first, this might seem like an extra burden. but in truth, you always had this prob‐ lem; you may just not have noticed. concretely, a user can make a request at time t just before you initiate an update. this request is serviced by a version 1 server. at t1 you update your service to version 2. at t2 the version 1 client code running on the user’s browser runs and hits an api endpoint being operated by a version 2 server. no matter how you update your software, you have to maintain backward and forward compatibility for reliable updates. the nature of the rollingupdate strategy simply makes it more clear and explicit that this is something to think about. note that this doesn’t just apply to javascript clients—the same thing is true of client libraries that are compiled into other services that make calls to your service. just because you updated doesn’t mean they have updated their client libraries. this sort of backward compatibility is critical to decoupling your service from systems that depend on your service. if you don’t formalize your apis and decouple yourself, you are forced to carefully manage your rollouts with all of the other systems that call into your service. this kind of tight coupling makes it extremely hard to produce the nec‐ essary agility to be able to push out new software every week, let alone every hour or every day. in the decoupled architecture shown in figure 10-1, the frontend is iso‐ lated from the backend via an api contract and a load balancer, whereas in the cou‐ pled architecture, a thick client compiled into the frontend is used to connect directly to the backends. 124 | chapter 10: deployments figure 10-1. diagrams of both decoupled (left) and coupled (right) application architectures con\\x80guring a rolling update rollingupdate is a fairly generic strategy; it can be used to update a variety of appli‐ cations in a variety of settings. consequently, the rolling update itself is quite configu‐ rable; you can tune its behavior to suit your particular needs. there are two parameters you can use to tune the rolling update behavior: maxunavailable and maxsurge. the maxunavailable parameter sets the maximum number of pods that can be unavailable during a rolling update. it can either be set to an absolute number (e.g., 3, meaning a maximum of three pods can be unavailable) or to a percentage (e.g., 20%, meaning a maximum of 20% of the desired number of replicas can be unavailable). generally speaking, using a percentage is a good approach for most services, since the value is correctly applicable regardless of the desired number of replicas in the deployment. however, there are times when you may want to use an absolute number (e.g., limiting the maximum unavailable pods to one). at its core, the maxunavailable parameter helps tune how quickly a rolling update proceeds. for example, if you set maxunavailable to 50%, then the rolling update will immediately scale the old replicaset down to 50% of its original size. if you have four replicas, it will scale it down to two replicas. the rolling update will then replace the removed pods by scaling the new replicaset up to two replicas, for a total of four rep‐ licas (two old, two new). it will then scale the old replicaset down to zero replicas, for a total size of two new replicas. finally, it will scale the new replicaset up to four rep‐ licas, completing the rollout. thus, with maxunavailable set to 50%, our rollout com‐ pletes in four steps, but with only 50% of our service capacity at times. deployment strategies | 125 consider what happens if we instead set maxunavailable to 25%. in this situation, each step is only performed with a single replica at a time and thus it takes twice as many steps for the rollout to complete, but availability only drops to a minimum of 75% during the rollout. this illustrates how maxunavailable allows us to trade roll‐ out speed for availability. the observant among you will note that the recreate strategy is identical to the rollingupdate strategy with maxunavailable set to 100%. using reduced capacity to achieve a successful rollout is useful either when your ser‐ vice has cyclical traffic patterns (e.g., much less traffic at night) or when you have limited resources, so scaling to larger than the current maximum number of replicas isn’t possible. however, there are situations where you don’t want to fall below 100% capacity, but you are willing to temporarily use additional resources in order to perform a rollout. in these situations, you can set the maxunavailable parameter to 0%, and instead con‐ trol the rollout using the maxsurge parameter. like maxunavailable, maxsurge can be specified either as a specific number or a percentage. the maxsurge parameter controls how many extra resources can be created to ach‐ ieve a rollout. to illustrate how this works, imagine we have a service with 10 replicas. we set maxunavailable to 0 and maxsurge to 20%. the first thing the rollout will do is scale the new replicaset up to 2 replicas, for a total of 12 (120%) in the service. it will then scale the old replicaset down to 8 replicas, for a total of 10 (8 old, 2 new) in the service. this process proceeds until the rollout is complete. at any time, the capacity of the service is guaranteed to be at least 100% and the maximum extra resources used for the rollout are limited to an additional 20% of all resources. setting maxsurge to 100% is equivalent to a blue/green deployment. the deployment controller first scales the new version up to 100% of the old version. once the new version is healthy, it immediately scales the old version down to 0%. slowing rollouts to ensure service health the purpose of a staged rollout is to ensure that the rollout results in a healthy, stable service running the new software version. to do this, the deployment controller always waits until a pod reports that it is ready before moving on to updating the next pod. 126 | chapter 10: deployments the deployment controller examines the pod’s status as determined by its readiness checks. readiness checks are part of the pod’s health probes, and they are described in detail in chapter 5. if you want to use deployments to reliably roll out your software, you have to specify readiness health checks for the containers in your pod. without these checks, the deployment controller is running blind. sometimes, however, simply noticing that a pod has become ready doesn’t give you sufficient confidence that the pod actually is behaving correctly. some error condi‐ tions only occur after a period of time. for example, you could have a serious mem‐ ory leak that takes a few minutes to show up, or you could have a bug that is only triggered by 1% of all requests. in most real-world scenarios, you want to wait a period of time to have high confidence that the new version is operating correctly before you move on to updating the next pod. for deployments, this time to wait is defined by the minreadyseconds parameter: ... spec:  minreadyseconds: 60 ... setting minreadyseconds to 60 indicates that the deployment must wait for 60 sec‐ onds after seeing a pod become healthy before moving on to updating the next pod. in addition to waiting a period of time for a pod to become healthy, you also want to set a timeout that limits how long the system will wait. suppose, for example, the new version of your service has a bug and immediately deadlocks. it will never become ready, and in the absence of a timeout, the deployment controller will stall your rollout forever. the correct behavior in such a situation is to time out the rollout. this in turn marks the rollout as failed. this failure status can be used to trigger alerting that can indicate to an operator that there is a problem with the rollout. at first blush, timing out a rollout might seem like an unnecessary complication. however, increasingly, things like rollouts are being triggered by fully automated systems with little to no human involvement. in such a situation, timing out becomes a critical exception, which can either trigger an automated rollback of the release or create a ticket/event that triggers human intervention. to set the timeout period, the deployment parameter progressdeadlineseconds is used: deployment strategies | 127 ... spec:  progressdeadlineseconds: 600 ... this example sets the progress deadline to 10 minutes. if any particular stage in the rollout fails to progress in 10 minutes, then the deployment is marked as failed, and all attempts to move the deployment forward are halted. it is important to note that this timeout is given in terms of deployment progress, not the overall length of a deployment. in this context, progress is defined as any time the deployment creates or deletes a pod. when that happens, the timeout clock is reset to zero. figure 10-2 is an illustration of the deployment lifecycle. figure 10-2. the kubernetes deployment lifecycle deleting a deployment if you ever want to delete a deployment, you can do it either with the imperative command: $ kubectl delete deployments kuard or using the declarative yaml file we created earlier: $ kubectl delete -f kuard-deployment.yaml in either case, by default, deleting a deployment deletes the entire service. it will delete not just the deployment, but also any replicasets being managed by the deployment, as well as any pods being managed by the replicasets. as with replica‐ sets, if this is not the desired behavior, you can use the --cascade=false flag to exclusively delete the deployment object. monitoring a deployment when it comes to a deployment, it is important to note that if it fails to makes pro‐ gress after a certain amount of time, the deployment will time out. when this hap‐ pens, the status of the deployment will transition to a failed state. this status can be 128 | chapter 10: deployments obtained from the status.conditions array, where there will be a condition whose type is progressing and whose status is false. a deployment in such a state has failed and will not progress further. to set how long the deployment controller should wait before transitioning into this state, use the spec.progressdeadlineseconds field. summary at the end of the day, the primary goal of kubernetes is to make it easy for you to build and deploy reliable distributed systems. this means not just instantiating the application once, but managing the regularly scheduled rollout of new versions of that software service. deployments are a critical piece of reliable rollouts and rollout management for your services. summary | 129  chapter 11 daemonsets deployments and replicasets are generally about creating a service (e.g., a web server) with multiple replicas for redundancy. but that is not the only reason you may want to replicate a set of pods within a cluster. another reason to replicate a set of pods is to schedule a single pod on every node within the cluster. generally, the moti‐ vation for replicating a pod to every node is to land some sort of agent or daemon on each node, and the kubernetes object for achieving this is the daemonset. a daemonset ensures a copy of a pod is running across a set of nodes in a kuber‐ netes cluster. daemonsets are used to deploy system daemons such as log collectors and monitoring agents, which typically must run on every node. daemonsets share similar functionality with replicasets; both create pods that are expected to be longrunning services and ensure that the desired state and the observed state of the clus‐ ter match. given the similarities between daemonsets and replicasets, it’s important to under‐ stand when to use one over the other. replicasets should be used when your applica‐ tion is completely decoupled from the node and you can run multiple copies on a given node without special consideration. daemonsets should be used when a single copy of your application must run on all or a subset of the nodes in the cluster. you should generally not use scheduling restrictions or other parameters to ensure that pods do not colocate on the same node. if you find yourself wanting a single pod per node, then a daemonset is the correct kubernetes resource to use. likewise, if you find yourself building a homogeneous replicated service to serve user traffic, then a replicaset is probably the right kubernetes resource to use. you can use labels to run daemonset pods on specific nodes; for example, you may want to run specialized intrusion-detection software on nodes that are exposed to the edge network. 131 you can also use daemonsets to install software on nodes in a cloud-based cluster. for many cloud services, an upgrade or scaling of a cluster can delete and/or recreate new virtual machines. this dynamic immutable infrastructure approach can cause problems if you want (or are required by central it) to have specific software on every node. to ensure that specific software is installed on every machine despite upgrades and scale events, a daemonset is the right approach. you can even mount the host filesystem and run scripts that install rpm/deb packages onto the host operating system. in this way, you can have a cloud-native cluster that still meets the enterprise requirements of your it department. daemonset scheduler by default a daemonset will create a copy of a pod on every node unless a node selec‐ tor is used, which will limit eligible nodes to those with a matching set of labels. dae‐ monsets determine which node a pod will run on at pod creation time by specifying the nodename field in the pod spec. as a result, pods created by daemonsets are ignored by the kubernetes scheduler. like replicasets, daemonsets are managed by a reconciliation control loop that measures the desired state (a pod is present on all nodes) with the observed state (is the pod present on a particular node?). given this information, the daemonset con‐ troller creates a pod on each node that doesn’t currently have a matching pod. if a new node is added to the cluster, then the daemonset controller notices that it is missing a pod and adds the pod to the new node. daemonsets and replicasets are a great demonstration of the value of kubernetes’s decoupled architecture. it might seem that the right design would be for a replicaset to own the pods it manages, and for pods to be subresources of a replicaset. likewise, the pods managed by a daemonset would be subresources of that daemon‐ set. however, this kind of encapsulation would require that tools for dealing with pods be written two different times, once for dae‐ monsets and once for replicasets. instead, kubernetes uses a decoupled approach where pods are top-level objects. this means that every tool you have learned for introspecting pods in the con‐ text of replicasets (e.g., kubectl logs <pod-name>) is equally applicable to pods created by daemonsets. creating daemonsets daemonsets are created by submitting a daemonset configuration to the kubernetes api server. the daemonset in example 11-1 will create a fluentd logging agent on every node in the target cluster. 132 | chapter 11: daemonsets example 11-1. fluentd.yaml apiversion: extensions/v1beta1 kind: daemonset metadata:  name: fluentd  labels:  app: fluentd spec:  template:  metadata:  labels:  app: fluentd  spec:  containers:  - name: fluentd  image: fluent/fluentd:v0.14.10  resources:  limits:  memory: 200mi  requests:  cpu: 100m  memory: 200mi  volumemounts:  - name: varlog  mountpath: /var/log  - name: varlibdockercontainers  mountpath: /var/lib/docker/containers  readonly: true  terminationgraceperiodseconds: 30  volumes:  - name: varlog  hostpath:  path: /var/log  - name: varlibdockercontainers  hostpath:  path: /var/lib/docker/containers daemonsets require a unique name across all daemonsets in a given kubernetes namespace. each daemonset must include a pod template spec, which will be used to create pods as needed. this is where the similarities between replicasets and dae‐ monsets end. unlike replicasets, daemonsets will create pods on every node in the cluster by default unless a node selector is used. once you have a valid daemonset configuration in place, you can use the kubectl apply command to submit the daemonset to the kubernetes api. in this section we will create a daemonset to ensure the fluentd http server is running on every node in our cluster: $ kubectl apply -f fluentd.yaml daemonset \"fluentd\" created creating daemonsets | 133 once the fluentd daemonset has been successfully submitted to the kubernetes api, you can query its current state using the kubectl describe command: $ kubectl describe daemonset fluentd name: fluentd image(s): fluent/fluentd:v0.14.10 selector: app=fluentd node-selector: <none> labels: app=fluentd desired number of nodes scheduled: 3 current number of nodes scheduled: 3 number of nodes misscheduled: 0 pods status: 3 running / 0 waiting / 0 succeeded / 0 failed this output indicates a fluentd pod was successfully deployed to all three nodes in our cluster. we can verify this using the kubectl get pods command with the -o flag to print the nodes where each fluentd pod was assigned: $ kubectl get pods -o wide name age node fluentd-1q6c6 13m k0-default-pool-35609c18-z7tb fluentd-mwi7h 13m k0-default-pool-35609c18-ydae fluentd-zr6l7 13m k0-default-pool-35609c18-pol3 with the fluentd daemonset in place, adding a new node to the cluster will result in a fluentd pod being deployed to that node automatically: $ kubectl get pods -o wide name age node fluentd-1q6c6 13m k0-default-pool-35609c18-z7tb fluentd-mwi7h 13m k0-default-pool-35609c18-ydae fluentd-oipmq 43s k0-default-pool-35609c18-0xnl fluentd-zr6l7 13m k0-default-pool-35609c18-pol3 this is exactly the behavior you want when managing logging daemons and other cluster-wide services. no action was required from our end; this is how the kuber‐ netes daemonset controller reconciles its observed state with our desired state. limiting daemonsets to speci\\x80c nodes the most common use case for daemonsets is to run a pod across every node in a kubernetes cluster. however, there are some cases where you want to deploy a pod to only a subset of nodes. for example, maybe you have a workload that requires a gpu or access to fast storage only available on a subset of nodes in your cluster. in cases like these, node labels can be used to tag specific nodes that meet workload requirements. 134 | chapter 11: daemonsets adding labels to nodes the first step in limiting daemonsets to specific nodes is to add the desired set of labels to a subset of nodes. this can be achieved using the kubectl label command. the following command adds the ssd=true label to a single node: $ kubectl label nodes k0-default-pool-35609c18-z7tb ssd=true node \"k0-default-pool-35609c18-z7tb\" labeled just like with other kubernetes resources, listing nodes without a label selector returns all nodes in the cluster: $ kubectl get nodes name status age k0-default-pool-35609c18-0xnl ready 23m k0-default-pool-35609c18-pol3 ready 1d k0-default-pool-35609c18-ydae ready 1d k0-default-pool-35609c18-z7tb ready 1d using a label selector, we can filter nodes based on labels. to list only the nodes that have the ssd label set to true, use the kubectl get nodes command with the --selector flag: $ kubectl get nodes --selector ssd=true name status age k0-default-pool-35609c18-z7tb ready 1d node selectors node selectors can be used to limit what nodes a pod can run on in a given kuber‐ netes cluster. node selectors are defined as part of the pod spec when creating a dae‐ monset. the daemonset configuration in example 11-2 limits nginx to running only on nodes with the ssd=true label set. example 11-2. nginx-fast-storage.yaml apiversion: extensions/v1beta1 kind: \"daemonset\" metadata:  labels:  app: nginx  ssd: \"true\"  name: nginx-fast-storage spec:  template:  metadata:  labels:  app: nginx  ssd: \"true\"  spec: limiting daemonsets to speci\\x80c nodes | 135  nodeselector:  ssd: \"true\"  containers:  - name: nginx  image: nginx:1.10.0 let’s see what happens when we submit the nginx-fast-storage daemonset to the kubernetes api: $ kubectl apply -f nginx-fast-storage.yaml daemonset \"nginx-fast-storage\" created since there is only one node with the ssd=true label, the nginx-fast-storage pod will only run on that node: $ kubectl get pods -o wide name status node nginx-fast-storage-7b90t running k0-default-pool-35609c18-z7tb adding the ssd=true label to additional nodes will cause the nginx-fast-storage pod to be deployed on those nodes. the inverse is also true: if a required label is removed from a node, the pod will be removed by the daemonset controller. removing labels from a node that are required by a daemonset’s node selector will cause the pod being managed by that daemonset to be removed from the node. updating a daemonset daemonsets are great for deploying services across an entire cluster, but what about upgrades? prior to kubernetes 1.6, the only way to update pods managed by a dae‐ monset was to update the daemonset and then manually delete each pod that was managed by the daemonset so that it would be re-created with the new configura‐ tion. with the release of kubernetes 1.6, daemonsets gained an equivalent to the deployment object that manages a daemonset rollout inside the cluster. rolling update of a daemonset daemonsets can be rolled out using the same rollingupdate strategy that deploy‐ ments use. you can configure the update strategy using the spec.updatestrat egy.type field, which should have the value rollingupdate. when a daemonset has an update strategy of rollingupdate, any change to the spec.template field (or subfields) in the daemonset will initiate a rolling update. 136 | chapter 11: daemonsets as with rolling updates of deployments (see chapter 10), the rollingupdate strategy gradually updates members of a daemonset until all of the pods are running the new configuration. there are two parameters that control the rolling update of a daemonset: • spec.minreadyseconds, which determines how long a pod must be “ready” before the rolling update proceeds to upgrade subsequent pods • spec.updatestrategy.rollingupdate.maxunavailable, which indicates how many pods may be simultaneously updated by the rolling update you will likely want to set spec.minreadyseconds to a reasonably long value, for example 30–60 seconds, to ensure that your pod is truly healthy before the rollout proceeds. the setting for spec.updatestrategy.rollingupdate.maxunavailable is more likely to be application-dependent. setting it to 1 is a safe, general-purpose strategy, but it also takes a while to complete the rollout (number of nodes × minreadysec onds). increasing the maximum unavailability will make your rollout move faster, but increases the “blast radius” of a failed rollout. the characteristics of your application and cluster environment dictate the relative values of speed versus safety. a good approach might be to set maxunavailable to 1 and only increase it if users or admin‐ istrators complain about daemonset rollout speed. once a rolling update has started, you can use the kubectl rollout commands to see the current status of a daemonset rollout. for example, kubectl rollout status daemonsets my-daemon-set will show the current rollout status of a daemonset named my-daemon-set. deleting a daemonset deleting a daemonset is pretty straightforward using the kubectl delete com‐ mand. just be sure to supply the correct name of the daemonset you would like to delete: $ kubectl delete -f fluentd.yaml deleting a daemonset will also delete all the pods being managed by that daemonset. set the --cascade flag to false to ensure only the daemonset is deleted and not the pods. deleting a daemonset | 137 summary daemonsets provide an easy-to-use abstraction for running a set of pods on every node in a kubernetes cluster, or, if the case requires it, on a subset of nodes based on labels. the daemonset provides its own controller and scheduler to ensure key serv‐ ices like monitoring agents are always up and running on the right nodes in your cluster. for some applications, you simply want to schedule a certain number of replicas; you don’t really care where they run as long as they have sufficient resources and distribu‐ tion to operate reliably. however, there is a different class of applications, like agents and monitoring applications, that need to be present on every machine in a cluster to function properly. these daemonsets aren’t really traditional serving applications, but rather add additional capabilities and features to the kubernetes cluster itself. because the daemonset is an active declarative object managed by a controller, it makes it easy to declare your intent that an agent run on every machine without explicitly placing it on every machine. this is especially useful in the context of an autoscaled kubernetes cluster where nodes may constantly be coming and going without user intervention. in such cases, the daemonset automatically adds the proper agents to each node as it is added to the cluster by the autoscaler. 138 | chapter 11: daemonsets chapter 12 jobs so far we have focused on long-running processes such as databases and web applica‐ tions. these types of workloads run until either they are upgraded or the service is no longer needed. while long-running processes make up the large majority of work‐ loads that run on a kubernetes cluster, there is often a need to run short-lived, oneoff tasks. the job object is made for handling these types of tasks. a job creates pods that run until successful termination (i.e., exit with 0). in contrast, a regular pod will continually restart regardless of its exit code. jobs are useful for things you only want to do once, such as database migrations or batch jobs. if run as a regular pod, your database migration task would run in a loop, continually repopulat‐ ing the database after every exit. in this chapter we’ll explore the most common job patterns afforded by kubernetes. we will also leverage these patterns in real-life scenarios. the job object the job object is responsible for creating and managing pods defined in a template in the job specification. these pods generally run until successful completion. the job object coordinates running a number of pods in parallel. if the pod fails before a successful termination, the job controller will create a new pod based on the pod template in the job specification. given that pods have to be scheduled, there is a chance that your job will not execute if the required resources are not found by the scheduler. also, due to the nature of distributed systems there is a small chance, during certain failure scenarios, that duplicate pods will be created for a specific task. 139 job patterns jobs are designed to manage batch-like workloads where work items are processed by one or more pods. by default, each job runs a single pod once until successful termi‐ nation. this job pattern is defined by two primary attributes of a job, namely the number of job completions and the number of pods to run in parallel. in the case of the “run once until completion” pattern, the completions and parallelism parame‐ ters are set to 1. table 12-1 highlights job patterns based on the combination of completions and parallelism for a job configuration. table 12-1. job patterns type use case behavior completions parallelism one shot database migrations a single pod running once until successful termination 1 1 parallel \\x80xed completions multiple pods processing a set of work in parallel one or more pods running one or more times until reaching a \\x80xed completion count 1+ 1+ work queue: parallel jobs multiple pods processing from a centralized work queue one or more pods running once until successful termination 1 2+ one shot one-shot jobs provide a way to run a single pod once until successful termination. while this may sound like an easy task, there is some work involved in pulling this off. first, a pod must be created and submitted to the kubernetes api. this is done using a pod template defined in the job configuration. once a job is up and running, the pod backing the job must be monitored for successful termination. a job can fail for any number of reasons, including an application error, an uncaught exception during runtime, or a node failure before the job has a chance to complete. in all cases, the job controller is responsible for recreating the pod until a successful termination occurs. there are multiple ways to create a one-shot job in kubernetes. the easiest is to use the kubectl command-line tool: $ kubectl run -i oneshot \\\\  --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\  --restart=onfailure \\\\  -- --keygen-enable \\\\  --keygen-exit-on-complete \\\\  --keygen-num-to-gen 10 ... 140 | chapter 12: jobs (id 0) workload starting (id 0 1/10) item done: sha256:nasusg54xokrkjwyn+oshkupkew3mwq7occ (id 0 2/10) item done: sha256:hvkx1anns6sgf/er1lyo+zcdnb8gefgt0/8 (id 0 3/10) item done: sha256:irjclrov3mtt0p0jfsvuyhkrq1tdgr8h1jg (id 0 4/10) item done: sha256:nbqaivy/yrhmegk3ui2sahuxb/o6myo0qrk (id 0 5/10) item done: sha256:ccpboxnlxomqvr2v38yqimxgaa/w2tym+ai (id 0 6/10) item done: sha256:wey2ttidz4atjcr1iimxavczzznjrmboqp8 (id 0 7/10) item done: sha256:t3jsrct7sqwebgqg5crbmobulwk4lfdwiti (id 0 8/10) item done: sha256:e84/vze7kkyjch9ozh02mkxjgoty9phacec (id 0 9/10) item done: sha256:uomyex79qqbi1mhcifg4hdngkonlsij2k3s (id 0 10/10) item done: sha256:wcr8wigofag84bsa8f/9qhukqf+0mencady (id 0) workload exiting there are some things to note here: • the -i option to kubectl indicates that this is an interactive command. kubectl will wait until the job is running and then show the log output from the first (and in this case only) pod in the job. • --restart=onfailure is the option that tells kubectl to create a job object. • all of the options after -- are command-line arguments to the container image. these instruct our test server (kuard) to generate 10 4,096-bit ssh keys and then exit. • your output may not match this exactly. kubectl often misses the first couple of lines of output with the -i option. after the job has completed, the job object and related pod are still around. this is so that you can inspect the log output. note that this job won’t show up in kubectl get jobs unless you pass the -a flag. without this flag, kubectl hides completed jobs. delete the job before continuing: $ kubectl delete jobs oneshot the other option for creating a one-shot job is using a configuration file, as shown in example 12-1. example 12-1. job-oneshot.yaml apiversion: batch/v1 kind: job metadata:  name: oneshot spec:  template:  spec:  containers:  - name: kuard  image: gcr.io/kuar-demo/kuard-amd64:blue  imagepullpolicy: always job patterns | 141  args:  - \"--keygen-enable\"  - \"--keygen-exit-on-complete\"  - \"--keygen-num-to-gen=10\"  restartpolicy: onfailure submit the job using the kubectl apply command: $ kubectl apply -f job-oneshot.yaml job \"oneshot\" created then describe the oneshot job: $ kubectl describe jobs oneshot name: oneshot namespace: default image(s): gcr.io/kuar-demo/kuard-amd64:blue selector: controller-uid=cf87484b-e664-11e6-8222-42010a8a007b parallelism: 1 completions: 1 start time: sun, 29 jan 2017 12:52:13 -0800 labels: job=oneshot pods statuses: 0 running / 1 succeeded / 0 failed no volumes. events:  ... reason message  ... ------ -------  ... successfulcreate created pod: oneshot-4kfdt you can view the results of the job by looking at the logs of the pod that was created: $ kubectl logs oneshot-4kfdt ... serving on :8080 (id 0) workload starting (id 0 1/10) item done: sha256:+r6b4w81dbejxmcd3lhju+eignlezbpxitkn8iqhkpi (id 0 2/10) item done: sha256:mzhewajay1ka8vluslonnmk9fde5zdn7vvbs5ne8axm (id 0 3/10) item done: sha256:trteqhffljmwkqnnyggqm/ivxnyksbig8c03h0g3one (id 0 4/10) item done: sha256:tswpyh/j347il/mgqtxrrdezcoazetgzla8a3/hwbro (id 0 5/10) item done: sha256:ip8xtguj6gbwwlhqjkecvfds96b17nno21i/tnc1j9k (id 0 6/10) item done: sha256:zfnxdqvust/6zzevkyxdrg98p73c/5tm99sebperwfc (id 0 7/10) item done: sha256:th+cnl/iul/huukdmsq2xemdq8oavmhmo6iwj8zeoj0 (id 0 8/10) item done: sha256:3gfsuaalvehqcgnlbou4qd1zqqqj8j738i5r+i5xwvi (id 0 9/10) item done: sha256:5wv4l/xeihsjxwlut2fhf0sckm2g3xh3svtnbgskcxw (id 0 10/10) item done: sha256:bpqqoonwsbjzlqe9zuvrmzkz+dbjantz9hwmqhbdwli (id 0) workload exiting congratulations, your job has run successfully! 142 | chapter 12: jobs you may have noticed that we didn’t specify any labels when creat‐ ing the job object. like with other controllers (daemonsets, repli‐ casets, deployments, etc.) that use labels to identify a set of pods, unexpected behaviors can happen if a pod is reused across objects. because jobs have a finite beginning and ending, it is common for users to create many of them. this makes picking unique labels more difficult and more critical. for this reason, the job object will automatically pick a unique label and use it to identify the pods it creates. in advanced scenarios (such as swapping out a running job without killing the pods it is managing), users can choose to turn off this automatic behavior and manually specify labels and selectors. pod failure we just saw how a job can complete successfully. but what happens if something fails? let’s try that out and see what happens. let’s modify the arguments to kuard in our configuration file to cause it to fail out with a nonzero exit code after generating three keys, as shown in example 12-2. example 12-2. job-oneshot-failure1.yaml ... spec:  template:  spec:  containers:  ...  args:  - \"--keygen-enable\"  - \"--keygen-exit-on-complete\"  - \"--keygen-exit-code=1\"  - \"--keygen-num-to-gen=3\" ... now launch this with kubectl apply -f job-oneshot-failure1.yaml. let it run for a bit and then look at the pod status: $ kubectl get pod -a -l job-name=oneshot name ready status restarts age oneshot-3ddk0 0/1 crashloopbackoff 4 3m here we see that the same pod has restarted four times. kubernetes is in crashloop backoff for this pod. it is not uncommon to have a bug someplace that causes a pro‐ gram to crash as soon as it starts. in that case, kubernetes will wait a bit before job patterns | 143 restarting the pod to avoid a crash loop eating resources on the node. this is all han‐ dled local to the node by the kubelet without the job being involved at all. kill the job (kubectl delete jobs oneshot), and let’s try something else. modify the config file again and change the restartpolicy from onfailure to never. launch this with kubectl apply -f jobs-oneshot-failure2.yaml. if we let this run for a bit and then look at related pods we’ll find something interesting: $ kubectl get pod -l job-name=oneshot -a name ready status restarts age oneshot-0wm49 0/1 error 0 1m oneshot-6h9s2 0/1 error 0 39s oneshot-hkzw0 1/1 running 0 6s oneshot-k5swz 0/1 error 0 28s oneshot-m1rdw 0/1 error 0 19s oneshot-x157b 0/1 error 0 57s what we see is that we have multiple pods here that have errored out. by setting restartpolicy: never we are telling the kubelet not to restart the pod on failure, but rather just declare the pod as failed. the job object then notices and creates a replacement pod. if you aren’t careful, this’ll create a lot of “junk” in your cluster. for this reason, we suggest you use restartpolicy: onfailure so failed pods are rerun in place. clean this up with kubectl delete jobs oneshot. so far we’ve seen a program fail by exiting with a nonzero exit code. but workers can fail in other ways. specifically, they can get stuck and not make any forward progress. to help cover this case, you can use liveness probes with jobs. if the liveness probe policy determines that a pod is dead, it’ll be restarted/replaced for you. parallelism generating keys can be slow. let’s start a bunch of workers together to make key gen‐ eration faster. we’re going to use a combination of the completions and parallelism parameters. our goal is to generate 100 keys by having 10 runs of kuard with each run generating 10 keys. but we don’t want to swamp our cluster, so we’ll limit ourselves to only five pods at a time. this translates to setting completions to 10 and parallelism to 5. the config is shown in example 12-3. 144 | chapter 12: jobs example 12-3. job-parallel.yaml apiversion: batch/v1 kind: job metadata:  name: parallel  labels:  chapter: jobs spec:  parallelism: 5  completions: 10  template:  metadata:  labels:  chapter: jobs  spec:  containers:  - name: kuard  image: gcr.io/kuar-demo/kuard-amd64:blue  imagepullpolicy: always  args:  - \"--keygen-enable\"  - \"--keygen-exit-on-complete\"  - \"--keygen-num-to-gen=10\"  restartpolicy: onfailure start it up: $ kubectl apply -f job-parallel.yaml job \"parallel\" created now watch as the pods come up, do their thing, and exit. new pods are created until 10 have completed altogether. here we use the --watch flag to have kubectl stay around and list changes as they happen: $ kubectl get pods -w name ready status restarts age parallel-55tlv 1/1 running 0 5s parallel-5s7s9 1/1 running 0 5s parallel-jp7bj 1/1 running 0 5s parallel-lssmn 1/1 running 0 5s parallel-qxcxp 1/1 running 0 5s name ready status restarts age parallel-jp7bj 0/1 completed 0 26s parallel-tzp9n 0/1 pending 0 0s parallel-tzp9n 0/1 pending 0 0s parallel-tzp9n 0/1 containercreating 0 1s parallel-tzp9n 1/1 running 0 1s parallel-tzp9n 0/1 completed 0 48s parallel-x1kmr 0/1 pending 0 0s parallel-x1kmr 0/1 pending 0 0s parallel-x1kmr 0/1 containercreating 0 0s job patterns | 145 parallel-x1kmr 1/1 running 0 1s parallel-5s7s9 0/1 completed 0 1m parallel-tprfj 0/1 pending 0 0s parallel-tprfj 0/1 pending 0 0s parallel-tprfj 0/1 containercreating 0 0s parallel-tprfj 1/1 running 0 2s parallel-x1kmr 0/1 completed 0 52s parallel-bgvz5 0/1 pending 0 0s parallel-bgvz5 0/1 pending 0 0s parallel-bgvz5 0/1 containercreating 0 0s parallel-bgvz5 1/1 running 0 2s parallel-qxcxp 0/1 completed 0 2m parallel-xplw2 0/1 pending 0 1s parallel-xplw2 0/1 pending 0 1s parallel-xplw2 0/1 containercreating 0 1s parallel-xplw2 1/1 running 0 3s parallel-bgvz5 0/1 completed 0 40s parallel-55tlv 0/1 completed 0 2m parallel-lssmn 0/1 completed 0 2m feel free to study the completed jobs and check out their logs to see the fingerprints of the keys they generated. clean up by deleting the finished job object with kubectl delete job parallel. work queues a common use case for jobs is to process work from a work queue. in this scenario, some task creates a number of work items and publishes them to a work queue. a worker job can be run to process each work item until the work queue is empty (figure 12-1). figure 12-1. parallel jobs starting a work queue we start by launching a centralized work queue service. kuard has a simple memorybased work queue system built in. we will start an instance of kuard to act as a coor‐ dinator for all the work to be done. next, we create a simple replicaset to manage a singleton work queue daemon. we are using a replicaset to ensure that a new pod will get created in the face of machine failure, as shown in example 12-4. 146 | chapter 12: jobs example 12-4. rs-queue.yaml apiversion: extensions/v1beta1 kind: replicaset metadata:  labels:  app: work-queue  component: queue  chapter: jobs  name: queue spec:  replicas: 1  template:  metadata:  labels:  app: work-queue  component: queue  chapter: jobs  spec:  containers:  - name: queue  image: \"gcr.io/kuar-demo/kuard-amd64:blue\"  imagepullpolicy: always run the work queue with the following command: $ kubectl apply -f rs-queue.yaml at this point the work queue daemon should be up and running. let’s use port for‐ warding to connect to it. leave this command running in a terminal window: $ queuepod=$(kubectl get pods -l app=work-queue,component=queue \\\\  -o jsonpath=\\'{.items.metadata.name}\\') $ kubectl port-forward $queuepod 8080:8080 forwarding from 127.0.0.1:8080 -> 8080 forwarding from :8080 -> 8080 you can open your browser to http://localhost:8080 and see the kuard interface. switch to the “memq server” tab to keep an eye on what is going on. with the work queue server in place, we should expose it using a service. this will make it easy for producers and consumers to locate the work queue via dns, as example 12-5 shows. example 12-5. service-queue.yaml apiversion: v1 kind: service metadata:  labels:  app: work-queue  component: queue job patterns | 147  chapter: jobs  name: queue spec:  ports:  - port: 8080  protocol: tcp  targetport: 8080  selector:  app: work-queue  component: queue create the queue service with kubectl: $ kubectl apply -f service-queue.yaml service \"queue\" created loading up the queue we are now ready to put a bunch of work items in the queue. for the sake of simplic‐ ity we’ll just use curl to drive the api for the work queue server and insert a bunch of work items. curl will communicate to the work queue through the kubectl portforward we set up earlier, as shown in example 12-6. example 12-6. load-queue.sh # create a work queue called \\'keygen\\' curl -x put localhost:8080/memq/server/queues/keygen # create 100 work items and load up the queue. for i in work-item-{0..99}; do  curl -x post localhost:8080/memq/server/queues/keygen/enqueue \\\\  -d \"$i\" done run these commands, and you should see 100 json objects output to your terminal with a unique message identifier for each work item. you can confirm the status of the queue by looking at the “memq server” tab in the ui, or you can ask the work queue api directly: $ curl 127.0.0.1:8080/memq/server/stats {  \"kind\": \"stats\",  \"queues\": [  {  \"depth\": 100,  \"dequeued\": 0,  \"drained\": 0,  \"enqueued\": 100,  \"name\": \"keygen\"  } 148 | chapter 12: jobs  ] } now we are ready to kick off a job to consume the work queue until it’s empty. creating the consumer job this is where things get interesting! kuard is also able to act in consumer mode. we can set it up to draw work items from the work queue, create a key, and then exit once the queue is empty, as shown in example 12-7. example 12-7. job-consumers.yaml apiversion: batch/v1 kind: job metadata:  labels:  app: message-queue  component: consumer  chapter: jobs  name: consumers spec:  parallelism: 5  template:  metadata:  labels:  app: message-queue  component: consumer  chapter: jobs  spec:  containers:  - name: worker  image: \"gcr.io/kuar-demo/kuard-amd64:blue\"  imagepullpolicy: always  args:  - \"--keygen-enable\"  - \"--keygen-exit-on-complete\"  - \"--keygen-memq-server=http://queue:8080/memq/server\"  - \"--keygen-memq-queue=keygen\"  restartpolicy: onfailure here, we are telling the job to start up five pods in parallel. as the completions parameter is unset, we put the job into a worker pool mode. once the first pod exits with a zero exit code, the job will start winding down and will not start any new pods. this means that none of the workers should exit until the work is done and they are all in the process of finishing up. job patterns | 149 now, create the consumers job: $ kubectl apply -f job-consumers.yaml job \"consumers\" created once the job has been created, you can view the pods backing the job: $ kubectl get pods name ready status restarts age queue-43s87 1/1 running 0 5m consumers-6wjxc 1/1 running 0 2m consumers-7l5mh 1/1 running 0 2m consumers-hvz42 1/1 running 0 2m consumers-pc8hr 1/1 running 0 2m consumers-w20cc 1/1 running 0 2m note there are five pods running in parallel. these pods will continue to run until the work queue is empty. you can watch as it happens in the ui on the work queue server. as the queue empties, the consumer pods will exit cleanly and the consumers job will be considered complete. cleaning up using labels, we can clean up all of the stuff we created in this section: $ kubectl delete rs,svc,job -l chapter=jobs cronjobs sometimes you want to schedule a job to be run at a certain interval. to achieve this you can declare a cronjob in kubernetes, which is responsible for creating a new job object at a particular interval. the declaration of a cronjob looks like: apiversion: batch/v1beta1 kind: cronjob metadata:  name: example-cron spec:  # run every fifth hour  schedule: \"0 */5 * * *\"  jobtemplate:  spec:  template:  spec:  containers:  - name: batch-job  image: my-batch-image  restartpolicy: onfailure note the spec.schedule field, which contains the interval for the cronjob in stan‐ dard cron format. 150 | chapter 12: jobs you can save this file as cron-job.yaml, and create the cronjob with kubectl create -f cron-job.yaml. if you are interested in the current state of a cronjob, you can use kubectl describe <cron-job> to get the details. summary on a single cluster, kubernetes can handle both long-running workloads such as web applications and short-lived workloads such as batch jobs. the job abstraction allows you to model batch job patterns ranging from simple one-time tasks to parallel jobs that process many items until work has been exhausted. jobs are a low-level primitive and can be used directly for simple workloads. how‐ ever, kubernetes is built from the ground up to be extensible by higher-level objects. jobs are no exception; they can easily be used by higher-level orchestration systems to take on more complex tasks. summary | 151  chapter 13 con\\x80gmaps and secrets it is a good practice to make container images as reusable as possible. the same image should be able to be used for development, staging, and production. it is even better if the same image is general-purpose enough to be used across applications and services. testing and versioning get riskier and more complicated if images need to be recreated for each new environment. but then how do we specialize the use of that image at runtime? this is where configmaps and secrets come into play. configmaps are used to pro‐ vide configuration information for workloads. this can either be fine-grained infor‐ mation (a short string) or a composite value in the form of a file. secrets are similar to configmaps but focused on making sensitive information available to the workload. they can be used for things like credentials or tls certificates. con\\x80gmaps one way to think of a configmap is as a kubernetes object that defines a small file‐ system. another way is as a set of variables that can be used when defining the envi‐ ronment or command line for your containers. the key thing is that the configmap is combined with the pod right before it is run. this means that the container image and the pod definition itself can be reused across many apps by just changing the configmap that is used. creating con\\x80gmaps let’s jump right in and create a configmap. like many objects in kubernetes, you can create these in an immediate, imperative way, or you can create them from a manifest on disk. we’ll start with the imperative method. 153 first, suppose we have a file on disk (called my-config.txt) that we want to make avail‐ able to the pod in question, as shown in example 13-1. example 13-1. my-config.txt # this is a sample config file that i might use to configure an application parameter1 = value1 parameter2 = value2 next, let’s create a configmap with that file. we’ll also add a couple of simple key/ value pairs here. these are referred to as literal values on the command line: $ kubectl create configmap my-config \\\\  --from-file=my-config.txt \\\\  --from-literal=extra-param=extra-value \\\\  --from-literal=another-param=another-value the equivalent yaml for the configmap object we just created is: $ kubectl get configmaps my-config -o yaml apiversion: v1 data:  another-param: another-value  extra-param: extra-value  my-config.txt: |  # this is a sample config file that i might use to configure an application  parameter1 = value1  parameter2 = value2 kind: configmap metadata:  creationtimestamp: ...  name: my-config  namespace: default  resourceversion: \"13556\"  selflink: /api/v1/namespaces/default/configmaps/my-config  uid: 3641c553-f7de-11e6-98c9-06135271a273 as you can see, the configmap is really just some key/value pairs stored in an object. the interesting stuff happens when you try to use a configmap. using a con\\x80gmap there are three main ways to use a configmap: filesystem you can mount a configmap into a pod. a file is created for each entry based on the key name. the contents of that file are set to the value. 154 | chapter 13: con\\x80gmaps and secrets environment variable a configmap can be used to dynamically set the value of an environment variable. command-line argument kubernetes supports dynamically creating the command line for a container based on configmap values. let’s create a manifest for kuard that pulls all of these together, as shown in example 13-2. example 13-2. kuard-config.yaml apiversion: v1 kind: pod metadata:  name: kuard-config spec:  containers:  - name: test-container  image: gcr.io/kuar-demo/kuard-amd64:blue  imagepullpolicy: always  command:  - \"/kuard\"  - \"$(extraparam)\"  env:  - name: anotherparam  valuefrom:  configmapkeyref:  name: my-config  key: another-param  - name: extraparam  valuefrom:  configmapkeyref:  name: my-config  key: extra-param  volumemounts:  - name: config-volume  mountpath: /config  volumes:  - name: config-volume  configmap:  name: my-config  restartpolicy: never for the filesystem method, we create a new volume inside the pod and give it the name config-volume. we then define this volume to be a configmap volume and point at the configmap to mount. we have to specify where this gets mounted into the kuard container with a volumemount. in this case we are mounting it at /config. con\\x80gmaps | 155 environment variables are specified with a special valuefrom member. this refer‐ ences the configmap and the data key to use within that configmap. command-line arguments build on environment variables. kubernetes will perform the correct substitution with a special $(<env-var-name>) syntax. run this pod, and let’s port forward to examine how the app sees the world: $ kubectl apply -f kuard-config.yaml $ kubectl port-forward kuard-config 8080 now point your browser at http://localhost:8080. we can look at how we’ve injected configuration values into the program in all three ways. click the “server env” tab on the left. this will show the command line that the app was launched with along with its environment, as shown in figure 13-1. figure 13-1. kuard showing its environment 156 | chapter 13: con\\x80gmaps and secrets here we can see that we’ve added two environment variables (anotherparam and extraparam) whose values are set via the configmap. furthermore, we’ve added an argument to the command line of kuard based on the extraparam value. next, click the “file system browser” tab (figure 13-2). this lets you explore the file‐ system as the application sees it. you should see an entry called /config. this is a vol‐ ume created based on our configmap. if you navigate into that, you’ll see that a file has been created for each entry of the configmap. you’ll also see some hidden files (prepended with ..) that are used to do a clean swap of new values when the config‐ map is updated. figure 13-2. the /config directory as seen through kuard secrets while configmaps are great for most configuration data, there is certain data that is extra-sensitive. this can include passwords, security tokens, or other types of private secrets | 157 keys. collectively, we call this type of data “secrets.” kubernetes has native support for storing and handling this data with care. secrets enable container images to be created without bundling sensitive data. this allows containers to remain portable across environments. secrets are exposed to pods via explicit declaration in pod manifests and the kubernetes api. in this way, the kubernetes secrets api provides an application-centric mechanism for exposing sensitive configuration information to applications in a way that’s easy to audit and leverages native os isolation primitives. by default, kubernetes secrets are stored in plain text in the etcd storage for the cluster. depending on your requirements, this may not be sufficient security for you. in particular, anyone who has cluster administration rights in your cluster will be able to read all of the secrets in the cluster. in recent versions of kubernetes, sup‐ port has been added for encrypting the secrets with a user-supplied key, generally integrated into a cloud key store. additionally, most cloud key stores have integration with kubernetes flexible volumes, enabling you to skip kubernetes secrets entirely and rely exclu‐ sively on the cloud provider’s key store. all of these options should provide you with sufficient tools to craft a security profile that suits your needs. the remainder of this section will explore how to create and manage kubernetes secrets, and also lay out best practices for exposing secrets to pods that require them. creating secrets secrets are created using the kubernetes api or the kubectl command-line tool. secrets hold one or more data elements as a collection of key/value pairs. in this section we will create a secret to store a tls key and certificate for the kuard application that meets the storage requirements listed previously. the kuard container image does not bundle a tls certificate or key. this allows the kuard container to remain portable across environments and distributable through public docker repositories. the first step in creating a secret is to obtain the raw data we want to store. the tls key and certificate for the kuard application can be downloaded by running the fol‐ lowing commands: $ curl -o kuard.crt https://storage.googleapis.com/kuar-demo/kuard.crt $ curl -o kuard.key https://storage.googleapis.com/kuar-demo/kuard.key 158 | chapter 13: con\\x80gmaps and secrets these certificates are shared with the world and they provide no actual security. please do not use them except as a learning tool in these examples. with the kuard.crt and kuard.key files stored locally, we are ready to create a secret. create a secret named kuard-tls using the create secret command: $ kubectl create secret generic kuard-tls \\\\  --from-file=kuard.crt \\\\  --from-file=kuard.key the kuard-tls secret has been created with two data elements. run the following command to get details: $ kubectl describe secrets kuard-tls name: kuard-tls namespace: default labels: <none> annotations: <none> type: opaque data ==== kuard.crt: 1050 bytes kuard.key: 1679 bytes with the kuard-tls secret in place, we can consume it from a pod by using a secrets volume. consuming secrets secrets can be consumed using the kubernetes rest api by applications that know how to call that api directly. however, our goal is to keep applications portable. not only should they run well in kubernetes, but they should run, unmodified, on other platforms. instead of accessing secrets through the api server, we can use a secrets volume. secrets volumes secret data can be exposed to pods using the secrets volume type. secrets volumes are managed by the kubelet and are created at pod creation time. secrets are stored on tmpfs volumes (aka ram disks), and as such are not written to disk on nodes. each data element of a secret is stored in a separate file under the target mount point specified in the volume mount. the kuard-tls secret contains two data elements: secrets | 159 kuard.crt and kuard.key. mounting the kuard-tls secrets volume to /tls results in the following files: /tls/kuard.crt /tls/kuard.key the pod manifest in example 13-3 demonstrates how to declare a secrets volume, which exposes the kuard-tls secret to the kuard container under /tls. example 13-3. kuard-secret.yaml apiversion: v1 kind: pod metadata:  name: kuard-tls spec:  containers:  - name: kuard-tls  image: gcr.io/kuar-demo/kuard-amd64:blue  imagepullpolicy: always  volumemounts:  - name: tls-certs  mountpath: \"/tls\"  readonly: true  volumes:  - name: tls-certs  secret:  secretname: kuard-tls create the kuard-tls pod using kubectl and observe the log output from the run‐ ning pod: $ kubectl apply -f kuard-secret.yaml connect to the pod by running: $ kubectl port-forward kuard-tls 8443:8443 now navigate your browser to https://localhost:8443. you should see some invalid cer‐ tificate warnings as this is a self-signed certificate for kuard.example.com. if you navi‐ gate past this warning, you should see the kuard server hosted via https. use the “file system browser” tab to find the certificates on disk. private docker registries a special use case for secrets is to store access credentials for private docker regis‐ tries. kubernetes supports using images stored on private registries, but access to those images requires credentials. private images can be stored across one or more private registries. this presents a challenge for managing credentials for each private registry on every possible node in the cluster. 160 | chapter 13: con\\x80gmaps and secrets image pull secrets leverage the secrets api to automate the distribution of private reg‐ istry credentials. image pull secrets are stored just like normal secrets but are con‐ sumed through the spec.imagepullsecrets pod specification field. use the create secret docker-registry to create this special kind of secret: $ kubectl create secret docker-registry my-image-pull-secret \\\\  --docker-username=<username> \\\\  --docker-password=<password> \\\\  --docker-email=<email-address> enable access to the private repository by referencing the image pull secret in the pod manifest file, as shown in example 13-4. example 13-4. kuard-secret-ips.yaml apiversion: v1 kind: pod metadata:  name: kuard-tls spec:  containers:  - name: kuard-tls  image: gcr.io/kuar-demo/kuard-amd64:blue  imagepullpolicy: always  volumemounts:  - name: tls-certs  mountpath: \"/tls\"  readonly: true  imagepullsecrets:  - name: my-image-pull-secret  volumes:  - name: tls-certs  secret:  secretname: kuard-tls if you are repeatedly pulling from the same registry, you can add the secrets to the default service account associated with each pod to avoid having to specify the secrets in every pod you create. naming constraints the key names for data items inside of a secret or configmap are defined to map to valid environment variable names. they may begin with a dot followed by a letter or number. following characters include dots, dashes, and underscores. dots cannot be repeated and dots and underscores or dashes cannot be adjacent to each other. more formally, this means that they must conform to the regular expression naming constraints | 161 ^[.[?[a-zaz0-9[([.[?[a-za-z0-9[+[-a-za-z0-9[?)*$. some examples of valid and invalid names for configmaps and secrets are given in table 13-1. table 13-1. configmap and secret key examples valid key name invalid key name .authtoken token..properties key.pem auth file.json configfile password.txt when selecting a key name, consider that these keys can be exposed to pods via a volume mount. pick a name that is going to make sense when specified on a command line or in a config file. storing a tls key as key.pem is more clear than tls-key when configuring applications to access secrets. configmap data values are simple utf-8 text specified directly in the manifest. as of kubernetes 1.6, configmaps are unable to store binary data. secret data values hold arbitrary data encoded using base64. the use of base64 encoding makes it possible to store binary data. this does, however, make it more difficult to manage secrets that are stored in yaml files as the base64-encoded value must be put in the yaml. note that the maximum size for a configmap or secret is 1 mb. managing con\\x80gmaps and secrets secrets and configmaps are managed through the kubernetes api. the usual create, delete, get, and describe commands work for manipulating these objects. listing you can use the kubectl get secrets command to list all secrets in the current namespace: $ kubectl get secrets name type data age default-token-f5jq2 kubernetes.io/service-account-token 3 1h kuard-tls opaque 2 20m similarly, you can list all of the configmaps in a namespace: $ kubectl get configmaps name data age my-config 3 1m 162 | chapter 13: con\\x80gmaps and secrets kubectl describe can be used to get more details on a single object: $ kubectl describe configmap my-config name: my-config namespace: default labels: <none> annotations: <none> data ==== another-param: 13 bytes extra-param: 11 bytes my-config.txt: 116 bytes finally, you can see the raw data (including values in secrets!) with something like kubectl get configmap my-config -o yaml or kubectl get secret kuard-tls -o yaml. creating the easiest way to create a secret or a configmap is via kubectl create secret generic or kubectl create configmap. there are a variety of ways to specify the data items that go into the secret or configmap. these can be combined in a single command: --from-file=<filename> load from the file with the secret data key the same as the filename. --from-file=<key>=<filename> load from the file with the secret data key explicitly specified. --from-file=<directory> load all the files in the specified directory where the filename is an acceptable key name. --from-literal=<key>=<value> use the specified key/value pair directly. updating you can update a configmap or secret and have it reflected in running programs. there is no need to restart if the application is configured to reread configuration val‐ ues. this is a rare feature but might be something you put in your own applications. the following are three ways to update configmaps or secrets. managing con\\x80gmaps and secrets | 163 update from \\x80le if you have a manifest for your configmap or secret, you can just edit it directly and push a new version with kubectl replace -f <filename>. you can also use kubectl apply -f <filename> if you previously created the resource with kubectl apply. due to the way that datafiles are encoded into these objects, updating a configuration can be a bit cumbersome as there is no provision in kubectl to load data from an external file. the data must be stored directly in the yaml manifest. the most common use case is when the configmap is defined as part of a directory or list of resources and everything is created and updated together. oftentimes these manifests will be checked into source control. it is generally a bad idea to check secret yaml files into source control. it is too easy to push these files someplace public and leak your secrets. recreate and update if you store the inputs into your configmaps or secrets as separate files on disk (as opposed to embedded into yaml directly), you can use kubectl to recreate the man‐ ifest and then use it to update the object. this will look something like this: $ kubectl create secret generic kuard-tls \\\\  --from-file=kuard.crt --from-file=kuard.key \\\\  --dry-run -o yaml | kubectl replace -f - this command line first creates a new secret with the same name as our existing secret. if we just stopped there, the kubernetes api server would return an error complaining that we are trying to create a secret that already exists. instead, we tell kubectl not to actually send the data to the server but instead to dump the yaml that it would have sent to the api server to stdout. we then pipe that to kubectl replace and use -f - to tell it to read from stdin. in this way, we can update a secret from files on disk without having to manually base64-encode data. edit current version the final way to update a configmap is to use kubectl edit to bring up a version of the configmap in your editor so you can tweak it (you could also do this with a secret, but you’d be stuck managing the base64 encoding of values on your own): $ kubectl edit configmap my-config 164 | chapter 13: con\\x80gmaps and secrets you should see the configmap definition in your editor. make your desired changes and then save and close your editor. the new version of the object will be pushed to the kubernetes api server. live updates once a configmap or secret is updated using the api, it’ll be automatically pushed to all volumes that use that configmap or secret. it may take a few seconds, but the file listing and contents of the files, as seen by kuard, will be updated with these new val‐ ues. using this live update feature you can update the configuration of applications without restarting them. currently there is no built-in way to signal an application when a new version of a configmap is deployed. it is up to the application (or some helper script) to look for the config files to change and reload them. using the file browser in kuard (accessed through kubectl port-forward) is a great way to interactively play with dynamically updating secrets and configmaps. summary configmaps and secrets are a great way to provide dynamic configuration in your application. they allow you to create a container image (and pod definition) once and reuse it in different contexts. this can include using the exact same image as you move from dev to staging to production. it can also include using a single image across multiple teams and services. separating configuration from application code will make your applications more reliable and reusable. summary | 165  chapter 14 role-based access control for kubernetes at this point, nearly every kubernetes cluster you encounter has role-based access control (rbac) enabled. so you likely have at least partially encountered rbac before. perhaps you initially couldn’t access your cluster until you used some magical incantation to add a rolebinding to map a user to a role. however, even though you may have had some exposure to rbac, you may not have had a great deal of experi‐ ence understanding rbac in kubernetes, what it is for, and how to use it successfully. that is the subject of this chapter. rbac was introduced into kubernetes with version 1.5 and became generally avail‐ able in kubernetes 1.8. role-based access control provides a mechanism for restrict‐ ing both access to and actions on kubernetes apis to ensure that only appropriate users have access to apis in the cluster. rbac is a critical component to both harden access to the kubernetes cluster where you are deploying your application and (possi‐ bly more importantly) prevent unexpected accidents where one person in the wrong namespace mistakenly takes down production when they think they are destroying their test cluster. multitenant security in kubernetes is a complex, multifaceted topic worthy of its own volume. while rbac can be quite useful in lim‐ iting access to the kubernetes api, it’s important to remember that anyone who can run arbitrary code inside the kubernetes cluster can effectively obtain root privileges on the entire cluster. there are approaches that you can take to make such attacks harder and more expensive, and a correct rbac setup is part of this defense. but if you are focused on hostile multitenant security, do not believe that rbac by itself is sufficient to protect you. you must isolate the pods running in your cluster to provide effective multitenant security. generally this is done with a hypervisor isolated container, or some sort of container sandbox, or both. 167 before we dive into the details of rbac in kubernetes, it’s valuable to have a highlevel understanding of rbac as a concept, as well as authentication and authoriza‐ tion more generally. every request to kubernetes is first authenticated. authentication provides the iden‐ tity of the caller issuing the request. it could be as simple as saying that the request is unauthenticated, or it could integrate deeply with a pluggable authentication provider (e.g., azure active directory) to establish an identity within that third-party system. interestingly enough, kubernetes does not have a built-in identity store, focusing instead on integrating other identity sources within itself. once users have been properly identified, the authorization phase determines whether they are authorized to perform the request. authorization is a combination of the identity of the user, the resource (effectively the http path), and the verb or action the user is attempting to perform. if the particular user is authorized for per‐ forming that action on that resource, then the request is allowed to proceed. other‐ wise, an http 403 error is returned. more details of this process are given in the following sections. role-based access control to properly manage access in kubernetes, it’s critical to understand how identity, roles, and role bindings interact to control who can do what with what resources. at first, rbac can seem like a challenge to understand, with a series of interconnected, abstract concepts; but once understood, managing cluster access is straightforward and safe. identity in kubernetes every request that comes to kubernetes is associated with some identity. even a request with no identity is associated with the system:unauthenticated group. kubernetes makes a distinction between user identities and service account identities. service accounts are created and managed by kubernetes itself and are generally asso‐ ciated with components running inside the cluster. user accounts are all other accounts associated with actual users of the cluster, and often include automation like continuous delivery as a service that runs outside of the cluster. kubernetes uses a generic interface for authentication providers. each of the provid‐ ers supplies a username and optionally the set of groups to which the user belongs. kubernetes supports a number of different authentication providers, including: • http basic authentication (largely deprecated) • x509 client certificates 168 | chapter 14: role-based access control for kubernetes • static token files on the host • cloud authentication providers like azure active directory and aws identity and access management (iam) • authentication webhooks while most managed kubernetes installations configure authentication for you, if you are deploying your own authentication you will need to configure flags on the kubernetes api server appropriately. understanding roles and role bindings identity is just the beginning of authorization in kubernetes. once the system knows the identity of the request, it needs to determine if the request is authorized for that user. to achieve this, it uses the general concept of roles and role bindings. a role is a set of abstract capabilities. for example, the appdev role might represent the ability to create pods and services. a role binding is an assignment of a role to one or more identities. thus, binding the appdev role to the user identity alice indicates that alice has the ability to create pods and services. roles and role bindings in kubernetes in kubernetes there are two pairs of related resources that represent roles and role bindings. one pair applies to just a namespace (role and rolebinding) while the other pair applies across the cluster (clusterrole and clusterrolebinding). let’s examine role and rolebinding first. role resources are namespaced, and repre‐ sent capabilities within that single namespace. you cannot use namespaced roles for non-namespaced resources (e.g., customresourcedefinitions), and binding a role binding to a role only provides authorization within the kubernetes namespace that contains both the role and the roledefinition. as a concrete example, here is a simple role that gives an identity the ability to create and modify pods and services: kind: role apiversion: rbac.authorization.k8s.io/v1 metadata:  namespace: default  name: pod-and-services rules: - apigroups:   resources:   verbs:  role-based access control | 169 to bind this role to the user alice, we need to create a rolebinding that looks as follows. this role binding also binds the group mydevs to the same role: apiversion: rbac.authorization.k8s.io/v1 kind: rolebinding metadata:  namespace: default  name: pods-and-services subjects: - apigroup: rbac.authorization.k8s.io  kind: user  name: alice - apigroup: rbac.authorization.k8s.io  kind: group  name: mydevs roleref:  apigroup: rbac.authorization.k8s.io  kind: role  name: pod-and-services of course, sometimes you want to create a role that applies to the entire cluster, or you want to limit access to cluster-level resources. to achieve this, you use the clusterrole and clusterrolebinding resources. they are largely identical to their namespaced peers, but with larger scope. verbs for kubernetes roles roles are defined in terms of both a resource (e.g., “pods”) and a verb that describes an action that can be performed on that resource. the verbs correspond roughly to http methods. the commonly used verbs in kubernetes rbac are listed in table 14-1. table 14-1. common kubernetes rbac verbs verb http method description create post create a new resource. delete delete delete an existing resource. get get get a resource. list get list a collection of resources. patch patch modify an existing resource via a partial change. update put modify an existing resource via a complete object. watch get watch for streaming updates to a resource. proxy get connect to resource via a streaming websocket proxy. 170 | chapter 14: role-based access control for kubernetes using built-in roles of course, designing your own roles can be complicated and time-consuming. fur‐ thermore, kubernetes has a large number of well-known system identities (e.g., a scheduler) that require a known set of capabilities. consequently, kubernetes has a large number of built-in cluster roles. you can view these by running: $ kubectl get clusterroles while most of these built-in roles are for system utilities, four are designed for generic end users: • the cluster-admin role provides complete access to the entire cluster. • the admin role provides complete access to a complete namespace. • the edit role allows an end user to modify things in a namespace. • the view role allows for read-only access to a namespace. most clusters already have numerous clusterrole bindings set up, and you can view these bindings with kubectl get clusterrolebindings. auto-reconciliation of built-in roles when the kubernetes api server starts up, it automatically installs a number of default clusterroles that are defined in the code of the api server itself. this means that if you modify any built-in cluster role, those modifications are transient. whenever the api server is restarted (e.g., for an upgrade) your changes will be overwritten. to prevent this from happening, before you make any other modifications you need to add the rbac.authorization.kubernetes.io/autoupdate annotation with a value of false to the built-in clusterrole resource. if this annotation is set to false, the api server will not overwrite the modified clusterrole resource. by default, the kubernetes api server installs a cluster role that allows system:unauthenticated users access to the api server’s api discovery endpoint. for any cluster exposed to a hostile envi‐ ronment (e.g., the public internet) this is a bad idea, and there has been at least one serious security vulnerability via this exposure. consequently, if you are running a kubernetes service on the pub‐ lic internet or an other hostile environment, you should ensure that the --anonymous-auth=false flag is set on your api server. role-based access control | 171 techniques for managing rbac managing rbac for a cluster can be complicated and frustrating. possibly more con‐ cerning is that misconfigured rbac can lead to security issues. fortunately, there are several tools and techniques that make managing rbac easier. testing authorization with can-i the first useful tool is the auth can-i command for kubectl. this tool is very useful for testing if a particular user can do a particular action. you can use can-i to validate configuration settings as you configure your cluster, or you can ask users to use the tool to validate their access when filing errors or bug reports. in its simplest usage, the can-i command takes a verb and a resource. for example, this command will indicate if the current kubectl user is authorized to create pods: $ kubectl auth can-i create pods you can also test subresources like logs or port forwarding with the --subresource command-line flag: $ kubectl auth can-i get pods --subresource=logs managing rbac in source control like all resources in kubernetes, rbac resources are modeled using json or yaml. given this text-based representation it makes sense to store these resources in version control. indeed, the strong need for audit, accountability, and rollback for changes to rbac policy means that version control for rbac resources is essential. fortunately, the kubectl command-line tool comes with a reconcile command that operates somewhat like kubectl apply and will reconcile a text-based set of roles and role bindings with the current state of the cluster. you can run: $ kubectl auth reconcile -f some-rbac-config.yaml and the data in the file will be reconciled with the cluster. if you want to see changes before they are made, you can add the --dry-run flag to the command to print but not submit the changes. advanced topics once you orient to the basics of role-based access control it is relatively easy to man‐ age access to a kubernetes cluster, but when managing a large number of users or 172 | chapter 14: role-based access control for kubernetes roles, there are additional advanced capabilities you can use to manage rbac at scale. aggregating clusterroles sometimes you want to be able to define roles that are combinations of other roles. one option would be to simply clone all of the rules from one clusterrole into another clusterrole, but this is complicated and error-prone, since changes to one clusterrole aren’t automatically reflected in the other. instead, kubernetes rbac supports the usage of an aggregation rule to combine multiple roles together in a new role. this new role combines all of the capabilities of all of the aggregate roles together, and any changes to any of the constituent subroles will automatically be propogated back into the aggregate role. like with all other aggregations or groupings in kubernetes, the clusterroles to be aggregated are specified using label selectors. in this particular case, the aggregation rule field in the clusterrole resource contains a clusterroleselector field, which in turn is a label selector. all clusterrole resources that match this selector are dynamically aggregated into the rules array in the aggregate clusterrole resource. a best practice for managing clusterrole resources is to create a number of finegrained cluster roles and then aggregate them together to form higher-level or broadly defined cluster roles. this is how the built-in cluster roles are defined. for example, you can see that the built-in edit role looks like this: apiversion: rbac.authorization.k8s.io/v1 kind: clusterrole metadata:  name: edit  ... aggregationrule:  clusterroleselectors:  - matchlabels:  rbac.authorization.k8s.io/aggregate-to-edit: \"true\" ... this means that the edit role is defined to be the aggregate of all clusterrole objects that have a label of rbac.authorization.k8s.io/aggregate-to-edit set to true. using groups for bindings when managing a large number of people in different organizations with similar access to the cluster, it’s generally a best practice to use groups to manage the roles that define access to the cluster, rather than individually adding bindings to specific identities. when you bind a group to a clusterrole or a namespace role, anyone advanced topics | 173 who is a member of that group gains access to the resources and verbs defined by that role. thus, to enable any individual to gain access to the group’s role, that individual needs to be added to the group. there are several reasons why using groups is a preferred strategy for managing access at scale. the first is that in any large organization, access to the cluster is defined in terms of the team that someone is part of, rather than their specific iden‐ tity. for example, someone who is part of the frontend operations team will need access to both view and edit the resources associated with the frontends, while they may only need view/read access to resources associated with the backend. granting privileges to a group makes the association between the specific team and its capabili‐ ties clear. when granting roles to individuals, it’s much harder to clearly understand the appropriate (i.e., minimal) privileges required for each team, especially when an individual may be part of multiple different teams. additional benefits of binding roles to groups instead of individuals are simplicity and consistency. when someone joins or leaves a team, it is straightforward to simply add or remove them to or from a group in a single operation. if you instead have to remove a number of different role bindings for their identity, you may either remove too few or too many bindings, resulting in unnecessary access or preventing them from being able to do necessary actions. additionally, because there is only a single set of group role bindings to maintain, you don’t have to do lots of work to ensure that all team members have the same, consistent set of permissions. furthermore, many group systems enable “just in time” (jit) access such that people are only temporarily added to a group in response to an event (say, a page in the mid‐ dle of the night) rather than having standing access. this means that you can both audit who had access at any particular time and ensure that, in general, even a com‐ promised identity can’t have access to your production infrastructure. finally, in many cases these same groups are used to manage access to other resour‐ ces, from facilities to documents and machine logins. thus, using the same groups for access control to kubernetes dramatically simplifies management. to bind a group to a clusterrole you use a group kind for the subject in the binding: ... subjects: - apigroup: rbac.authorization.k8s.io  kind: group  name: my-great-groups-name ... in kubernetes, groups are supplied by authentication providers. there is no strong notion of a group within kubernetes, only that an identity can be part of one or more groups, and those groups can be associated with a role or clusterrole via a binding. 174 | chapter 14: role-based access control for kubernetes summary when you begin with a small cluster and a small team, it is sufficient to have every member of the team have equivalent access to the cluster. but as teams grow and products become more mission critical, limiting access to parts of the cluster is cru‐ cial. in a well-designed cluster, access is limited to the minimal set of people and capabilities needed to efficiently manage the applications in the cluster. understand‐ ing how kubernetes implements rbac and how those capabilities can be used to control access to your cluster is important for both developers and cluster adminis‐ trators. as with building out testing infrastructure, best practice is to set up proper rbac earlier rather than later. it’s far easier to start with the right foundation than to try to retrofit it later on. hopefully, the information in this chapter has provided the necessary grounding for adding rbac to your cluster. summary | 175  chapter 15 integrating storage solutions and kubernetes in many cases, decoupling state from applications and building your microservices to be as stateless as possible results in maximally reliable, manageable systems. however, nearly every system that has any complexity has state in the system some‐ where, from the records in a database to the index shards that serve results for a web search engine. at some point, you have to have data stored somewhere. integrating this data with containers and container orchestration solutions is often the most complicated aspect of building a distributed system. this complexity largely stems from the fact that the move to containerized architectures is also a move toward decoupled, immutable, and declarative application development. these pat‐ terns are relatively easy to apply to stateless web applications, but even “cloud-native” storage solutions like cassandra or mongodb involve some sort of manual or imper‐ ative steps to set up a reliable, replicated solution. as an example of this, consider setting up a replicaset in mongodb, which involves deploying the mongo daemon and then running an imperative command to identify the leader, as well as the participants in the mongo cluster. of course, these steps can be scripted, but in a containerized world it is difficult to see how to integrate such commands into a deployment. likewise, even getting dns-resolvable names for indi‐ vidual containers in a replicated set of containers is challenging. additional complexity comes from the fact that there is data gravity. most container‐ ized systems aren’t built in a vacuum; they are usually adapted from existing systems deployed onto vms, and these systems likely include data that has to be imported or migrated. 177 finally, evolution to the cloud often means that storage is an externalized cloud ser‐ vice, and in that context it can never really exist inside of the kubernetes cluster. this chapter covers a variety of approaches for integrating storage into containerized microservices in kubernetes. first, we cover how to import existing external storage solutions (either cloud services or running on vms) into kubernetes. next, we explore how to run reliable singletons inside of kubernetes that enable you to have an environment that largely matches the vms where you previously deployed storage solutions. finally, we cover statefulsets, which are still under development but repre‐ sent the future of stateful workloads in kubernetes. importing external services in many cases, you have an existing machine running in your network that has some sort of database running on it. in this situation you may not want to immediately move that database into containers and kubernetes. perhaps it is run by a different team, or you are doing a gradual move, or the task of migrating the data is simply more trouble than it’s worth. regardless of the reasons for staying put, this legacy server and service are not going to move into kubernetes—but it’s still worthwhile to represent this server in kuber‐ netes. when you do this, you get to take advantage of all of the built-in naming and service-discovery primitives provided by kubernetes. additionally, this enables you to configure all your applications so that it looks like the database that is running on a machine somewhere is actually a kubernetes service. this means that it is trivial to replace it with a database that is a kubernetes service. for example, in production, you may rely on your legacy database that is running on a machine, but for continu‐ ous testing you may deploy a test database as a transient container. since it is created and destroyed for each test run, data persistence isn’t important in the continuous testing case. representing both databases as kubernetes services enables you to main‐ tain identical configurations in both testing and production. high fidelity between test and production ensures that passing tests will lead to successful deployment in production. to see concretely how you maintain high fidelity between development and produc‐ tion, remember that all kubernetes objects are deployed into namespaces. imagine that we have test and production namespaces defined. the test service is imported using an object like: kind: service metadata:  name: my-database  # note \\'test\\' namespace here  namespace: test ... 178 | chapter 15: integrating storage solutions and kubernetes the production service looks the same, except it uses a different namespace: kind: service metadata:  name: my-database  # note \\'prod\\' namespace here  namespace: prod ... when you deploy a pod into the test namespace and it looks up the service named my-database, it will receive a pointer to my-database.test.svc.cluster.internal, which in turn points to the test database. in contrast, when a pod deployed in the prod namespace looks up the same name (my-database) it will receive a pointer to my-database.prod.svc.cluster.internal, which is the production database. thus, the same service name, in two different namespaces, resolves to two different serv‐ ices. for more details on how this works, see chapter 7. the following techniques all use database or other storage services, but these approaches can be used equally well with other services that aren’t running inside your kubernetes cluster. services without selectors when we first introduced services, we talked at length about label queries and how they were used to identify the dynamic set of pods that were the backends for a par‐ ticular service. with external services, however, there is no such label query. instead, you generally have a dns name that points to the specific server running the data‐ base. for our example, let’s assume that this server is named database.company.com. to import this external database service into kubernetes, we start by creating a ser‐ vice without a pod selector that references the dns name of the database server (example 15-1). example 15-1. dns-service.yaml kind: service apiversion: v1 metadata:  name: external-database spec:  type: externalname  externalname: database.company.com when a typical kubernetes service is created, an ip address is also created and the kubernetes dns service is populated with an a record that points to that ip address. when you create a service of type externalname, the kubernetes dns service is importing external services | 179 instead populated with a cname record that points to the external name you speci‐ fied (database.company.com in this case). when an application in the cluster does a dns lookup for the hostname external-database.svc.default.cluster, the dns protocol aliases that name to database.company.com. this then resolves to the ip address of your external database server. in this way, all containers in kubernetes believe that they are talking to a service that is backed with other containers, when in fact they are being redirected to the external database. note that this is not restricted to databases you are running on your own infrastruc‐ ture. many cloud databases and other services provide you with a dns name to use when accessing the database (e.g., my-database.databases.cloudprovider.com). you can use this dns name as the externalname. this imports the cloud-provided database into the namespace of your kubernetes cluster. sometimes, however, you don’t have a dns address for an external database service, just an ip address. in such cases, it is still possible to import this service as a kuber‐ netes service, but the operation is a little different. first, you create a service without a label selector, but also without the externalname type we used before (example 15-2). example 15-2. external-ip-service.yaml kind: service apiversion: v1 metadata:  name: external-ip-database at this point, kubernetes will allocate a virtual ip address for this service and popu‐ late an a record for it. however, because there is no selector for the service, there will be no endpoints populated for the load balancer to redirect traffic to. given that this is an external service, the user is responsible for populating the end‐ points manually with an endpoints resource (example 15-3). example 15-3. external-ip-endpoints.yaml kind: endpoints apiversion: v1 metadata:  name: external-ip-database subsets:  - addresses:  - ip: 192.168.0.1  ports:  - port: 3306 180 | chapter 15: integrating storage solutions and kubernetes if you have more than one ip address for redundancy, you can repeat them in the addresses array. once the endpoints are populated, the load balancer will start redi‐ recting traffic from your kubernetes service to the ip address endpoint(s). because the user has assumed responsibility for keeping the ip address of the server up to date, you need to either ensure that it never changes or make sure that some automated process updates the endpoints record. limitations of external services: health checking external services in kubernetes have one significant restriction: they do not perform any health checking. the user is responsible for ensuring that the endpoint or dns name supplied to kubernetes is as reliable as necessary for the application. running reliable singletons the challenge of running storage solutions in kubernetes is often that primitives like replicaset expect that every container is identical and replaceable, but for most stor‐ age solutions this isn’t the case. one option to address this is to use kubernetes primi‐ tives, but not attempt to replicate the storage. instead, simply run a single pod that runs the database or other storage solution. in this way the challenges of running replicated storage in kubernetes don’t occur, since there is no replication. at first blush, this might seem to run counter to the principles of building reliable distributed systems, but in general, it is no less reliable than running your database or storage infrastructure on a single virtual or physical machine, which is how many people currently have built their systems. indeed, in reality, if you structure the sys‐ tem properly the only thing you are sacrificing is potential downtime for upgrades or in case of machine failure. while for large-scale or mission-critical systems this may not be acceptable, for many smaller-scale applications this kind of limited downtime is a reasonable trade-off for the reduced complexity. if this is not true for you, feel free to skip this section and either import existing services as described in the previ‐ ous section, or move on to kubernetes-native statefulsets, described in the following section. for everyone else, we’ll review how to build reliable singletons for data storage. running a mysql singleton in this section, we’ll describe how to run a reliable singleton instance of the mysql database as a pod in kubernetes, and how to expose that singleton to other applica‐ tions in the cluster. to do this, we are going to create three basic objects: running reliable singletons | 181 • a persistent volume to manage the lifespan of the on-disk storage independently from the lifespan of the running mysql application • a mysql pod that will run the mysql application • a service that will expose this pod to other containers in the cluster in chapter 5 we described persistent volumes, but a quick review makes sense. a per‐ sistent volume is a storage location that has a lifetime independent of any pod or con‐ tainer. this is very useful in the case of persistent storage solutions where the on-disk representation of a database should survive even if the containers running the data‐ base application crash, or move to different machines. if the application moves to a different machine, the volume should move with it, and data should be preserved. separating the data storage out as a persistent volume makes this possible. to begin, we’ll create a persistent volume for our mysql database to use. this exam‐ ple uses nfs for maximum portability, but kubernetes supports many different per‐ sistent volume drive types. for example, there are persistent volume drivers for all major public cloud providers, as well as many private cloud providers. to use these solutions, simply replace nfs with the appropriate cloud provider volume type (e.g., azure, awselasticblockstore, or gcepersistentdisk). in all cases, this change is all you need. kubernetes knows how to create the appropriate storage disk in the respec‐ tive cloud provider. this is a great example of how kubernetes simplifies the develop‐ ment of reliable distributed systems. example 15-4 shows the persistentvolume object. example 15-4. nfs-volume.yaml apiversion: v1 kind: persistentvolume metadata:  name: database  labels:  volume: my-volume spec:  accessmodes:  - readwritemany  capacity:  storage: 1gi  nfs:  server: 192.168.0.1  path: \"/exports\" this defines an nfs persistentvolume object with 1 gb of storage space. we can create this persistent volume as usual with: $ kubectl apply -f nfs-volume.yaml 182 | chapter 15: integrating storage solutions and kubernetes now that we have a persistent volume created, we need to claim that persistent vol‐ ume for our pod. we do this with a persistentvolumeclaim object (example 15-5). example 15-5. nfs-volume-claim.yaml kind: persistentvolumeclaim apiversion: v1 metadata:  name: database spec:  accessmodes:  - readwritemany  resources:  requests:  storage: 1gi  selector:  matchlabels:  volume: my-volume the selector field uses labels to find the matching volume we defined previously. this kind of indirection may seem overly complicated, but it has a purpose—it serves to isolate our pod definition from our storage definition. you can declare volumes directly inside a pod specification, but this locks that pod specification to a particular volume provider (e.g., a specific public or private cloud). by using volume claims, you can keep your pod specifications cloud-agnostic; simply create different volumes, specific to the cloud, and use a persistentvolumeclaim to bind them together. fur‐ thermore, in many cases, the persistent volume controller will actually automatically create a volume for you—there are more details of this process in the following section. now that we’ve claimed our volume, we can use a replicaset to construct our single‐ ton pod. it might seem odd that we are using a replicaset to manage a single pod, but it is necessary for reliability. remember that once scheduled to a machine, a bare pod is bound to that machine forever. if the machine fails, then any pods that are on that machine that are not being managed by a higher-level controller like a replicaset vanish along with the machine and are not rescheduled elsewhere. consequently, to ensure that our database pod is rescheduled in the presence of machine failures, we use the higher-level replicaset controller, with a replica size of one, to manage our database (example 15-6). example 15-6. mysql-replicaset.yaml apiversion: extensions/v1 kind: replicaset metadata:  name: mysql running reliable singletons | 183  # labels so that we can bind a service to this pod  labels:  app: mysql spec:  replicas: 1  selector:  matchlabels:  app: mysql  template:  metadata:  labels:  app: mysql  spec:  containers:  - name: database  image: mysql  resources:  requests:  cpu: 1  memory: 2gi  env:  # environment variables are not a best practice for security,  # but we\\'re using them here for brevity in the example.  # see chapter 11 for better options.  - name: mysqlrootpassword  value: some-password-here  livenessprobe:  tcpsocket:  port: 3306  ports:  - containerport: 3306  volumemounts:  - name: database  # /var/lib/mysql is where mysql stores its databases  mountpath: \"/var/lib/mysql\"  volumes:  - name: database  persistentvolumeclaim:  claimname: database once we create the replicaset it will, in turn, create a pod running mysql using the persistent disk we originally created. the final step is to expose this as a kubernetes service (example 15-7). example 15-7. mysql-service.yaml apiversion: v1 kind: service metadata:  name: mysql spec: 184 | chapter 15: integrating storage solutions and kubernetes  ports:  - port: 3306  protocol: tcp  selector:  app: mysql now we have a reliable singleton mysql instance running in our cluster and exposed as a service named mysql, which we can access at the full domain name mysql.svc.default.cluster. similar instructions can be used for a variety of data stores, and if your needs are sim‐ ple and you can survive limited downtime in the face of a machine failure or when you need to upgrade the database software, a reliable singleton may be the right approach to storage for your application. dynamic volume provisioning many clusters also include dynamic volume provisioning. with dynamic volume pro‐ visioning, the cluster operator creates one or more storageclass objects. example 15-8 shows a default storage class that automatically provisions disk objects on the microsoft azure platform. example 15-8. storageclass.yaml apiversion: storage.k8s.io/v1 kind: storageclass metadata:  name: default  annotations:  storageclass.beta.kubernetes.io/is-default-class: \"true\"  labels:  kubernetes.io/cluster-service: \"true\" provisioner: kubernetes.io/azure-disk once a storage class has been created for a cluster, you can refer to this storage class in your persistent volume claim, rather than referring to any specific persistent vol‐ ume. when the dynamic provisioner sees this storage claim, it uses the appropriate volume driver to create the volume and bind it to your persistent volume claim. example 15-9 shows an example of a persistentvolumeclaim that uses the default storage class we just defined to claim a newly created persistent volume. example 15-9. dynamic-volume-claim.yaml kind: persistentvolumeclaim apiversion: v1 metadata: running reliable singletons | 185  name: my-claim  annotations:  volume.beta.kubernetes.io/storage-class: default spec:  accessmodes:  - readwriteonce  resources:  requests:  storage: 10gi the volume.beta.kubernetes.io/storage-class annotation is what links this claim back up to the storage class we created. automatic provisioning of a persistent volume is a great feature that makes it significantly easier to build and manage stateful appli‐ cations in kubernetes. however, the lifespan of these persistent volumes is dictated by the reclamation policy of the persistentvo lumeclaim and the default is to bind that lifespan to the lifespan of the pod that creates the volume. this means that if you happen to delete the pod (e.g., via a scaledown or other event), then the volume is deleted as well. while this may be what you want in certain circumstances, you need to be careful to ensure that you don’t accidentally delete your persistent volumes. persistent volumes are great for traditional applications that require storage, but if you need to develop high-availability, scalable storage in a kubernetes-native fashion, the newly released statefulset object can be used instead. we’ll describe how to deploy mongodb using statefulsets in the next section. kubernetes-native storage with statefulsets when kubernetes was first developed, there was a heavy emphasis on homogeneity for all replicas in a replicated set. in this design, no replica had an individual identity or configuration. it was up to the application developer to determine a design that could establish this identity for their application. while this approach provides a great deal of isolation for the orchestration system, it also makes it quite difficult to develop stateful applications. after significant input from the community and a great deal of experimentation with various existing state‐ ful applications, statefulsets were introduced in kubernetes version 1.5. 186 | chapter 15: integrating storage solutions and kubernetes properties of statefulsets statefulsets are replicated groups of pods, similar to replicasets. but unlike a replica‐ set, they have certain unique properties: • each replica gets a persistent hostname with a unique index (e.g., database-0, database-1, etc.). • each replica is created in order from lowest to highest index, and creation will block until the pod at the previous index is healthy and available. this also applies to scaling up. • when a statefulset is deleted, each of the managed replica pods is also deleted in order from highest to lowest. this also applies to scaling down the number of replicas. it turns out that this simple set of requirements makes it drastically easier to deploy storage applications on kubernetes. for example, the combination of stable host‐ names (e.g., database-0) and the ordering constraints mean that all replicas, other than the first one, can reliably reference database-0 for the purposes of discovery and establishing replication quorum. manually replicated mongodb with statefulsets in this section, we’ll deploy a replicated mongodb cluster. for now, the replication setup itself will be done manually to give you a feel for how statefulsets work. even‐ tually we will automate this setup as well. to start, we’ll create a replicated set of three mongodb pods using a statefulset object (example 15-10). example 15-10. mongo-simple.yaml apiversion: apps/v1 kind: statefulset metadata:  name: mongo spec:  servicename: \"mongo\"  replicas: 3  template:  metadata:  labels:  app: mongo  spec:  containers:  - name: mongodb  image: mongo:3.4.1 kubernetes-native storage with statefulsets | 187  command:  - mongod  - --replset  - rs0  ports:  - containerport: 27017  name: peer as you can see, the definition is similar to the replicaset definitions we’ve seen previ‐ ously. the only changes are in the apiversion and kind fields. create the statefulset: $ kubectl apply -f mongo-simple.yaml once created, the differences between a replicaset and a statefulset become appa‐ rent. run kubectl get pods and you will likely see: name ready status restarts age mongo-0 1/1 running 0 1m mongo-1 0/1 containercreating 0 10s there are two important differences between this and what you would see with a replicaset. the first is that each replicated pod has a numeric index (0, 1, …), instead of the random suffix that is added by the replicaset controller. the second is that the pods are being slowly created in order, not all at once as they would be with a replicaset. once the statefulset is created, we also need to create a “headless” service to manage the dns entries for the statefulset. in kubernetes a service is called “headless” if it doesn’t have a cluster virtual ip address. since with statefulsets each pod has a unique identity, it doesn’t really make sense to have a load-balancing ip address for the repli‐ cated service. you can create a headless service using clusterip: none in the service specification (example 15-11). example 15-11. mongo-service.yaml apiversion: v1 kind: service metadata:  name: mongo spec:  ports:  - port: 27017  name: peer  clusterip: none  selector:  app: mongo 188 | chapter 15: integrating storage solutions and kubernetes once you create that service, there are usually four dns entries that are populated. as usual, mongo.default.svc.cluster.local is created, but unlike with a standard service, doing a dns lookup on this hostname provides all the addresses in the state‐ fulset. in addition, entries are created for mongo-0.mongo.default.svc.cluster .local as well as mongo-1.mongo and mongo-2.mongo. each of these resolves to the specific ip address of the replica index in the statefulset. thus, with statefulsets you get well-defined, persistent names for each replica in the set. this is often very useful when you are configuring a replicated storage solution. you can see these dns entries in action by running the following commands in one of the mongo replicas: $ kubectl run -it --rm --image busybox busybox ping mongo-1.mongo next, we’re going to manually set up mongo replication using these per-pod hostnames. we’ll choose mongo-0.mongo to be our initial primary. run the mongo tool in that pod: $ kubectl exec -it mongo-0 mongo > rs.initiate( {  id: \"rs0\",  members:  });  ok this command tells mongodb to initiate the replicaset rs0 with mongo-0.mongo as the primary replica. the rs0 name is arbitrary. you can use whatever you’d like, but you’ll need to change it in the mongo.yaml statefulset definition as well. once you have initiated the mongo replicaset, you can add the remaining replicas by running the following commands in the mongo tool on the mongo-0.mongo pod: > rs.add(\"mongo-1.mongo:27017\"); > rs.add(\"mongo-2.mongo:27017\"); as you can see, we are using the replica-specific dns names to add them as replicas in our mongo cluster. at this point, we’re done. our replicated mongodb is up and running. but it’s really not as automated as we’d like it to be—in the next section, we’ll see how to use scripts to automate the setup. automating mongodb cluster creation to automate the deployment of our statefulset-based mongodb cluster, we’re going to add an additional container to our pods to perform the initialization. kubernetes-native storage with statefulsets | 189 to configure this pod without having to build a new docker image, we’re going to use a configmap to add a script into the existing mongodb image. here’s the container we’re adding: ...  - name: init-mongo  image: mongo:3.4.1  command:  - bash  - /config/init.sh  volumemounts:  - name: config  mountpath: /config  volumes:  - name: config  configmap:  name: \"mongo-init\" note that it is mounting a configmap volume whose name is mongo-init. this con‐ figmap holds a script that performs our initialization. first, the script determines whether it is running on mongo-0 or not. if it is on mongo-0, it creates the replicaset using the same command we ran imperatively previously. if it is on a different mongo replica, it waits until the replicaset exists, and then it registers itself as a member of that replicaset. example 15-12 has the complete configmap object. example 15-12. mongo-configmap.yaml apiversion: v1 kind: configmap metadata:  name: mongo-init data:  init.sh: |  #!/bin/bash  # need to wait for the readiness health check to pass so that the  # mongo names resolve. this is kind of wonky.  until ping -c 1 ${hostname}.mongo; do  echo \"waiting for dns (${hostname}.mongo)...\"  sleep 2  done  until /usr/bin/mongo --eval \\'printjson(db.serverstatus())\\'; do  echo \"connecting to local mongo...\"  sleep 2  done  echo \"connected to local.\" 190 | chapter 15: integrating storage solutions and kubernetes  host=mongo-0.mongo:27017  until /usr/bin/mongo --host=${host} --eval \\'printjson(db.serverstatus())\\'; do  echo \"connecting to remote mongo...\"  sleep 2  done  echo \"connected to remote.\"  if ]; then  until /usr/bin/mongo --host=${host} --eval=\"printjson(rs.status())\" \\\\  | grep -v \"no replset config has been received\"; do  echo \"waiting for replication set initialization\"  sleep 2  done  echo \"adding self to mongo-0\"  /usr/bin/mongo --host=${host} \\\\  --eval=\"printjson(rs.add(\\'${hostname}.mongo\\'))\"  fi  if ]; then  echo \"initializing replica set\"  /usr/bin/mongo --eval=\"printjson(rs.initiate(\\\\  {\\'id\\': \\'rs0\\', \\'members\\': [{\\'id\\': 0, \\\\  \\'host\\': \\'mongo-0.mongo:27017\\'}]}))\"  fi  echo \"initialized\"  while true; do  sleep 3600  done this script currently sleeps forever after initializing the cluster. every container in a pod has to have the same restartpolicy. since we do not want our main mongo container to be restarted, we need to have our initialization container run forever too, or else kubernetes might think our mongo pod is unhealthy. putting it all together, example 15-13 is the complete statefulset that uses the configmap. example 15-13. mongo.yaml apiversion: apps/v1 kind: statefulset metadata:  name: mongo spec:  servicename: \"mongo\"  replicas: 3  template: kubernetes-native storage with statefulsets | 191  metadata:  labels:  app: mongo  spec:  containers:  - name: mongodb  image: mongo:3.4.1  command:  - mongod  - --replset  - rs0  ports:  - containerport: 27017  name: web  # this container initializes the mongodb server, then sleeps.  - name: init-mongo  image: mongo:3.4.1  command:  - bash  - /config/init.sh  volumemounts:  - name: config  mountpath: /config  volumes:  - name: config  configmap:  name: \"mongo-init\" given all of these files, you can create a mongo cluster with: $ kubectl apply -f mongo-config-map.yaml $ kubectl apply -f mongo-service.yaml $ kubectl apply -f mongo.yaml or if you want, you can combine them all into a single yaml file where the individ‐ ual objects are separated by ---. ensure that you keep the same ordering, since the statefulset definition relies on the configmap definition existing. persistent volumes and statefulsets for persistent storage, you need to mount a persistent volume into the /data/db direc‐ tory. in the pod template, you need to update it to mount a persistent volume claim to that directory: ...  volumemounts:  - name: database  mountpath: /data/db while this approach is similar to the one we saw with reliable singletons, because the statefulset replicates more than one pod you cannot simply reference a persistent 192 | chapter 15: integrating storage solutions and kubernetes volume claim. instead, you need to add a persistent volume claim template. you can think of the claim template as being identical to the pod template, but instead of cre‐ ating pods, it creates volume claims. you need to add the following onto the bottom of your statefulset definition:  volumeclaimtemplates:  - metadata:  name: database  annotations:  volume.alpha.kubernetes.io/storage-class: anything  spec:  accessmodes:   resources:  requests:  storage: 100gi when you add a volume claim template to a statefulset definition, each time the statefulset controller creates a pod that is part of the statefulset it will create a persis‐ tent volume claim based on this template as part of that pod. in order for these replicated persistent volumes to work correctly, you either need to have autoprovisioning set up for persistent vol‐ umes, or you need to prepopulate a collection of persistent volume objects for the statefulset controller to draw from. if there are no claims that can be created, the statefulset controller will not be able to create the corresponding pods. one final thing: readiness probes the final piece in productionizing our mongodb cluster is to add liveness checks to our mongo-serving containers. as we learned in “health checks” on page 54, the liveness probe is used to determine if a container is operating correctly. for the liven‐ ess checks, we can use the mongo tool itself by adding the following to the pod tem‐ plate in the statefulset object: ...  livenessprobe:  exec:  command:  - /usr/bin/mongo  - --eval  - db.serverstatus()  initialdelayseconds: 10  timeoutseconds: 10  ... kubernetes-native storage with statefulsets | 193 summary once we have combined statefulsets, persistent volume claims, and liveness probing, we have a hardened, scalable cloud-native mongodb installation running on kuber‐ netes. while this example dealt with mongodb, the steps for creating statefulsets to manage other storage solutions are quite similar and similar patterns can be followed. 194 | chapter 15: integrating storage solutions and kubernetes chapter 16 extending kubernetes from the beginning, it was clear that kubernetes was going to be more than its core set of apis; once an application is orchestrated within the cluster, there are countless other useful tools and utilities that can be represented and deployed as api objects in the kubernetes cluster. the challenge was how to embrace this explosion of objects and use cases without having an api that sprawled without bound. to resolve this tension between extended use cases and api sprawl, significant effort was put into making the kubernetes api extensible. this extensibility meant that cluster operators could customize their clusters with the additional components that suited their needs. this extensibility enables people to augment their clusters them‐ selves, consume community-developed cluster add-ons, and even develop extensions that are bundled and sold in an ecosystem of cluster plug-ins. extensibility has also given rise to whole new patterns of managing systems, such as the operator pattern. regardless of whether you are building your own extensions or consuming operators from the ecosystem, understanding how the kubernetes api server is extended and how extensions can be built and delivered is a key component to unlocking the com‐ plete power of kubernetes and its ecosystem. as more and more advanced tools and platforms are built on top of kubernetes using these extensibility mechanisms, a working knowledge of how they operate is critical to understanding how to build applications in a modern kubernetes cluster. what it means to extend kubernetes in general, extensions to the kubernetes api server either add new functionality to a cluster or limit and tweak the ways that users can interact with their clusters. there is a rich ecosystem of plug-ins that cluster administrators can use to add additional services and capabilities to their clusters. it’s worth noting that extending the cluster 195 is a very high-privilege thing to do. it is not a capability that should be extended to arbitrary users or arbitrary code, because cluster administrator privileges are required to extend a cluster. even cluster administrators should be careful and use diligence when installing third-party tools. some extensions, like admission controllers, can be used to view all objects being created in the cluster, and could easily be used as a vec‐ tor to steal secrets or run malicious code. additionally, extending a cluster makes it different than stock kubernetes. when running on multiple clusters, it is very valua‐ ble to build tooling to maintain consistency of experience across the clusters, and this includes the extensions that are installed. points of extensibility there are many different ways to extend kubernetes, from customresourcedefini‐ tions through to container network interface (cni) plug-ins. this chapter is going to focus on the extensions to the api server via adding new resource types or admis‐ sion controllers to api requests. we will not cover the cni/csi/cri (container net‐ work interface/container storage interface/container runtime interface) extensions, as they are more commonly used by kubernetes cluster providers as opposed to the end users of kubernetes, for whom this book was written. in addition to admission controllers and api extensions, there are actually a number of ways to “extend” your cluster without ever modifying the api server at all. these include daemonsets that install automatic logging and monitoring, tools that scan your services for cross-site scripting (xss) vulnerabilities, and more. before embark‐ ing on extending your cluster yourself, however, it’s worth considering the landscape of things that are possible within the confines of the existing kubernetes apis. to help understand the role of admission controllers and customresourcedefini‐ tions, it is very helpful to understand the flow of requests through the kubernetes api server, which is shown in figure 16-1. figure 16-1. api server request flow admission controllers are called prior to the api object being written into the back‐ ing storage. admission controllers can reject or modify api requests. there are sev‐ eral admission controllers that are built into the kubernetes api server; for example, the limit range admission controller that sets default limits for pods without them. many other systems use custom admission controllers to auto-inject sidecar contain‐ ers into all pods created on the system to enable “auto-magic” experiences. 196 | chapter 16: extending kubernetes the other form of extension, which can also be used in conjunction with admission controllers, is custom resources. with custom resources, whole new api objects are added to the kubernetes api surface area. these new api objects can be added to namespaces, are subject to rbac, and can be accessed with existing tools like kubectl as well as via the kubernetes api. the following sections describe these kubernetes extension points in greater detail and give both use cases and hands-on examples of how to extend your cluster. the first thing that you do to create a custom resource is to create a customresour‐ cedefinition. this object is actually a meta-resource; that is, a resource that is the def‐ inition of another resource. as a concrete example, consider defining a new resource to represent load tests in your cluster. when a new loadtest resource is created, a load test is spun up in your kubernetes cluster and drives traffic to a service. the first step in creating this new resource is defining it through a customresource‐ definition. an example definition looks as follows: apiversion: apiextensions.k8s.io/v1beta1 kind: customresourcedefinition metadata:  name: loadtests.beta.kuar.com spec:  group: beta.kuar.com  versions:  - name: v1  served: true  storage: true  scope: namespaced  names:  plural: loadtests  singular: loadtest  kind: loadtest  shortnames:  - lt you can see that this is a kubernetes object like any other. it has a metadata subobject, and within that subobject the resource is named. however, in the case of custom resources, the name is special. it has to be the following format: <resourceplural>.<api-group>. the reason for this is to ensure that each resource definition is unique in the cluster, because the name of each customresourcedefinition has to match this pattern, and no two objects in the cluster can have the same name. we are thus guaranteed that no two customresourcedefinitions define the same resource. in addition to metadata, the customresourcedefinition has a spec subobject. this is where the resource itself is defined. in that spec object, there is an apigroup field which supplies the api group for the resource. as mentioned previously, it must points of extensibility | 197 match the suffix of the customresourcedefinition’s name. additionally, there is a list of versions for the resource. in addition to the name of the version (e.g., v1, v2, etc.), there are fields that indicate if that version is served by the api server and which ver‐ sion is used for storing data in the backing storage for the api server. the storage field must be true for only a single version for the resource. there is also a scope field to indicate if the resource is namespaced or not (the default is namespaced), and a names field that allows for the definition of the singular, plural, and kind values for the resource. it also allows the definition of convenience “short names” for the resource for use in kubectl and elsewhere. given this definition, you can create the resource in the kubernetes api server. but first, to show the true nature of dynamic resource types, try to list our loadtests resource using kubectl: $ kubectl get loadtests you’ll see that there is no such resource currently defined. now use loadtest-resource.yaml to create this resource: $ kubectl create -f loadtest-resource.yaml then get the loadtests resource again: $ kubectl get loadtests this time you’ll see that there is a loadtest resource type defined, though there are still no instances of this resource type. let’s change that by creating a new loadtest resource. as with all built-in kubernetes api objects, you can use yaml or json to define a custom resource (in this case our loadtest). see the following definition: apiversion: beta.kuar.com/v1 kind: loadtest metadata:  name: my-loadtest spec:  service: my-service  scheme: https  requestspersecond: 1000  paths:  - /index.html  - /login.html  - /shares/my-shares/ one thing that you’ll note is that we never defined the schema for the custom resource in the customresourcedefinition. it actually is possible to provide an openapi specification for a custom resource, but this complexity is generally not 198 | chapter 16: extending kubernetes worth it for simple resource types. if you do want to perform validation, you can reg‐ ister a validating admission controller, as described in the following sections. you can now use this loadtest.yaml file to create a resource just like you would with any built-in type: $ kubectl create -f loadtest.yaml now when you list the loadtests resource, you’ll see your newly created resource: $ kubectl get loadtests this may be exciting, but it doesn’t really do anything yet. sure, you can use this sim‐ ple crud (create/read/update/delete) api to manipulate the data for loadtest objects, but no actual load tests are created in response to this new api we defined. this is because there is no controller present in the cluster to react and take action when a loadtest object is defined. the loadtest custom resource is only half of the infrastructure needed to add loadtests to our cluster. the other is a piece of code that will continuously monitor the custom resources and create, modify, or delete loadtests as necessary to implement the api. just like the user of the api, the controller interacts with the api server to list loadtests and watches for any changes that might occur. this interaction between controller and api server is shown in figure 16-2. figure 16-2. customresourcedefinition interactions the code for such a controller can range from simple to complex. the simplest con‐ trollers run a for loop and repeatedly poll for new custom objects, and then take actions to create or delete the resources that implement those custom objects (e.g., the loadtest worker pods). however, this polling-based approach is inefficient: the period of the polling loop adds unnecessary latency, and the overhead of polling may add unnecessary load on the api server. a more efficient approach is to use the watch api on the api server, which provides a stream of updates when they occur, eliminating both the latency and overhead of polling. however, using this api correctly in a bug-free way is com‐ plicated. as a result, if you want to use watches, it is highly recommended that you use a well-supported mechanism such as the informer pattern exposed in the client-go library. points of extensibility | 199 now that we have created a custom resource and implemented it via a controller, we have the basic functionality of a new resource in our cluster. however, many parts of what it means to be a well-functioning resource are missing. the two most important are validation and defaulting. validation is the process of ensuring that loadtest objects sent to the api server are well formed and can be used to create load tests, while defaulting makes it easier for people to use our resources by providing auto‐ matic, commonly used values by default. we’ll now cover adding these capabilities to our custom resource. as mentioned earlier, one option for adding validation is via an openapi specifica‐ tion for our objects. this can be useful for basic validation of the presence of required fields or the absence of unknown fields. a complete openapi tutorial is beyond the scope of this book, but there are lots of resources online, including the complete kubernetes api specification. generally speaking, an api schema is actually insufficient for validation of api objects. for example, in our loadtests example, we may want to validate that the loadtest object has a valid scheme (e.g., http or https) or that requestspersecond is a nonzero positive number. to accomplish this, we will use a validating admission controller. as discussed previ‐ ously, admission controllers intercept requests to the api server before they are pro‐ cessed and can reject or modify the requests in flight. admission controllers can be added to a cluster via the dynamic admission control system. a dynamic admission controller is a simple http application. the api server connects to the admission controller via either a kubernetes service object or an arbitrary url. this means that admission controllers can optionally run outside of the cluster—for example, in a cloud provider’s function-as-a-service offering, like azure functions or aws lambda. to install our validating admission controller, we need to specify it as a kubernetes validatingwebhookconfiguration. this object specifies the endpoint where the admission controller runs, as well as the resource (in this case loadtest) and the action (in this case create) where the admission controller should be run. you can see the full definition for the validating admission controller in the following code: apiversion: admissionregistration.k8s.io/v1beta1 kind: validatingwebhookconfiguration metadata:  name: kuar-validator webhooks: - name: validator.kuar.com  rules:  - apigroups:  - \"beta.kuar.com\"  apiversions:  - v1 200 | chapter 16: extending kubernetes  operations:  - create  resources:  - loadtests  clientconfig:  # substitute the appropriate ip address for your webhook  url: https://192.168.1.233:8080  # this should be the base64-encoded ca certificate for your cluster,  # you can find it in your ${kubeconfig} file  cabundle: replaceme fortunately for security, but unfortunately for complexity, webhooks that are accessed by the kubernetes api server can only be accessed via https. this means that we need to generate a certificate to serve the webhook. the easiest way to do this is to use the cluster’s ability to generate new certificates using its own certificate authority (ca). first, we need a private key and a certificate signing request (csr). here’s a simple go program that generates these: package main import (  \"crypto/rand\"  \"crypto/rsa\"  \"crypto/x509\"  \"crypto/x509/pkix\"  \"encoding/asn1\"  \"encoding/pem\"  \"net/url\"  \"os\" ) func main() {  host := os.args  name := \"server\"  key, err := rsa.generatekey(rand.reader, 1024)  if err != nil {  panic(err)  }  keyder := x509.marshalpkcs1privatekey(key)  keyblock := pem.block{  type: \"rsa private key\",  bytes: keyder,  }  keyfile, err := os.create(name + \".key\")  if err != nil {  panic(err)  }  pem.encode(keyfile, &keyblock)  keyfile.close() points of extensibility | 201  commonname := \"myuser\"  emailaddress := \"someone@myco.com\"  org := \"my co, inc.\"  orgunit := \"widget farmers\"  city := \"seattle\"  state := \"wa\"  country := \"us\"  subject := pkix.name{  commonname: commonname,  country: string{country},  locality: string{city},  organization: string{org},  organizationalunit: string{orgunit},  province: string{state},  }  uri, err := url.parserequesturi(host)  if err != nil {  panic(err)  }  asn1, err := asn1.marshal(subject.tordnsequence())  if err != nil {  panic(err)  }  csr := x509.certificaterequest{  rawsubject: asn1,  emailaddresses: string{emailaddress},  signaturealgorithm: x509.sha256withrsa,  uris: *url.url{uri},  }  bytes, err := x509.createcertificaterequest(rand.reader, &csr, key)  if err != nil {  panic(err)  }  csrfile, err := os.create(name + \".csr\")  if err != nil {  panic(err)  }  pem.encode(csrfile, &pem.block{type: \"certificate request\", bytes: bytes})  csrfile.close() } you can run this program with: $ go run csr-gen.go <url-for-webook> and it will generate two files, server.csr and server-key.pem. 202 | chapter 16: extending kubernetes you can then create a certificate signing request for the kubernetes api server using the following yaml: apiversion: certificates.k8s.io/v1beta1 kind: certificatesigningrequest metadata:  name: validating-controller.default spec:  groups:  - system:authenticated  request: replaceme  usages:  usages:  - digital signature  - key encipherment  - key agreement  - server auth you will notice for the request field the value is replaceme; this needs to be replaced with the base64-encoded certificate signing request we produced in the preceding code: $ perl -pi -e s/replaceme/$(base64 server.csr | tr -d \\'\\\\n\\')/ \\\\ admission-controller-csr.yaml now that your certificate signing request is ready, you can send it to the api server to get the certificate: $ kubectl create -f admission-controller-csr.yaml next, you need to approve that request: $ kubectl certificate approve validating-controller.default once approved, you can download the new certificate: $ kubectl get csr validating-controller.default -o json | \\\\  jq -r .status.certificate | base64 -d > server.crt with the certificate, you are finally ready to create an ssl-based admission controller (phew!). when the admission controller code receives a request, it contains an object of type admissionreview, which contains metadata about the request as well as the body of the request itself. in our validating admission controller we have only regis‐ tered for a single resource type and a single action (create), so we don’t need to examine the request metadata. instead, we dive directly into the resource itself and validate that requestspersecond is positive and the url scheme is valid. if they aren’t, we return a json body disallowing the request. implementing an admission controller to provide defaulting is similar to the steps just described, but instead of using a validatingwebhookconfiguration you use a mutatingwebhookconfiguration, and you need to provide a jsonpatch object to mutate the request object before it is stored. points of extensibility | 203 here’s a typescript snippet that you can add to your validating admission controller to add defaulting. if the paths field in the loadtest is of length zero, add a single path for /index.html:  if (needspatch(loadtest)) {  const patch = [  { \\'op\\': \\'add\\', \\'path\\': \\'/spec/paths\\', \\'value\\':  },  ]  response = buffer.from(json.stringify(patch))  .tostring(\\'base64\\');  response = \\'jsonpatch\\';  } you can then register this webhook as a mutatingwebhookconfiguration by simply changing the kind field in the yaml object and saving the file as mutatingcontroller.yaml. then create the controller by running: $ kubectl create -f mutating-controller.yaml at this point you’ve seen a complete example of how to extend the kubernetes api server using custom resources and admission controllers. the following section describes some general patterns for various extensions. patterns for custom resources not all custom resources are identical. there are a variety of different reasons for extending the kubernetes api surface area, and the following sections discuss some general patterns you may want to consider. just data the easiest pattern for api extension is the notion of “just data.” in this pattern, you are simply using the api server for storage and retrieval of information for your application. it is important to note that you should not use the kubernetes api server for application data storage. the kubernetes api server is not designed to be a key/ value store for your app; instead, api extensions should be control or configuration objects that help you manage the deployment or runtime of your application. an example use case for the “just data” pattern might be configuration for canary deploy‐ ments of your application—for example, directing 10% of all traffic to an experimen‐ tal backend. while in theory such configuration information could also be stored in a configmap, configmaps are essentially untyped, and sometimes using a more strongly typed api extension object provides clarity and ease of use. extensions that are just data don’t need a corresponding controller to activate them, but they may have validating or mutating admission controllers to ensure that they are well formed. for example, in the canary use case a validating controller might ensure that all percentages in the canary object sum to 100%. 204 | chapter 16: extending kubernetes compilers a slightly more complicated pattern is the “compiler” or “abstraction” pattern. in this pattern the api extension object represents a higher-level abstraction that is “com‐ piled” into a combination of lower-level kubernetes objects. the loadtest extension in the previous example is an example of this compiler abstraction pattern. a user consumes the extension as a high-level concept, in this case a loadtest, but it comes into being by being deployed as a collection of kubernetes pods and services. to ach‐ ieve this, a compiled abstraction requires an api controller to be running somewhere in the cluster, to watch the current loadtests and create the “compiled” representa‐ tion (and likewise delete representations that no longer exist). in contrast to the oper‐ ator pattern described next, however, there is no online health maintenance for compiled abstractions; it is delegated down to the lower-level objects (e.g., pods). operators while compiler extensions provide easy-to-use abstractions, extensions that use the “operator” pattern provide online, proactive management of the resources created by the extensions. these extensions likely provide a higher-level abstraction (for exam‐ ple, a database) that is compiled down to a lower-level representation, but they also provide online functionality, such as snapshot backups of the database, or upgrade notifications when a new version of the software is available. to achieve this, the con‐ troller not only monitors the extension api to add or remove things as necessary, but also monitors the running state of the application supplied by the extension (e.g., a database) and takes actions to remediate unhealthy databases, take snapshots, or restore from a snapshot if a failure occurs. operators are the most complicated pat‐ tern for api extension of kubernetes, but they are also the most powerful, enabling users to get easy access to “self-driving” abstractions that are responsible not just for deployment, but also health checking and repair. getting started getting started extending the kubernetes api can be a daunting and exhausting experience. fortunately, there is a great deal of code to help you out. the kubebuilder project contains a library of code intended help you easily build reliable kubernetes api extensions. it’s a great resource to help you bootstrap your extension. summary one of the great “superpowers” of kubernetes is its ecosystem, and one of the most significant things powering this ecosystem is the extensibility of the kubernetes api. whether you’re designing your own extensions to customize your cluster or consum‐ ing off-the-shelf extensions as utilities, cluster services, or operators, api extensions summary | 205 are the key to making your cluster your own and building the right environment for the rapid development of reliable applications. 206 | chapter 16: extending kubernetes chapter 17 deploying real-world applications the previous chapters described a variety of api objects that are available in a kuber‐ netes cluster and ways in which those objects can best be used to construct reliable distributed systems. however, none of the preceding chapters really discussed how you might use the objects in practice to deploy a complete, real-world application. that is the focus of this chapter. we’ll take a look at four real-world applications: • jupyter, an open source scientific notebook • parse, an open source api server for mobile applications • ghost, a blogging and content management platform • redis, a lightweight, performant key/value store these complete examples should give you a better idea of how to structure your own deployments using kubernetes. jupyter the jupyter project is a web-based interactive scientific notebook for exploration and visualization. it is used by students and scientists around the world to build and explore data and data visualizations. because it is both simple to deploy and interest‐ ing to use, it’s a great first service to deploy on kubernetes. we begin by creating a namespace to hold the jupyter application: $ kubectl create namespace jupyter 207 and then create a deployment of size one with the program itself: apiversion: extensions/v1beta1 kind: deployment metadata:  labels:  run: jupyter  name: jupyter  namespace: jupyter spec:  replicas: 1  selector:  matchlabels:  run: jupyter  template:  metadata:  labels:  run: jupyter  spec:  containers  - image: jupyter/scipy-notebook:abdb27a6dfbb  name: jupyter  dnspolicy: clusterfirst  restartpolicy: always create a file named jupyter.yaml with these contents. once you have created this file, you can deploy it using: $ kubectl create -f jupyter.yaml now you need to wait for the container to be created. the jupyter container is quite large (2 gb at the time of writing), and thus it may take a few minutes to start up. you can wait for the container to become ready using the watch command (on macos, you’ll need to install this command using brew install watch): $ watch kubectl get pods --namespace jupyter once the jupyter container is up and running, you need to obtain the initial login token. you can do this by looking at the logs for the container: $ podname=$(kubectl get pods --namespace jupyter --no-headers | awk \\'{print $1}\\') \\\\ kubectl logs --namespace jupyter ${podname} you should then copy the token (it will look something like /? token=0195713c8e65088650fdd8b599db377b7ce6c9b10bd13766). next, set up port forwarding to the jupyter container: $ kubectl port-forward ${podname} 8888:8888 finally, you can visit http://localhost:8888/?token=<token>, inserting the token that you copied from the logs earlier. 208 | chapter 17: deploying real-world applications you should now have the jupyter dashboard loaded in your browser. you can find tutorials to get oriented to jupyter if you are so inclined on the jupyter project site. parse the parse server is a cloud api dedicated to providing easy-to-use storage for mobile applications. it provides a variety of different client libraries that make it easy to inte‐ grate with android, ios, and other mobile platforms. parse was purchased by face‐ book in 2013 and subsequently shut down. fortunately for us, a compatible server was open sourced by the core parse team and is available for us to use. this section describes how to set up parse in kubernetes. prerequisites parse uses a mongodb cluster for its storage. chapter 15 described how to set up a replicated mongodb cluster using kubernetes statefulsets. this section assumes you have a three-replica mongo cluster running in kubernetes with the names mongo-0.mongo, mongo-1.mongo, and mongo-2.mongo. these instructions also assume that you have a docker login; if you don’t have one, you can get one for free at https://docker.com. finally, we assume you have a kubernetes cluster deployed and the kubectl tool properly configured. building the parse-server the open source parse-server comes with a dockerfile by default, for easy contain‐ erization. first, clone the parse repository: $ git clone https://github.com/parseplatform/parse-server then move into that directory and build the image: $ cd parse-server $ docker build -t ${dockeruser}/parse-server . finally, push that image up to the docker hub: $ docker push ${dockeruser}/parse-server deploying the parse-server once you have the container image built, deploying the parse-server into your clus‐ ter is fairly straightforward. parse looks for three environment variables when being configured: parse | 209 parseserverapplicationid an identifier for authorizing your application parseservermasterkey an identifier that authorizes the master (root) user parseserverdatabaseuri the uri for your mongodb cluster putting this all together, you can deploy parse as a kubernetes deployment using the yaml file in example 17-1. example 17-1. parse.yaml apiversion: extensions/v1beta1 kind: deployment metadata:  name: parse-server  namespace: default spec:  replicas: 1  template:  metadata:  labels:  run: parse-server  spec:  containers:  - name: parse-server  image: ${dockeruser}/parse-server  env:  - name: parseserverdatabaseuri  value: \"mongodb://mongo-0.mongo:27017,\\\\  mongo-1.mongo:27017,mongo-2.mongo\\\\  :27017/dev?replicaset=rs0\"  - name: parseserverappid  value: my-app-id  - name: parseservermasterkey  value: my-master-key testing parse to test your deployment, you need to expose it as a kubernetes service. you can do that using the service definition in example 17-2. example 17-2. parse-service.yaml apiversion: v1 kind: service metadata: 210 | chapter 17: deploying real-world applications  name: parse-server  namespace: default spec:  ports:  - port: 1337  protocol: tcp  targetport: 1337  selector:  run: parse-server now your parse server is up and running and ready to receive requests from your mobile applications. of course, in any real application you are likely going to want to secure the connection with https. you can see the parse-server github page for more details on such a configuration. ghost ghost is a popular blogging engine with a clean interface written in javascript. it can either use a file-based sqlite database or mysql for storage. con\\x80guring ghost ghost is configured with a simple javascript file that describes the server. we will store this file as a configuration map. a simple development configuration for ghost looks like example 17-3. example 17-3. ghost-config.js var path = require(\\'path\\'),  config; config = {  development: {  url: \\'http://localhost:2368\\',  database: {  client: \\'sqlite3\\',  connection: {  filename: path.join(process.env.ghostcontent,  \\'/data/ghost-dev.db\\')  },  debug: false  },  server: {  host: \\'0.0.0.0\\',  port: \\'2368\\'  },  paths: {  contentpath: path.join(process.env.ghostcontent, \\'/\\')  } ghost | 211  } }; module.exports = config; once you have this configuration file saved to ghost-config.js, you can create a kuber‐ netes configmap object using: $ kubectl create cm --from-file ghost-config.js ghost-config this creates a configmap that is named ghost-config. as with the parse example, we will mount this configuration file as a volume inside of our container. we will deploy ghost as a deployment object, which defines this volume mount as part of the pod template (example 17-4). example 17-4. ghost.yaml apiversion: extensions/v1beta1 kind: deployment metadata:  name: ghost spec:  replicas: 1  selector:  matchlabels:  run: ghost  template:  metadata:  labels:  run: ghost  spec:  containers:  - image: ghost  name: ghost  command:  - sh  - -c  - cp /ghost-config/ghost-config.js /var/lib/ghost/config.js  && /usr/local/bin/docker-entrypoint.sh node current/index.js  volumemounts:  - mountpath: /ghost-config  name: config  volumes:  - name: config  configmap:  defaultmode: 420  name: ghost-config 212 | chapter 17: deploying real-world applications one thing to note here is that we are copying the config.js file from a different loca‐ tion into the location where ghost expects to find it, since the configmap can only mount directories, not individual files. ghost expects other files that are not in that configmap to be present in its directory, and thus we cannot simply mount the entire configmap into /var/lib/ghost. you can run this with: $ kubectl apply -f ghost.yaml once the pod is up and running, you can expose it as a service with: $ kubectl expose deployments ghost --port=2368 once the service is exposed, you can use the kubectl proxy command to access the ghost server: $ kubectl proxy then visit http://localhost:8001/api/v1/namespaces/default/services/ghost/proxy/ in your web browser to begin interacting with ghost. ghost + mysql of course, this example isn’t very scalable, or even reliable, since the contents of the blog are stored in a local file inside the container. a more scalable approach is to store the blog’s data in a mysql database. to do this, first modify config.js to include: ... database: {  client: \\'mysql\\',  connection: {  host : \\'mysql\\',  user : \\'root\\',  password : \\'root\\',  database : \\'ghostdb\\',  charset : \\'utf8\\'  }  }, ... obviously, in a real-world deployment you’ll want to change the password from root to something more secret. next, create a new ghost-config configmap object: $ kubectl create configmap ghost-config-mysql --from-file ghost-config.js then update the ghost deployment to change the name of the configmap mounted from config-map to config-map-mysql: ghost | 213 ...  - configmap:  name: ghost-config-mysql ... using the instructions from “kubernetes-native storage with statefulsets” on page 186, deploy a mysql server in your kubernetes cluster. make sure that it has a ser‐ vice named mysql defined as well. you will need to create the database in the mysql database: $ kubectl exec -it mysql-zzmlw -- mysql -u root -p enter password: welcome to the mysql monitor. commands end with ; or \\\\g. ... mysql> create database ghostdb; ... finally, perform a rollout to deploy this new configuration: $ kubectl apply -f ghost.yaml because your ghost server is now decoupled from its database, you can scale up your ghost server and it will continue to share the data across all replicas. edit ghost.yaml to set spec.replicas to 3, then run: $ kubectl apply -f ghost.yaml your ghost installation is now scaled up to three replicas. redis redis is a popular in-memory key/value store, with numerous additional features. it’s an interesting application to deploy because it is a good example of the value of the kubernetes pod abstraction. this is because a reliable redis installation actually is two programs working together. the first is redis-server, which implements the key/value store, and the other is redis-sentinel, which implements health checking and failover for a replicated redis cluster. when redis is deployed in a replicated manner, there is a single master server that can be used for both read and write operations. additionally, there are other replica servers that duplicate the data written to the master and can be used for loadbalancing read operations. any of these replicas can fail over to become the master if the original master fails. this failover is performed by the redis sentinel. in our deployment, both a redis server and a redis sentinel are colocated in the same file. 214 | chapter 17: deploying real-world applications con\\x80guring redis as before, we’re going to use kubernetes configmaps to configure our redis installa‐ tion. redis needs separate configurations for the master and slave replicas. to config‐ ure the master, create a file named master.conf that contains the code in example 17-5. example 17-5. master.conf bind 0.0.0.0 port 6379 dir /redis-data this directs redis to bind to all network interfaces on port 6379 (the default redis port) and store its files in the /redis-data directory. the slave configuration is identical, but it adds a single slaveof directive. create a file named slave.conf that contains what’s in example 17-6. example 17-6. slave.conf bind 0.0.0.0 port 6379 dir . slaveof redis-0.redis 6379 notice that we are using redis-0.redis for the name of the master. we will set up this name using a service and a statefulset. we also need a configuration for the redis sentinel. create a file named sentinel.conf with the contents of example 17-7. example 17-7. sentinel.conf bind 0.0.0.0 port 26379 sentinel monitor redis redis-0.redis 6379 2 sentinel parallel-syncs redis 1 sentinel down-after-milliseconds redis 10000 sentinel failover-timeout redis 20000 now that we have all of our configuration files, we need to create a couple of simple wrapper scripts to use in our statefulset deployment. redis | 215 the first script simply looks at the hostname for the pod and determines whether this is the master or a slave, and launches redis with the appropriate configuration. cre‐ ate a file named init.sh containing the code in example 17-8. example 17-8. init.sh #!/bin/bash if ]; then  redis-server /redis-config/master.conf else  redis-server /redis-config/slave.conf fi the other script is for the sentinel. in this case it is necessary because we need to wait for the redis-0.redis dns name to become available. create a script named sentinel.sh containing the code in example 17-9. example 17-9. sentinel.sh #!/bin/bash cp /redis-config-src/*.* /redis-config while ! ping -c 1 redis-0.redis; do  echo \\'waiting for server\\'  sleep 1 done redis-sentinel /redis-config/sentinel.conf now we need to package all of these files into a configmap object. you can do this with a single command line: $ kubectl create configmap \\\\  --from-file=slave.conf=./slave.conf \\\\  --from-file=master.conf=./master.conf \\\\  --from-file=sentinel.conf=./sentinel.conf \\\\  --from-file=init.sh=./init.sh \\\\  --from-file=sentinel.sh=./sentinel.sh \\\\  redis-config creating a redis service the next step in deploying redis is to create a kubernetes service that will provide naming and discovery for the redis replicas (e.g., redis-0.redis). to do this, we cre‐ ate a service without a cluster ip address (example 17-10). 216 | chapter 17: deploying real-world applications example 17-10. redis-service.yaml apiversion: v1 kind: service metadata:  name: redis spec:  ports:  - port: 6379  name: peer  clusterip: none  selector:  app: redis you can create this service with kubectl apply -f redis-service.yaml. don’t worry that the pods for the service don’t exist yet. kubernetes doesn’t care; it will add the right names when the pods are created. deploying redis we’re ready to deploy our redis cluster. to do this we’re going to use a statefulset. we introduced statefulsets in “manually replicated mongodb with statefulsets” on page 187, when we discussed our mongodb installation. statefulsets provide indexing (e.g., redis-0.redis) as well as ordered creation and deletion semantics (redis-0 will always be created before redis-1, and so on). they’re quite useful for stateful applications like redis, but honestly, they basically look like kubernetes deployments. example 17-11 shows what the statefulset looks like for our redis cluster. example 17-11. redis.yaml apiversion: apps/v1beta1 kind: statefulset metadata:  name: redis spec:  replicas: 3  servicename: redis  template:  metadata:  labels:  app: redis  spec:  containers:  - command:   image: redis:4.0.11-alpine  name: redis  ports:  - containerport: 6379  name: redis redis | 217  volumemounts:  - mountpath: /redis-config  name: config  - mountpath: /redis-data  name: data  - command:   image: redis:4.0.11-alpine  name: sentinel  volumemounts:  - mountpath: /redis-config-src  name: config  - mountpath: /redis-config  name: data  volumes:  - configmap:  defaultmode: 420  name: redis-config  name: config  - emptydir:  name: data  volumemounts:  - mountpath: /redis-config  name: config  - mountpath: /redis-data  name: data  - command:   image: redis:3.2.7-alpine  name: sentinel  volumemounts:  - mountpath: /redis-config  name: config you can see that there are two containers in this pod. one runs the init.sh script that we created and the main redis server, and the other is the sentinel that monitors the servers. you can also see that there are two volumes defined in the pod. one is the volume that uses our configmap to configure the two redis applications, and the other is a simple emptydir volume that is mapped into the redis server container to hold the application data so that it survives a container restart. for a more reliable redis installation, this could be a network-attached disk, as discussed in chapter 15. now that we’ve defined our redis cluster, we can create it using: $ kubectl apply -f redis.yaml playing with our redis cluster to demonstrate that we’ve actually successfully created a redis cluster, we can per‐ form some tests. 218 | chapter 17: deploying real-world applications first, we can determine which server the redis sentinel believes is the master. to do this, we can run the redis-cli command in one of the pods: $ kubectl exec redis-2 -c redis \\\\  -- redis-cli -p 26379 sentinel get-master-addr-by-name redis this should print out the ip address of the redis-0 pod. you can confirm this using kubectl get pods -o wide. next, we’ll confirm that the replication is actually working. to do this, first try to read the value foo from one of the replicas: $ kubectl exec redis-2 -c redis -- redis-cli -p 6379 get foo you should see no data in the response. next, try to write that data to a replica: $ kubectl exec redis-2 -c redis -- redis-cli -p 6379 set foo 10 readonly you can\\'t write against a read only slave. you can’t write to a replica, because it’s read-only. let’s try the same command against redis-0, which is the master: $ kubectl exec redis-0 -c redis -- redis-cli -p 6379 set foo 10 ok now try the original read from a replica: $ kubectl exec redis-2 -c redis -- redis-cli -p 6379 get foo 10 this shows that our cluster is set up correctly, and data is replicating between masters and slaves. summary in this chapter we described how to deploy a variety of applications using assorted kubernetes concepts. we saw how to put together service-based naming and discov‐ ery to deploy web frontends like ghost as well as api servers like parse, and we saw how pod abstraction makes it easy to deploy the components that make up a reliable redis cluster. regardless of whether you will actually deploy these applications to production, the examples demonstrated patterns that you can repeat to manage your applications using kubernetes. we hope that seeing the concepts we described in pre‐ vious chapters come to life in real-world examples helps you better understand how to make kubernetes work for you. summary | 219  chapter 18 organizing your application throughout this book we have described various components of an application built on top of kubernetes. we have described how to wrap programs up as containers, place those containers in pods, replicate those pods with replicasets, and roll out software each week with deployments. we have even described how to deploy stateful and real-world applications that put together a collection of these objects into a single distributed system. but we have not covered how to actually work with such an appli‐ cation in a practical way. how can you lay out, share, manage, and update the various configurations that make up your application? that is the topic for this chapter. principles to guide us before digging into the concrete details of how to structure your application, it’s worth considering the goals that drive this structure. obviously, reliability and agility are the general goals of developing a cloud-native application in kubernetes, but moving to the next level of detail, how does this actually relate to how you design the maintenance and deployment of your application? the following sections describe the various principles that we can use as a guide to design a structure that best suits these goals. the principles are: • filesystems as the source of truth • code review to ensure the quality of changes • feature flags for staged roll forward and roll back 221 filesystems as the source of truth when you first begin to explore kubernetes, as we did in the beginning of this book, you generally interact with it imperatively. you run commands like kubectl run or kubectl edit to create and modify pods or other objects running in your cluster. even when we started exploring how to write and use yaml or json files, this was presented in an ad-hoc manner, as if the file itself is just a way station on the way to modifying the state of the cluster. in reality, in a true productionized application the opposite should be true. rather than viewing the state of the cluster—the data in etcd—as the source of truth, it is optimal to view the filesystem of yaml objects as the source of truth for your application. the api objects deployed into your kubernetes cluster(s) are then a reflection of the truth stored in the filesystem. there are numerous reasons why this is the right point of view. the first and fore‐ most is that it largely enables you to treat your cluster as if it is immutable infrastruc‐ ture. as we have moved into cloud-native architectures, we have become increasingly comfortable with the notion that our applications and their containers are immutable infrastructure, but treating a cluster as such is less common. and yet, the same rea‐ sons for moving our applications to immutable infrastructure apply to our clusters. if your cluster is a snowflake made up by the ad-hoc application of various random yaml files downloaded from the internet, it is as dangerous as a virtual machine that has been built from imperative bash scripts. additionally, managing the cluster state via the filesystem makes it very easy to col‐ laborate with multiple team members. source-control systems are well understood and can easily enable multiple different people to simultaneously edit the state of the cluster while making conflicts (and the resolution of those conflicts) clear to everyone. the combination of these motivations means that it is absolutely a first principle that all applications deployed to kubernetes should first be described in files stored in a filesystem. the actual api objects are then just a projection of this filesystem into a particular cluster. the role of code review it wasn’t long ago that code review for application source code was a novel idea. but it is clear now that the notion of multiple people looking at a piece of code before it is committed to an application is a best practice for producing quality, reliable code. it is therefore surprising that the same is somewhat less true for the configurations used to deploy those applications. all of the same reasons for reviewing code apply 222 | chapter 18: organizing your application directly to application configurations also. but when you think about it, it is also obvious that code review of these configurations is critical to the reliable deployment of services. in our experience, most service outages are self-inflicted via unexpected consequences, typos, or other simple mistakes. ensuring that at least two people look at any configuration change significantly decreases the probability of such errors. consequently, the second principle of our application layout is that it must facilitate the review of every change merged into the set of files that represents the source of truth for our cluster. feature gates and guards once your application source code and your deployment configuration files are in source control, one of the most common questions that occurs is how these reposito‐ ries relate to one another. should you use the same repository for application source code as well as configuration? this can work for small projects, but in larger projects it often makes sense to separate the source code from the configuration to provide for a separation of concerns. even if the same people are responsible for both building and deploying the application, the perspectives of the builder versus the deployer are different enough that this separation of concerns makes sense. if that is the case, then how do you bridge the development of new features in source control with the deployment of those features into a production environment? this is where feature gates and guards play an important role. the idea is that when some new feature is developed, that development takes place entirely behind a feature flag or gate. this gate looks something like: if (featureflags.myflag) {  // feature implementation goes here } there are a variety of benefits to this approach. first, it enables the committing of code to the production branch long before the feature is ready to ship. this enables feature development to stay much more closely aligned with the head of a repository, and thus you avoid the horrendous merge conflicts of a long-lived branch. additionally, it means that the enabling a feature simply involves a configuration change to activate the flag. this makes it very clear what changed in the production environment, and likewise makes it very simple to roll back the activation of the fea‐ ture if it causes problems. the use of feature flags thus both simplifies debugging problems in production and ensures that disabling a feature doesn’t require a binary rollback to an older version of principles to guide us | 223 the code that would remove all of the bug fixes and other improvements made by the newer version of the code. the third principle of application layout is that code lands in source control, by default off, behind a feature flag, and is only acti‐ vated through a code-reviewed change to configuration files. managing your application in source control now that we have determined that the filesystem should represent the source of truth for your cluster, the next important question is how to actually lay out the files in the filesystem. obviously, filesystems contain hierarchical directories, and a sourcecontrol system adds concepts like tags and branches, so this section describes how to put these together to represent and manage your application. filesystem layout for the purposes of this section, we will describe how to lay out an instance of your application for a single cluster. in later sections we will describe how to parameterize this layout for multiple instances of your application. it’s worth noting that this orga‐ nization is worth getting right when you begin. much like modifying the layout of packages in source control, modifying your deployment configurations after the fact is a complicated and expensive refactor that you’ll probably never get around to. the first cardinality on which you want to organize your application is the semantic component or layer (e.g., frontend, batch work queue, etc.). though early on this might seem like overkill, since a single team manages all of these components, it sets the stage for team scaling—eventually, a different team (or subteam) may be responsi‐ ble for each of these components. thus, for an application with a frontend that uses two services, the filesystem might look like: frontend/ service-1/ service-2/ within each of these directories, the configurations for each application are stored. these are the yaml files that directly represent the current state of the cluster. it’s generally useful to include both the service name and the object type within the same file. 224 | chapter 18: organizing your application while kubernetes allows for the creation of yaml files with multi‐ ple objects in the same file, this should generally be considered an anti-pattern. the only good reason for grouping a number of objects in the same file is if they are conceptually identical. when deciding what to include in a single yaml file, consider design principles similar to those for defining a class or struct. if grouping the objects together doesn’t form a single concept, they probably shouldn’t be in a single file. thus, extending our previous example, the filesystem might look like: frontend/  frontend-deployment.yaml  frontend-service.yaml  frontend-ingress.yaml service-1/  service-1-deployment.yaml  service-1-service.yaml  service-1-configmap.yaml ... managing periodic versions the previous section described a file structure for laying out the various tiers in your application, but what about managing the releases of your application? it is very use‐ ful to be able to look back historically and see what your application deployment pre‐ viously looked like. similarly, it is very useful to be able to iterate a configuration forward while still being able to deploy a stable release configuration. consequently, it’s handy to be able to simultaneously store and maintain multiple dif‐ ferent revisions of your configuration. given the file and version control approach, there are two different approaches that you can use. the first is to use tags, branches, and source-control features. this is convenient because it maps to the same way that people manage revisions in source control, and it leads to a more simplified directory structure. the other option is to clone the configuration within the filesystem and use directories for different revisions. this approach is convenient because it makes simultaneous viewing of the configurations very straightforward. in reality, the approaches are more or less identical, and it is ultimately an aesthetic choice between the two. thus, we will discuss both approaches and let you or your team decide which you prefer. versioning with branches and tags when you use branches and tags to manage configuration revisions, the directory structure is unchanged from the example in the previous section. when you are ready for a release, you place a source-control tag (e.g., git tag v1.0) in the configu‐ managing your application in source control | 225 ration source-control system. the tag represents the configuration used for that ver‐ sion, and the head of source control continues to iterate forward. the world becomes somewhat more complicated when you need to update the release configuration, but the approach models what you would do in source control. first, you commit the change to the head of the repository. then you create a new branch named v1 at the v1.0 tag. you then cherry-pick the desired change onto the release branch (git cherry-pick <edit>), and finally, you tag this branch with the v1.1 tag to indicate a new point release. one common error when cherry-picking fixes into a release branch is to only pick the change into the latest release. it’s a good idea to cherry-pick it into all active releases, in case for some reason you need to roll back versions but the fix is still needed. versioning with directories an alternative to using source-control features is to use filesystem features. in this approach, each versioned deployment exists within its own directory. for example, the filesystem for your application might look like this: frontend/  v1/  frontend-deployment.yaml  frontend-service.yaml  current/  frontend-deployment.yaml  frontend-service.yaml service-1/  v1/  service-1-deployment.yaml  service-1-service.yaml  v2/  service-1-deployment.yaml  service-1-service.yaml  current/  service-1-deployment.yaml  service-1-service.yaml ... thus, each revision exists in a parallel directory structure within a directory associ‐ ated with the release. all deployments occur from head instead of from specific revi‐ sions or tags. when adding a new configuration, it is done to the files in the current directory. when creating a new release, the current directory is copied to create a new directory associated with the new release. 226 | chapter 18: organizing your application when performing a bugfix change to a release, the pull request must modify the yaml file in all the relevant release directories. this is a slightly better experience than the cherry-picking approach described earlier, since it is clear in a single change request that all of the relevant versions are being updated with the same change, instead of requiring a cherry-pick per version. structuring your application for development, testing, and deployment in addition to structuring your application for a periodic release cadence, you also want to structure your application to enable agile development, quality testing, and safe deployment. this enables developers to rapidly make and test changes to the dis‐ tributed application, and to safely roll those changes out to customers. goals there are two goals for your application with regard to development and testing. the first is that each developer should be able to easily develop new features for the appli‐ cation. in most cases, the developer is only working on a single component, and yet that component is interconnected to all of the other microservices within the cluster. thus, to facilitate development it is essential that developers be able to work in their own environment, yet with all services available. the other goal for structuring your application for testing is the ability to easily and accurately test your application prior to deployment. this is essential to the ability to quickly roll out features while maintaining high reliability. progression of a release to achieve both of these goals, it is important to relate the stages of development to the release versions described earlier. the stages of a release are: head the bleeding edge of the configuration; the latest changes. development largely stable, but not ready for deployment. suitable for developers to use for building features. staging the beginnings of testing, unlikely to change unless problems are found. canary the first real release to users, used to test for problems with real-world traffic and likewise give users a chance to test what is coming next. structuring your application for development, testing, and deployment | 227 release the current production release. introducing a development tag regardless of whether you structure releases using the filesystem or version control, the right way to model the development stage is via a source-control tag. this is because development is necessarily fast-moving as it tracks stability only slightly behind head. to introduce a development stage, a new development tag is added to the sourcecontrol system and an automated process is used to move this tag forward. on a peri‐ odic cadence, head is tested via automated integration testing. if these tests pass, the development tag is moved forward to head. thus, developers can track reasonably close to the latest changes when deploying their own environments, but they also can be assured that the deployed configurations have at least passed a limited smoke test. mapping stages to revisions it might be tempting to introduce a new set of configurations for each of these stages, but in reality, the cartesian product of versions and stages would create a mess that is very difficult to reason about. instead, the right practice is to introduce a mapping between revisions and stages. regardless of whether you are using the filesystem or source-control revisions to rep‐ resent different configuration versions, it is easy to implement a map from stage to revision. in the filesystem case you can use symbolic links to map a stage name to a revision: frontend/  canary/ -> v2/  release/ -> v1/  v1/  frontend-deployment.yaml ... in the case of version control, it is simply an additional tag at the same revision as the appropriate version. in either case, the versioning of releases proceeds using the processes described previ‐ ously, and separately the stages are moved forward to new versions as appropriate. effectively this means that there are two simultaneous processes, the first for cutting new release versions and the second for qualifying a release version for a particular stage in the application lifecycle. 228 | chapter 18: organizing your application parameterizing your application with templates once you have a cartesian product of environments and stages, it becomes clear that it is impractical or impossible to keep them all entirely identical. and yet, it is impor‐ tant to strive for the environments to be as identical as possible. variance and drift between different environments produces snowflakes and systems that are hard to reason about. if your staging environment is different than your release environment, can you really trust the load tests that you ran in the staging environment to qualify a release? to ensure that your environments stay as similar as possible, it is useful to use parameterized environments. parameterized environments use templates for the bulk of their configuration, but they mix in a limited set of parameters to produce the final configuration. in this way most of the configuration is contained within a shared template, while the parameterization is limited in scope and maintained in a small parameters file for easy visualization of differences between environments. parameterizing with helm and templates there are a variety of different languages for creating parameterized configurations. in general they all divide the files into a template file, which contains the bulk of the configuration, and a parameters file, which can be combined with the template to produce a complete configuration. in addition to parameters, most templating lan‐ guages allow parameters to have default values if no value is specified. the following gives examples of how to parameterize configurations using helm, a package manager for kubernetes. despite what devotees of various languages may say, all parameterization languages are largely equivalent, and as with programming langauges, which one you prefer is largely a matter of personal or team style. thus, the same patterns described here for helm apply regardless of the templating lan‐ guage you choose. the helm template language uses the “mustache” syntax, so for example: metadata:  name: {{ .release.name }}-deployment indicates that release.name should be substituted into the name of a deployment. to pass a parameter for this value you use a values.yaml file with contents like: release:  name: my-release which after parameter substitution results in: metadata:  name: my-release-deployment parameterizing your application with templates | 229 filesystem layout for parameterization now that you understand how to parameterize your configurations, how do you apply that to the filesystem layouts we have described previously? to achieve this, instead of treating each deployment lifecycle stage as a pointer to a version, each deployment lifecycle is the combination of a parameters file and a pointer to a spe‐ cific version. for example, in a directory-based layout this might look like: frontend/  staging/  templates -> ../v2  staging-parameters.yaml  production/  templates -> ../v1  production-parameters.yaml  v1/  frontend-deployment.yaml  frontend-service.yaml  v2/  frontend-deployment.yaml  frontend-service.yaml ... doing this with version control looks similar, except that the parameters for each life‐ cycle stage are kept at the root of the configuration directory tree: frontend/  staging-parameters.yaml  templates/  frontend-deployment.yaml .... deploying your application around the world now that you have multiple versions of your application moving through multiple stages of deployment, the final step in structuring your configurations is to deploy your application around the world. but don’t think that these approaches are only for large-scale applications. in reality, they can be used to scale from two different regions to tens or hundreds around the world. in the world of the cloud, where an entire region can fail, deploying to multiple regions (and managing that deployment) is the only way to achieve sufficient uptime for demanding users. architectures for worldwide deployment generally speaking, each kubernetes cluster is intended to live in a single region, and each kubernetes cluster is expected to contain a single, complete deployment of your application. consequently, a worldwide deployment of an application consists of mul‐ tiple different kubernetes clusters, each with its own application configuration. 230 | chapter 18: organizing your application describing how to actually build a worldwide application, especially with complex subjects like data replication, is beyond the scope of this chapter, but we will describe how to arrange the application configurations in the filesystem. ultimately, a particular region’s configuration is conceptually the same as a stage in the deployment lifecycle. thus, adding multiple regions to your configuration is iden‐ tical to adding new lifecycle stages. for example, instead of: • development • staging • canary • production you might have: • development • staging • canary • eastus • westus • europe • asia modeling this in the filesystem for configuration, this looks like: frontend/  staging/  templates -> ../v3/  parameters.yaml  eastus/  templates -> ../v1/  parameters.yaml  westus/  templates -> ../v2/  parameters.yaml  ... if you instead are using version control and tags, the filesystem would look like: frontend/  staging-parameters.yaml  eastus-parameters.yaml  westus-parameters.yaml  templates/  frontend-deployment.yaml ... deploying your application around the world | 231 using this structure, you would introduce a new tag for each region and use the file contents at that tag to deploy to that region. implementing worldwide deployment now that you have configurations for each region around the world, the question becomes one of how to update those various regions. one of the primary goals of using multiple regions is to ensure very high reliability and uptime. while it would be tempting to assume that cloud and data center outages are the primary causes of downtime, the truth is that outages are generally caused by new versions of software rolling out. because of this, the key to a highly available system is limiting the effect or “blast radius” of any change that you might make. thus, as you roll out a version across a variety of regions, it makes sense to move carefully from region to region in order to validate and gain confidence in one region before moving on to the next. rolling out software across the world generally looks more like a workflow than a single declarative update: you begin by updating the version in staging to the latest version and then proceed through all regions until it is rolled out everywhere. but how should you structure the various regions, and how long should you wait to vali‐ date between regions? to determine the length of time between rollouts to regions, you want to consider the “mean time to smoke” for your software. this is the time it takes on average after a new release is rolled out to a region for a problem (if it exists) to be discovered. obvi‐ ously, each problem is unique and can take a varying amount of time to make itself known, and that is why you want to understand the average time. managing software at scale is a business of probability, not certainty, so you want to wait for a time that makes the probability of an error low enough that you are comfortable moving on to the next region. something like two to three times the mean time to smoke is proba‐ bly a reasonable place to start, but it is highly variable depending on your application. to determine the order of regions, it is important to consider the characteristics of various regions. for example, you are likely to have high-traffic regions and lowtraffic regions. depending on your application, you may have features that are more popular in one geographic area or another. all of these characteristics should be con‐ sidered when putting together a release schedule. you likely want to begin by rolling out to a low-traffic region. this ensures that any early problems you catch are limited to an area of little impact. though it is not a hard-and-fast rule, early problems are often the most severe, since they manifest quickly enough to be caught in the first region you roll out to. thus, minimizing the impact of such problems on your cus‐ tomers makes sense. next, you likely want to roll out to a high-traffic region. once you have successfully validated that your release works correctly via the low-traffic region, you want to validate that it works correctly at scale. the only way to do this is to roll it out to a single high-traffic region. when you have successfully rolled out to 232 | chapter 18: organizing your application both a low- and a high-traffic region, you may have confidence that your application can safely roll out everywhere. however, if there are regional variations, you may want to also test more slowly across a variety of geographies before pushing your release more broadly. when you put your release schedule together, it is important to follow it completely for every release, no matter how big or how small. many outages have been caused by people accelerating releases either to fix some other problem, or because they believed it to be “safe.” dashboards and monitoring for worldwide deployments it may seem an odd concept when you are developing at a small scale, but one signifi‐ cant problem that you will likely run into at a medium or large scale is having differ‐ ent versions of your application deployed to different regions. this can happen for a variety of reasons (e.g., because a release has failed, been aborted, or had problems in a particular region), and if you don’t track things carefully you can rapidly end up with an unmanageable snowflake of different versions deployed around the world. furthermore, as customers inquire about fixes to bugs they are experiencing, a com‐ mon question will become: “is it deployed yet?” thus, it is essential to develop dashboards that can tell you at a glance what version is running in which region, as well as alerting that will fire when too many different ver‐ sions of your application are deployed. a best practice is to limit the number of active versions to no more than three: one testing, one rolling out, and one being replaced by the rollout. any more active versions than this is asking for trouble. summary this chapter provides guidance on how to manage a kubernetes application through software versions, deployment stages, and regions around the world. it highlights the principles that are the foundation of organizing your application: relying on the file‐ system for organization, using code review to ensure quality changes, and relying on feature flags or gates to make it easy to incrementally add and remove functionality. as with everything, the recipes in this chapter should be taken as inspiration, rather than absolute truth. read the guidance, and find the mix of approaches that works best for the particular circumstances of your application. but keep in mind that in laying out your application for deployment, you are setting a process that you will likely have to live with for a number of years. summary | 233  appendix a building a raspberry pi kubernetes cluster while kubernetes is often experienced through the virtual world of public cloud computing, where the closest you get to your cluster is a web browser or a terminal, it can be a very rewarding experience to physically build a kubernetes cluster on bare metal. likewise, nothing compares to physically pulling the power or network on a node and watching how kubernetes reacts to heal your application to convince you of its utility. building your own cluster might seem like both a challenging and an expensive effort, but fortunately it is neither. the ability to purchase low-cost, system-on-chip computer boards, as well as a great deal of work by the community to make kuber‐ netes easier to install, means that it is possible to build a small kubernetes cluster in a few hours. in the following instructions, we focus on building a cluster of raspberry pi machines, but with slight adaptations the same instructions could be made to work with a variety of different single-board machines. parts list the first thing you need to do is assemble the pieces for your cluster. in all of the examples here, we’ll assume a four-node cluster. you could build a cluster of three nodes, or even a cluster of a hundred nodes if you wanted to, but four is a pretty good number. to start, you’ll need to purchase (or scrounge) the various pieces needed to build the cluster. here is the shopping list, with some approximate prices as of the time of writing: 235 1. four raspberry pi 3 boards (raspberry pi 2 will also work)—$160 2. four sdhc memory cards, at least 8 gb (buy high-quality ones!)—$30–50 3. four 12-inch cat. 6 ethernet cables—$10 4. four 12-inch usb a–micro usb cables—$10 5. one 5-port 10/100 fast ethernet switch—$10 6. one 5-port usb charger—$25 7. one raspberry pi stackable case capable of holding four pis—$40 (or build your own) 8. one usb-to-barrel plug for powering the ethernet switch (optional)—$5 the total for the cluster comes out to be about $300, which you can drop down to $200 by building a three-node cluster and skipping the case and the usb power cable for the switch (though the case and the cable really clean up the whole cluster). one other note on memory cards: do not scrimp here. low-end memory cards behave unpredictably and make your cluster really unstable. if you want to save some money, buy a smaller, high-quality card. high-quality 8 gb cards can be had for around $7 each online. once you have your parts, you’re ready to move on to building the cluster. these instructions also assume that you have a device capable of flashing an sdhc card. if you do not, you will need to purchase a usb → memory card reader/writer. flashing images the default raspbian image now supports docker through the standard install meth‐ ods, but to make things even easier, the hypriot project provides images with docker preinstalled. visit the hypriot downloads page and download the latest stable image. unzip the image, and you should now have an .img file. the hypriot project also provides really excellent documentation for writing this image to your memory card for each of these platforms: • macos • windows • linux 236 | appendix a: building a raspberry pi kubernetes cluster write the same image onto each of your memory cards. first boot: master the first thing to do is to boot just your master node. assemble your cluster, and decide which is going to be the master node. insert the memory card, plug the board into an hdmi output, and plug a keyboard into the usb port. next, attach the power to boot the board. log in at the prompt using the username pirate and the password hypriot. the very first thing you should do with your raspberry pi (or any new device) is to change the default password. the default pass‐ word for every type of install everywhere is well known by people who will misbehave given a default login to a system. this makes the internet less safe for everyone. please change your default passwords! setting up networking the next step is to set up networking on the master. first, set up wifi. this is going to be the link between your cluster and the outside world. edit the /boot/user-data file. update the wifi ssid and password to match your environment. if you ever want to switch networks, this is the file you need to edit. once you have edited this, reboot with sudo reboot and validate that your net‐ working is working. the next step in networking is to set up a static ip address for your cluster’s internal network. to do this, edit /etc/network/interfaces.d/eth0 to read: allow-hotplug eth0 iface eth0 inet static  address 10.0.0.1  netmask 255.255.255.0  broadcast 10.0.0.255  gateway 10.0.0.1 this sets the main ethernet interface to have the statically allocated address 10.0.0.1. reboot the machine to claim the 10.0.0.1 address. next, we’re going to install dhcp on this master so it will allocate addresses to the worker nodes. run: $ apt-get install isc-dhcp-server then configure the dhcp server as follows (/etc/dhcp/dhcpd.conf): building a raspberry pi kubernetes cluster | 237 # set a domain name, can basically be anything option domain-name \"cluster.home\"; # use google dns by default, you can substitute isp-supplied values here option domain-name-servers 8.8.8.8, 8.8.4.4; # we\\'ll use 10.0.0.x for our subnet subnet 10.0.0.0 netmask 255.255.255.0 {  range 10.0.0.1 10.0.0.10;  option subnet-mask 255.255.255.0;  option broadcast-address 10.0.0.255;  option routers 10.0.0.1; } default-lease-time 600; max-lease-time 7200; authoritative; you may also need to edit /etc/defaults/isc-dhcp-server to set the interfaces environ‐ ment variable to eth0. restart the dhcp server with sudo systemctl restart isc-dhcp-server. now your machine should be handing out ip addresses. you can test this by hooking up a second machine to the switch via ethernet. this second machine should get the address 10.0.0.2 from the dhcp server. remember to edit the /etc/hostname file to rename this machine to node-1. the final step in setting up networking is setting up network address translation (nat) so that your nodes can reach the public internet (if you want them to be able to do so). edit /etc/sysctl.conf and set net.ipv4.ipforward=1 to turn on ip forwarding. you should then reboot the server for this to take effect. alternately you can run sudo sysctl net.ipv4.ipforward=1 to make the change without rebooting. if you choose to do this you will still want to edit /etc/sysctl.conf to make the setting permanent. then edit /etc/rc.local (or the equivalent) and add iptables rules for forwarding from eth0 to wlan0 (and back): iptables -t nat -a postrouting -o wlan0 -j masquerade iptables -a forward -i wlan0 -o eth0 -m state \\\\  --state related,established -j accept iptables -a forward -i eth0 -o wlan0 -j accept at this point, the basic networking setup should be complete. plug in and power up the remaining two boards (you should see them assigned the addresses 10.0.0.3 and 10.0.0.4). edit the /etc/hostname file on each machine to name them node-2 and node-3, respectively. 238 | appendix a: building a raspberry pi kubernetes cluster validate this by first looking at /var/lib/dhcp/dhcpd.leases, and then ssh to the nodes (remember again to change the default password first thing). validate that the nodes can connect to the external internet. extra credit there are a couple of extra steps you can take that will make it easier to manage your cluster. the first is to edit /etc/hosts on each machine to map the names to the right addresses. on each machine, add: ... 10.0.0.1 kubernetes 10.0.0.2 node-1 10.0.0.3 node-2 10.0.0.4 node-3 ... now you can use those names when connecting to those machines. the second is to set up passwordless ssh access. to do this, run ssh-keygen and then copy the $home/.ssh/idrsa.pub file into /home/pirate/.ssh/authorizedkeys on node-1, node-2, and node-3. installing kubernetes at this point you should have all nodes up, with ip addresses and capable of accessing the internet. now it’s time to install kubernetes on all of the nodes. using ssh, run the following commands on all nodes to install the kubelet and kubeadm tools. you will need to be root to execute these commands. use sudo su to elevate to the root user. first, add the encryption key for the packages: # curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - then add the repository to your list of repositories: # echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" \\\\  >> /etc/apt/sources.list.d/kubernetes.list finally, update and install the kubernetes tools. this will also update all packages on your system for good measure: # apt-get update $ apt-get upgrade $ apt-get install -y kubelet kubeadm kubectl kubernetes-cni building a raspberry pi kubernetes cluster | 239 kubernetes uses a number of different kernel cgroups when it starts. it expects these capabilities to be present and can fail to start if they are not. make sure that you are running the latest kernel available in the raspberry pi distribution. setting up the cluster on the master node (the one running dhcp and connected to the internet), run: $ sudo kubeadm init --pod-network-cidr 10.244.0.0/16 \\\\  --apiserver-advertise-address 10.0.0.1 \\\\  --apiserver-cert-extra-sans kubernetes.cluster.home note that you are advertising your internal-facing ip address, not your external address. eventually, this will print out a command for joining nodes to your cluster. it will look something like: $ kubeadm join --token=<token> 10.0.0.1 ssh onto each of the worker nodes in your cluster and run that command. when all of that is done, you should be able to run this command and see your work‐ ing cluster: $ kubectl get nodes setting up cluster networking you have your node-level networking set up, but you need to set up the pod-to-pod networking. since all of the nodes in your cluster are running on the same physical ethernet network, you can simply set up the correct routing rules in the host kernels. the easiest way to manage this is to use the flannel tool created by coreos. flannel supports a number of different routing modes; we will use the host-gw mode. you can download an example configuration from the flannel project page: $ curl https://rawgit.com/coreos/flannel/master/documentation/kube-flannel.yml \\\\  > kube-flannel.yaml the default configuration that coreos supplies uses vxlan mode instead, and also uses the amd64 architecture instead of arm. to fix this, open up that configuration file in your favorite editor; replace vxlan with host-gw and replace all instances of amd64 with arm. 240 | appendix a: building a raspberry pi kubernetes cluster you can also do this with the sed tool in place: $ curl https://rawgit.com/coreos/flannel/master/documentation/kube-flannel.yml \\\\  | sed \"s/amd64/arm/g\" | sed \"s/vxlan/host-gw/g\" \\\\  > kube-flannel.yaml once you have your updated kube-flannel.yaml file, you can create the flannel net‐ working setup with: $ kubectl apply -f kube-flannel.yaml this will create two objects, a configmap used to configure flannel and a daemonset that runs the actual flannel daemon. you can inspect these with: $ kubectl describe --namespace=kube-system configmaps/kube-flannel-cfg $ kubectl describe --namespace=kube-system daemonsets/kube-flannel-ds setting up the gui kubernetes ships with a rich gui. you can install it by running: $ dashsrc=https://raw.githubusercontent.com/kubernetes/dashboard/master $ curl -ssl \\\\  $dashsrc/src/deploy/recommended/kubernetes-dashboard-arm-head.yaml \\\\  | kubectl apply -f - to access this ui, you can run kubectl proxy and then point your browser to http:// localhost:8001/ui, where localhost is local to the master node in your cluster. to view this from your laptop/desktop, you may need to set up an ssh tunnel to the root node using ssh -l8001:localhost:8001 <master-ip-address>. summary at this point you should have a working kubernetes cluster operating on your rasp‐ berry pis. this can be great for exploring kubernetes. schedule some jobs, open up the ui, and try breaking your cluster by rebooting machines or disconnecting the network. building a raspberry pi kubernetes cluster | 241  index a abstraction pattern, 205 admin role, 171 admission controllers, 196 built-in and custom, 196 creating ssl-based admission controller, 203 implementing to provide defaulting, 203 installing validating admission controller, 200 validating, 200 admissionreview type, 203 aggregating clusterroles, 173 ambassador ingress controller, 100 annotations, 65, 71-73 change-cause, 121 defining, 72 in deployment template for update, 119 kubectl annotate command, 40 kubernetes.io/created-by, 108 kubernetes.io/ingress.class, 97 uses of, 72 apis decoupling servers via, 6 kubernetes api server request flow, 196 kubernetes api server, versions, 31 application container images, 13 building with docker, 16-20 image security, 19 optimizing image size, 18 using dockerfiles, 16 multistage builds, 20 application containers, 16 application-oriented container apis, benefits of, 9 applications easy scaling for, 6 organizing kubernetes applications, 221-233 code review, 222 deploying your application worldwide, 230-233 feature flag gates and guards, 223 filesystem layout, 224 filesystems as source of truth, 222 guiding principles, 221 managing applications in source control, 224 managing releases, 225 parameterizing applications with tem‐ plates, 229 structuring for development, testing, and deployment, 227-228 versioning with branches and tags, 225 versioning with directories, 226 real-world, deploying, 207-219 ghost, 211-214 jupyter, 207-209 parse, 209-211 redis, 214-219 in same and different pods, 46 authentication, 168 authentication providers supported by kubernetes, 168 to container registries, 22 authorization, 168 testing with can-i, 172 243 autocompletion for commands and resources, 42 autoscaling, 6 replicasets, 110 based on cpu usage, 111 availability, 1 az cli tool, installing, 28 azure cloud shell, 28 azure kubernetes service, installing kuber‐ netes with, 28 b base64 encoding, 162 bash-completion package, 42 branches, versioning with, 226 build image, 21 c caching, using volumes for, 60 cert-manager project, 99 certificate signing request for kubernetes api server, 203 certificates tls, 99 creating secret to store, 158 webhook, 201 cgroup technology (linux kernel), 24 containers in pods, 46 change-cause annotation, 119, 121 cli (command-line interface) azure cloud shell, 28 deploying containers with docker cli, 23 elastic kubernetes service and eksctl tool, 29 clients client-go library, informers, 199 rolling service updates and, 124 cloud container registries for different cloud pro‐ viders, 22 dns name for databases and other services, 180 infrastructure provided by, drawbacks of, 9 ingress implementations by providers, 99 installing kubernetes on public cloud pro‐ vider, 28-29 azure kubernetes service, 28 google kubernetes engine (gke), 28 kubernetes services, 27 kubernetes-as-a-service (kaas) on public clouds, 8 load-balancing capabilities by providers, 90 loadbalancer type, using, 81 storage in, 178 using daemonsets to install software on nodes, 132 volume types for providers, 182 cloud-native applications, 1 cluster ips, 77 dns address for, 77 environment variables, 85 kube-proxy and, 84 cluster-admin permissions, 91 cluster-admin role, 171 clusterrolebindings, 170 built-in, 171 clusterroles, 170 aggregating, 173 binding a group to, 174 built-in, 171 modifications of built-in roles, 171 clusters autoscaling, 111 cloud-native, 132 components, 34-36 kubernetes dns server, 34 kubernetes proxy, 34 kubernetes ui, 35 deploying, 27-31 installing kubernetes locally, 29 installing kubernetes on public cloud provider, 28-29 running kubernetes in docker, 30 running kubernetes on raspberry pi, 31 easy scaling for, 6 exploring with kubectl, 31-34 checking cluster status, 31 listing worker nodes, 32 listing running pods in, 49 mongodb cluster creation, automating, 189-192 viewing with tools other than kubectl, 42 cname records (dns), 180 code review for applications, 222 command-line arguments, configmap used for, 155, 156 communication/synchronization, using vol‐ umes, 60 244 | index compilers, 205 compute costs, forecasting with kubernetes, 6 configmaps, 153-157 creating, 153 creating for ghost, 212 ghost-config-mysql, 213 creating for redis installation, 216 data values, 162 managing, 162 creating configmaps, 163 listing all configmaps in a namespace, 162 updates, 163-165 viewing raw data, 163 mongodb configmap, 190 naming constraints, key names for data items, 161 using, 154 using to add script to mongodb image, 190 configurations container configuration file, 16 declarative configuration in kubernetes, 4 declarative configuration in pod manifests, 47 for deployments, 117 for rolling update, 125 ghost application, 211 ingress controller configuration, typical, 90 kubectl configuration file, 37 managing for load balancer, 90 parameterizing, 229 redis installation, 215 consumers job, creating for a work queue, 149 container images, 14-16 cleaning up or removing, 24 docker format, 15 making reusable, 153 multistage builds, 20 storing in remote registry, 22 updating, 119 container network interface (cni) plug-ins, 196 container registries, 14 (see also registries) containers and container apis, benefits of, 2 application container images, 13 copying files to and from, 53 creating for jupyter application, 208 decoupling application container image from machines, 7 docker runtime, 23 existing containers, adoption by replicasets, 105 grouping in pods, 46 criteria for, 47 immutable container images, 3 quarantining, 105 readiness checks, 55 resource requests per container, 57 restart policy for mongodb container, 191 running in a pod, information about, 51 system and application, 16 contexts, managing with kubectl, 37 contour ingress controller, installing, 91 configuring dns, 92 configuring local hosts file, 92 controller-manager, 32 controllers for custom resources, 199 core-dns server, 35 cpu-shares functionality (linux kernel), 58 cpus autoscaling based on usage, 111 capping usage with resource limits, 59 limiting usage with docker, 24 resource requests for, 57 cron jobs declaring a cronjob in kubernetes, 150 setting up to run image garbage collector, 25 curl utility, using to communicate with work queue, 148 custom resources, 197 naming, 197 patterns for, 204-205 compilers, 205 just data, 204 operators, 205 validation and defaulting, 200 customresourcedefinition, 91, 197 example definition, 197 spec subobject, 197 d daemonsets, 34, 131-138 creating, 132-134 deleting, 137 limiting to specific nodes, 134-136 adding labels to nodes, 135 index | 245 using node selectors, 135 scheduler, 132 similarities with replicasets, 131 updating, 136 rollingupdate strategy, 136 dashboards for worldwide application deployment, 233 kubernetes ui, 35 data items, specifying for configmaps or secrets, 163 debugging, kubectl commands for, 40-41, 52 declarative configuration, 47 and undoing rollouts, 122 declarative configuration objects, 4 decoupled architectures, 5 daemonsets and replicasets in kubernetes, 132 in kubernetes, 104 default-http-backend service, 95 dependencies, 19 deployment image, 21 deployment object, 72, 113 (see also deployments) deleting only the deployment object, 128 revision history attached to, 122 deployments, 113-129 creating, 116 creating with kubectl run, 76 deleting, 128 deploying real-world applications, 207-219 ghost, 211-214 jupyter, 207-209 parse, 209-211 redis, 214-219 deploying your application around the world, 230-233 architectures for worldwide deployment, 230 dashboards and monitoring, 233 implementing worldwide deployment, 232 editing to add readiness check, 78 internal workings of, 114-115 kubernetes deployment lifecycle, 128 managing, 117 monitoring, 128 strategies for, 123 recreate strategy, 123 rollingupdate strategy, 123-126 slowing rollouts to ensure service health, 126 updating, 118-123 container image, 119 rollout history, 120 scaling deployments, 118 development teams, scaling with microservices, 7 development, structuring your application for, 227 introducing a development tag, 228 directories, versioning with, 226 disk space on a node, getting information about, 33 distributed systems, reliable and scalable, 1 dns address for cluster ip, 77 configuring to external address for load bal‐ ancer, 92 entries for mongodb statefulset, 188 kubernetes dns server, 34 kubernetes dns service, 77 name resolution, limitations of, 75 names for external services, 179 docker, 14 building application images with, 16-20 cli tool, deploying container with, 23 container runtime, 23 image format, 15 private registries, storing access credentials, 160 running kubernetes in, 30 docker desktop, kubernetes installation with, 29 docker hub, 22 docker images command, 25 docker login command, 22 docker rmi command, 24 docker run command --cpu-shares flag, 24 --memory and --memory-swap flags, 24 --publish, -d, and --name flags, 23 docker system prune tool, 25 docker-in-docker clusters, 27 dockerfiles, 17 for multistage application builds, 21 .dockerignore file, 17 dynamic volume provisioning, 185 246 | index e eclipse, 42 edit role, 171 in clusterrole aggregation, 173 editors editing a deployment in, 78 plug-ins for integration with kubernetes, 42 efficiency provided by kubernetes, 10 eks (elastic kubernetes service), 29 eksctl command-line tool, 29 elasticsearch tool, 53 endpoints, 82 for default http backend service, 95 for external services, 180 watch command on, 79 environment variables for cluster ips, 85 for parse-server, 209 setting using configmap, 155, 156 envoy load balancer, 91 etcd server, 32 events related to pods, 50 getting information on, 51 killing a container, 55 exec probes, 56 running commands in your container with, 53 extending kubernetes, 195-206 getting started, using kubebuilder project library, 205 meaning of, 195 patterns for custom resources, 204-205 points of extensibility, 196-204 externalname type, 179 f feature flag gates and guards, 223 filesystems configmap defining small filesystem, 153, 154 layout for kubernetes application, 224 layout for parameterization, 230 mounting host filesystem using volumes, 61 overlay, 15 source of truth for kubernetes applications, 222 fluentd tool, 53 creating fluentd logging agent on every node in target cluster, 132-134 forecasting future compute costs, 6 g garbage collection, setting up for container images, 25 gcloud tool, installing, 28 ghost, 211-214 configuring, 211 using mysql for storage, 213 github cert-manager on, 99 minikube on, 30 parse-server page, 211 gloo ingress controller, 100 google cloud platform, 28 google container registry, 22 google kubernetes engine (gke), 28, 81 grace period for pod termination, 51 graceful shutdown, 79 groups, 168 identity and, 174 using for role bindings, 173 h headless services, 188 health checks, 54-56 external services and, 181 for replicated redis cluster, 214 liveness probes, 54 readiness probe, 55, 78, 127 tcpsocket and exec probes, 56 heapster pod, 110 helm, parameterizing configurations with, 229 heptio-contour namespace, 91 horizontal pod autoscaling (hpa), 110 no direct link between replicasets and, 111 horizontal vs. vertical scaling, 111 hostnames ingress and namespaces, 97 using with ingress, 94 hostpath volume, 61 hosts configuring local hosts file for contour, 92 multiple paths on same host in ingress sys‐ tem, 96 http health checks, 54, 79 http load balancing (see load balancing) https, webhook access via, 201 hypervisors, 30 index | 247 i identity in kubernetes, 168 groups, 174 image pull secrets, 161 immutability declarative configuration and, 4 value of, 3 imperative vs. declarative configuration, 4, 47 informer pattern, 199 infrastructure abstracting, 9 as code, 4 immutable, 3, 132 ingress, 7, 89-101 advanced topics and gotchas, 96-99 ingress and namespaces, 97 multiple ingress objects, 97 path rewriting, 98 running multiple ingress controllers, 97 serving tls, 98 alternate implementations, 99 installing contour controller, 91 configuring dns, 92 configuring local hosts file, 92 resource specification vs. controller imple‐ mentation, 90 typical software ingress controller configu‐ ration, 90 using, 92 hostnames, 94 paths, 95 simplest usage, 93 intellij, 42 ip addresses cluster ip, 77 external ip for contour, 91 external services and, 179 for watched service endpoints, 82 hosting many http sites on single address, 89 kube-proxy and cluster ips, 84 unique, for services of type loadbalancer, 92 iptables rules, 84 istio project, 100 j jobs, 139-151 cronjob, 150 job object, 139 patterns, 140-150 work queues, 146-150 json files representing kubernetes objects, 39 work queue items, 148 jupyter, 207-209 just data pattern, 204 k kaas (kubernetes-as-a-service), 8 kube-apiserver, --service-cluster-ip-range flag, 85 kube-dns server, 35 kube-proxy, 34, 86 cluster ips and, 84 kube-system namespace, 34, 95 listing pods in, 110 kubeadm, 27 kubebuilder project, library for kubernetes api extensions, 205 kubectl tool, 31-34 checking cluster status, 31 commands, 37-43 apply, 49, 91, 93, 107, 110 apply -f, 164 auth can-i, 172 auth reconcile, 172 autocompletion, 42 autoscale, 111 contexts, 37 cp, 53 create, 114 create configmap, 163 create secret, 159, 163 create secret docker-registry, 161 create secret generic, 164 create secret tls, 98 creating, updating, and destroying kubernetes objects, 39 debugging, 40-41 delete, 48, 51, 111, 128, 137 describe, 50, 80, 93, 108, 117, 159 edit, 78, 80 edit configmap, 165 exec, 53 expose, 76 get, 48, 49, 68, 93 get clusterrolebindings, 171 248 | index get clusterroles, 171 get configmaps, 162 get secrets, 162 label, 68, 135 labeling and annotating objects, 40 logs, 52 namespaces, 37 port-forward, 52, 77 replace --save-config, 117 replace -f, 164, 164 rolling-update, 114 rollout, 118, 120, 137 rollout history deployment, 120 rollout pause deployments, 120 rollout resume deployments, 120 rollout undo deployments, 121 run, 48 scale, 109, 115 viewing kubernetes api objects, 38 default-http-backend service, 95 getting more information via command-line flags, 50 installing, 29, 29 listing worker nodes on a cluster, 32 kubelet tool, 49 managing secrets volumes, 159 terminating containers for excess memory usage, 58 kubernetes objects, 38, 39 (see also objects (kubernetes)) kubernetes service, 76 kubernetes-as-a-service (kaas), 8 kubernetes.io/created-by annotation, 108 kubernetes.io/ingress.class annotation, 97 l labels, 65-71 adding to nodes, 135 applying, 67 deployment object, 114 external services without label selectors, 179 for kubernetes objects, using kubectl, 40 for pods managed by a replicaset, 108 for pods, use by replicasets, 107 in kubernetes architecture, 71 key/value pairs, rules for, 66 modifying, 68 modifying on sick pod, 105 motivations for using, 66 node selectors, 135 selectors, 68, 76 in api objects, 70 using in service discovery, 83 using to run daemonset pods on specific nodes, 131 “let\\'s encrypt” free certificate authority, 99 libraries, external and shared, 13 lifecycle stages, mapping to revisions, 228 linkerd project, 100 live updates, 165 liveness probes, 54 load balancers cloud-based, configuring with ingress objects, 99 decoupling components via, 6 load balancing, 7 for kubernetes dashboard server, 35 for kubernetes dns server, 35 http load balancing with ingress, 89-101 alternate ingress implementations, 99 ingress spec vs. ingress controllers, 90 installing contour controller, 91-92 using ingress, 92-96 loadbalancer type, 81, 89 loadtest custom resource, 198 creating, 199 validation, 200 logging agent fluentd, creating on every node in target cluster, 132 logical and, 69 login token for jupyter application, 208 logs for jupyter application container, 208 getting information on pods from, 52 log aggregation services, 53 testing with auth can-i, 172 m machine/operating system (os), decoupling from application container, 8 manifests (pod), 47 adding volume to, 59 created by replicasets, 106 creating, 48 declaring secrets volume, 160 using to delete a pod, 51 master and slave replicas, redis installation, 215 index | 249 master nodes, 32 maxsurge parameter (rolling updates), 126 maxunavailable parameter (rolling updates), 125, 137 memory capping usage with resource limits, 59 limiting usage for applications in container, 24 on a node, getting information about, 33 requests for, 58 meta-resources, 197 metadata labels providing metadata for objects, 65 metadata section in kubernetes objects, annotation definitions in, 73 metadata section (pod manifests), 49 microservices, 6 building with kubernetes, advantages of, 7 represented by replicasets, 105 minikube, 27 minikube tunnel command, 91 using to install kubernetes locally, 29 minreadyseconds parameter for daemonsets, 137 for deployments), 127 mobile application, accessing kubernetes clus‐ ter from your phone, 42 modularity in kubernetes, 104 mongodb cluster creation, automating, 189-192 manually replicated with statefulsets, 187-189 readiness probes for mongo-serving con‐ tainers, 193 use of mongodb cluster by parse, 209 multistage image builds, 20 multitenant security, 167 mutable vs. immutable infrastructure, 3 mutatingwebhookconfiguration, 203 mysql databases running a mysql singleton, 181-185 using with ghost application, 213 n namespaces, 7, 11 creating for jupyter application, 207 default, changing with kubectl, 37 deployment of kubernetes objects into, 178 heptio-contour, 91 in annotation keys, 72 in kubectl commands, 37 ingress and, 97 network traffic, restricting in a cluster, 71 network-based storage, 61 networkpolicy, 71 newreplicaset, 118 nfs persistent volume object, 182 nginx ingress controller, 100 nodeports, 80, 89 services type for installing contour, 91 nodes limiting daemonsets to specific nodes, 134-136 adding labels to nodes, 135 using node selectors, 135 listing worker nodes for kubernetes cluster, 32 nodename field in pod specs, 132 resource use by, monitoring with kubectl, 41 “not my monkey, not my circus” line, 8 o objects (kubernetes), 38 annotations, 71-73 creating, updating, and destroying, 39 labeling and annotating with kubectl, 40 labels providing metadata for, 65 selectors in, 70 oldreplicasets, 118 openapi, 200 operations teams, decoupling of, 8 operators extensions using, 205 health detection and healing with, 5 osi model, 89 overlay filesystems, 15 p parse application, 209-211 building the parse-server, 209 deploying the parse-server, 209 prerequisites for, 209 testing, 210 paths in http requests, using with ingress, 95 ingress and namespaces, 97 rewriting with ingress controller, 98 pending state, 50 250 | index persistent volume claim template, 193 persistentvolume, 10 persistentvolumeclaim, 10, 183 reclamation policy, and lifespan of persis‐ tent volumes, 186 referring to a storage class, 185 persistentvolumes, 51 persisting data with volumes, 59-61 pods, 7, 45-63 accessing, 52 copying files to/from containers, 53 getting more information with logs, 52 running commands in container with exec, 53 using port forwarding, 52 configmaps and, 153 created by replicaset using pod template, 106 creating via kubectl run command, 48 currently running in a cluster, listing and showing labels, 69 currently running on a node, getting infor‐ mation about, 33 daemonsets determining which node pods run on, 132 decoupling from daemonsets, 132 deleting, 51 designing, question to ask, 46 example pod with two containers and shared filesystem, 45 finding a replicaset from, 108 finding set of pods for a replicaset, 108 getting details about, 50 health checks, 54-56 liveness probes, 54 other types of, 56 readiness probe, 55, 78, 127 horizontal pod autoscaling (hpa), 110 imagepullsecrets spec field, 161 in kubernetes, 46 managed by daemonsets, deleting (or not), 137 managed by replicasets automatic rescheduling in failures, 103 deleting, 111, 128 manifests, 47 creating, 48 declaring secrets volume, 160 maximum number unavailable during roll‐ ing updates, 125, 137 node selectors in pod spec when creating daemonsets, 135 persisting data with volumes, 59-61 adding volumes to pods, 59 using volumes for communication/ synchronization, 60 pod template in deployment specs, 117 relating pods and replicasets, 104 replicated sets of, 103 (see also replicasets) replicating a set of, reasons for, 131 resource management for, 56-59 capping usage with resource limits, 58 monitoring resource use with kubectl, 41 resource requests and minimum required resources, 56 running, 49 in parallel for consumers job, 149 listing running pods in a cluster, 49 singleton pod running mysql, 184 updating in rollingupdate strategy, 123 port forwarding, 77 for updated deployment definition, 79 setting up for jupyter container, 208 testing with auth can-i, 172 using to access a pod, 52 using to connect to work queue daemon, 147 port-forward command (kubectl), 41 ports in deployment definitions, 76 nodeport feature, 80 private container registries, 22 storing access credentials for private docker registries, 160 process health checks, 54 (see also health checks) progressdeadlineseconds parameter (deploy‐ ments), 127, 129 progressing type, 129 proxy (kubernetes), 34 (see also kube-proxy) public container registries, 22 r raspberry pi building a kubernetes cluster, 235-241 index | 251 running kubernetes on, 31 rbac (see role-based access control) rbac.authorization.kubernetes.io/autoupdate annotation, 171 readiness probes, 55, 127 for mongo-serving containers, 193 for services, 78 reconciliation loops, 104, 132 recreate strategy, 123 redis, 214-219 configuring, 215 creating a redis service, 216 deploying redis cluster, 217 redis-sentinel, 214 redis-server, 214 testing our redis cluster, 218 registries (container), 14 private docker registries, storing access cre‐ dentials, 160 storing container images in remote registry, 22 regular expressions for key names in configmap or secret key, 161 for path rewriting by ingress controllers, 98 releases, progression of, 227-228 development tag, 228 mapping stages to revisions, 228 reliability, 1 remote disks, persisting data with, 61 replicas (statefulsets), 187 replicasets, 48, 103-112 autoscaling no direct link between hpa and replica‐ sets, 111 creating, 107 creating for mysql singleton pod, 183 creating to manage singleton work queue daemon, 146 deleting, 111 designing with, 105 for mongodb, 189 inspecting, 108 finding a replicaset from a pod, 108 finding a set of pods for a replicaset, 108 managed by a deployment, 114 deleting, 128 fields pointing to, 118 old and new, managed by a deployment, 120 reconciliation loops, 104 relating pods and replicasets, 104 relationship between deployments and, 115 scaling, 109-111 autoscaling, 110 declarative scaling with kubectl apply, 109 imperative scaling with kubectl scale, 109 similarities with daemonsets, 131 specification for, 106 labels, 107 pod templates, 106 resource management, 56-59 capping usage with resource limits, 58 resource requests and minimum required resources, 56 request limit details, 57 resources custom, 197 (see also custom resources) external, connecting to kubernetes services, 86 isolation with containers, 46 limiting applications’ usage, 24 monitoring use of with kubectl top com‐ mand, 41 rest api, kubernetes, 159 restart policy for pods, 55, 191 revisionhistorylimit property, 123 revisions, 120 mapping to stages, 228 specifying revision of 0, 122 role object, 169 role-based access control (rbac), 167-175 advanced topics aggregating clusterroles, 173 using groups for bindings, 173 general concept of roles and role bindings, 169 identity in kubernetes, 168 multitenant security and, 167 roles and role bindings in kubernetes, 169 auto-reconciliation of built-in roles, 171 using built-in roles, 171 verbs for kubernetes roles, 170 techniques for managing, 172 managing in source control, 172 252 | index testing authorization with can-i, 172 rolebinding object, 169 creating, 170 rolling updates, 114 rollingupdate strategy, 123 configuring a rolling update, 125 managing multiple versions of your service, 124 using with daemonsets, 136 rollout commands for deployments, 118 rollouts current status of daemonset rollout, 137 history of, 120 and undoing a deployment, 122 monitoring, 120 pausing, 120 resuming, 120 slowing to ensure service health, 126 undoing last rollout, 121 root path (/), 96 s scalability, 1 scaling your service and your teams, 5-9 decoupled architectures, 6 easy scaling for applications and clusters, 6 scaling development teams with micro‐ services, 7 separation of concerns for consistency and scaling, 8 scaling deployments, 115, 118 replicasets, 109-111, 115 autoscaling, 110 declarative scaling with kubectl apply, 109 imperative scaling with kubectl scale, 109 scheduler, 32 daemonsets and, 132 placing pods onto nodes, 48 secrets, 158-161 consuming, 159 secrets volumes, 159 storing access credentials for private docker registries, 160 creating, 158 data values, 162 managing, 162 creating secrets, 163 listing all secrets in current namespace, 162 updates, 163-165 viewing raw data, 163 naming constraints, key names for data items, 161 specifying with tls certificate and keys, 98 security, 167 (see also authentication; role-based access control; secrets) for application container images, 19 selectors (label), 68, 76 filtering nodes based on labels, 135 finding pods matching, 108 identifying clusterroles to be aggregated, 173 in api objects, 70 in replicaset spec section, 107 in the kubernetes architecture, 71 selector operators, 70 self-healing systems, 5 separation of concerns, 8 service accounts, 168 service discovery, 75-87 advanced details, 82-85 cluster ip environment variables, 85 endpoints, 82 kube-proxy and cluster ips, 84 manual service discovery, 83 cloud integration, 81 connecting with other environments, 86 defined, 75 looking beyond the cluster, 79 readiness checks, 78 service dns, 77 service object, 76 service meshes, 100 service object, 84 endpoints object for, 82 operating at osi level 4, 89 services backend, creating, 92 creating redis service, 216 default-http-backend, 95 ensuring health by slowing rollouts, 126 exposing ghost service, 213 exposing mysql singleton as, 184 index | 253 hosting multiple services on paths of a sin‐ gle domain, 95 importing external storage services, 178-181 limitation, no health checking, 181 services without label selectors, 179 ingress and namespaces, 97 kubernetes, 7 managing multiple versions during rolling updates, 124 of type loadbalancer, 90 (see also ingress; loadbalancer type) queue service, creating, 147 stateless, replicasets designed for, 106 shutdown, graceful, 79 singletons running reliable singletons for storage, 181-186 dynamic volume provisioning, 185 software on a node, getting information about, 33 source control managing rbac in, 172 managing your application in, 224 storing declarative configuration in, 4 spec for customresourcedefinition, 197 spec for replicasets, 106 spec section (pod manifests), 49 spec.type field, 80, 81 spec.volume section (pod manifest), 59 ssh tunneling, 80 state daemonset management of, 132 desired state of a deployment, matching, 115 of all replicas managed by a replicaset, 108 reconciliation loop approach to managing, 104 stateless services, 106 updating desired state of a deployment, 119 statefulsets, 186-194 for redis cluster, 217 manually replicated mongodb with, 187-189 mongodb cluster creation, automating, 189-192 persistent volumes and, 192 properties of, 187 redis deployment, wrapper scripts for, 215 status.conditions array (deployments), 128 storage solutions, integrating with kubernetes, 177-194 importing external services, 178-181 limitation, no health checking, 181 services without selectors, 179 mysql database for ghost storage, 213 native storage with statefulsets, 186-194 manually replicated mongodb, 187-189 persistent volumes and statefulsets, 192 readiness probes for mongo-serving containers, 193 running reliable singletons, 181-186 dynamic volume provisioning, 185 mysql singleton, 181-185 storageclass objects, 185 strategy object, 117 system containers, 16 system daemons, deploying, 131 (see also daemonsets) system:unauthenticated group, 168 cluster role allowing access to api server, 171 t tab completion for commands and resources, 42 tags, source-control, 226 tcp, 89 tcpsocket health checks, 56 templates annotation in template for deployment with update information, 119 for replicasets and pods, 68 (see also deployments) parameterizing applications with, 229 pod template in deployment specs, 117 terminating state, 51 testing creating test environments with kubernetes, 11 structuring your application for, 227 timing out a rollout, 127, 128 tls key and certificate, creating secret to store, 158 tls, serving in ingress system, 98 tokens (login) for jupyter application, 208 traefik, 100 254 | index u udp, 89 ui (kubernetes), 35 undoing rollouts, 121 update strategy, configuring for daemonsets, 136 updating configmaps or secrets, 163 editing configmap current version, 164 live updates, 165 recreate and update, 164 update from file, 164 user accounts, 168 utf-8 text, configmap data values, 162 utilization, 56 v validatingwebhookconfiguration, 200 validation, adding to custom resource, 200 velocity in software development, 2 declarative configuration, 4 immutability, value of, 3 self-healing systems, 5 versioning in kubernetes, 31 managing periodic versions of applications, 225 using directories, 226 with branches and tags, 225 vertical vs. horizontal scaling, 111 view role, 171 virtual hosting, 89 virtualbox, 30 visual studio code, 42 volume.beta.kubernetes.io/storage-class anno‐ tation, 186 volumes, persisting data with, 59-61 configmap volume, creating, 155 different ways to use volumes with pods, 60 caching, 60 communication/synchronization, 60 dynamic volume provisioning, 185 mongo-init configmap volume, 190 persistent volume for mysql singleton, 182 persistent volumes and statefulsets, 192 secrets volumes, 159 using volumes with pods, 59 w watch command, 79, 208 watches, 199 webhooks mutatingwebhookconfiguration, 203 validatingwebhookconfiguration, 200 work queues, 146-150 creating the consumers job, 149 loading up, 148 starting, 146 worker nodes, listing for kubernetes cluster, 32 y yaml deployment.yaml file, 114, 116 files representing kubernetes objects, 39 for configmap object, 154 host-ingress.yaml file, 94 path-ingress.yaml file, 95 simple-ingress.yaml file, 93 tls-secret.yaml file, 98 index | 255 about the authors brendan burns began his career with a brief stint in the software industry followed by a phd in robotics focused on motion planning for human-like robot arms. this was followed by a brief stint as a professor of computer science. eventually, he returned to seattle and joined google, where he worked on web search infrastructure with a special focus on low-latency indexing. while at google, he created the kuber‐ netes project with joe beda and craig mcluckie. brendan is currently a director of engineering at microsoft azure. joe beda started his career at microsoft working on internet explorer (he was young and naive). throughout 7 years at microsoft and 10 at google, joe has worked on gui frameworks, real-time voice and chat, telephony, machine learning for ads, and cloud computing. most notably, while at google, joe started the google compute engine and, along with brendan burns and craig mcluckie, created kubernetes. along with craig, joe founded and sold a startup (heptio) to vmware, where he is now a principal engineer. joe proudly calls seattle home. kelsey hightower is a principal developer advocate at google working on google’s cloud platform. he has helped develop and refine many google cloud products including google’s kubernetes engine, cloud functions, and apigees’s api gateway. kelsey spends most of his time with executives and developers spanning the global fortune 1000, helping them understand and leverage google technologies and plat‐ forms to grow their businesses. kelsey is a huge open source contributor, maintaining projects that aid software developers and operations professionals in building and shipping cloud native applications. he is an accomplished author and keynote speaker, and was the inaugural winner of the cncf top ambassador award for help‐ ing bootstrap the kubernetes community. he is a mentor and technical advisor, help‐ ing founders turn their visions into reality. colophon the animal on the cover of kubernetes: up and running is an atlantic white-sided dolphin (lagenorhynchus acutus). as its name suggests, the white-sided dolphin has light patches on its sides and a light gray strip that runs from above the eye to below the dorsal fin. it is among the largest species of oceanic dolphins, and ranges throughout the north atlantic ocean. it prefers open water, so it is not often seen from the shore, but will readily approach boats and perform various acrobatic feats. white-sided dolphins are social animals commonly found in large groups (known as pods) of about 60 individuals, though the size will vary depending on location and the availability of food. dolphins often work as a team to harvest schools of fish, but they also hunt individually. they primarily search for prey using echolocation, which is similar to sonar. the bulk of this marine mammal’s diet consists of herring, mack‐ erel, and squid. the average lifespan of the white-sided dolphin is between 22–27 years. females only mate every 2–3 years, and the gestation period is 11 months. calves are typically born in june or july, and are weaned after 18 months. dolphins have very great intelligence and display complex social behaviors like grieving, cooperation, and problem solving, due to their high brain-to-body ratio (the highest among aquatic mammals). many of the animals on o’reilly covers are endangered; all of them are important to the world. the cover illustration is by karen montgomery, based on a black and white engraving from british quadrupeds. the cover fonts are gilroy semibold and guardian sans. the text font is adobe minion pro; the heading font is adobe myriad condensed; and the code font is dalton maag’s ubuntu mono. there’s much more where this came from. experience books, videos, live online training courses, and more from o’reilly and our 200+ partners—all in one place. learn more at oreilly.com/online-learning ©2019 o’reilly media, inc. o’reilly is a registered trademark of o’reilly media, inc. | 175'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bin/corpus.pkl', 'wb') as file:\n",
    "    pickle.dump(text, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transcript': 'brendan burns, joe beda & kelsey hightower kubernetes  up & running dive into the future of infrastructure second edition  brendan burns, joe beda, and kelsey hightower kubernetes: up and running dive into the future of infrastructure second edition beijing boston farnham sebastopol tokyo 978-1-492-04653-0  kubernetes: up and running by brendan burns, joe beda, and kelsey hightower copyright © 2019 brendan burns, joe beda, and kelsey hightower. all rights reserved. printed in the united states of america. published by o’reilly media, inc., 1005 gravenstein highway north, sebastopol, ca 95472. o’reilly books may be purchased for educational, business, or sales promotional use. online editions are also available for most titles (http://oreilly.com). for more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com. acquisition editor: john devins development editor: virginia wilson production editor: kristen brown copyeditor: kim cofer proofreader: rachel head indexer: ellen troutman-zaig interior designer: david futato cover designer: karen montgomery illustrator: rebecca demarest september 2017: first edition august 2019: second edition revision history for the second edition 2019-07-15: first release 2019-10-04: second release see http://oreilly.com/catalog/errata.csp?isbn=9781492046530 for release details. the o’reilly logo is a registered trademark of o’reilly media, inc. kubernetes: up and running, the cover image, and related trade dress are trademarks of o’reilly media, inc. the views expressed in this work are those of the authors, and do not represent the publisher’s views. while the publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. use of the information and instructions contained in this work is at your own risk. if any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights. this work is part of a collaboration between o’reilly and microsoft. see our statement of editorial inde‐ pendence. for robin, julia, ethan, and everyone who bought cookies to pay for that commodore 64 in my third-grade class. —brendan burns for my dad, who helped me fall in love with computers by bringing home punch cards and dot matrix banners. —joe beda for klarissa and kelis, who keep me sane. and for my mom, who taught me a strong work ethic and how to rise above all odds. —kelsey hightower  table of contents preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii 1. introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 velocity 2 the value of immutability 3 declarative configuration 4 self-healing systems 5 scaling your service and your teams 5 decoupling 6 easy scaling for applications and clusters 6 scaling development teams with microservices 7 separation of concerns for consistency and scaling 8 abstracting your infrastructure 9 efficiency 10 summary 11 2. creating and running containers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 container images 14 the docker image format 15 building application images with docker 16 dockerfiles 16 optimizing image sizes 18 image security 19 multistage image builds 20 storing images in a remote registry 22 the docker container runtime 23 running containers with docker 23 exploring the kuard application 23 v limiting resource usage 24 cleanup 24 summary 25 3. deploying a kubernetes cluster. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 installing kubernetes on a public cloud provider 28 google kubernetes engine 28 installing kubernetes with azure kubernetes service 28 installing kubernetes on amazon web services 29 installing kubernetes locally using minikube 29 running kubernetes in docker 30 running kubernetes on raspberry pi 31 the kubernetes client 31 checking cluster status 31 listing kubernetes worker nodes 32 cluster components 34 kubernetes proxy 34 kubernetes dns 34 kubernetes ui 35 summary 36 4. common kubectl commands. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 namespaces 37 contexts 37 viewing kubernetes api objects 38 creating, updating, and destroying kubernetes objects 39 labeling and annotating objects 40 debugging commands 40 command autocompletion 42 alternative ways of viewing your cluster 42 summary 43 5. pods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 pods in kubernetes 46 thinking with pods 46 the pod manifest 47 creating a pod 48 creating a pod manifest 48 running pods 49 listing pods 49 pod details 50 deleting a pod 51 vi | table of contents accessing your pod 52 using port forwarding 52 getting more info with logs 52 running commands in your container with exec 53 copying files to and from containers 53 health checks 54 liveness probe 54 readiness probe 55 types of health checks 56 resource management 56 resource requests: minimum required resources 56 capping resource usage with limits 58 persisting data with volumes 59 using volumes with pods 59 different ways of using volumes with pods 60 persisting data using remote disks 61 putting it all together 61 summary 63 6. labels and annotations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 labels 65 applying labels 67 modifying labels 68 label selectors 68 label selectors in api objects 70 labels in the kubernetes architecture 71 annotations 71 defining annotations 72 cleanup 73 summary 73 7. service discovery. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 what is service discovery? 75 the service object 76 service dns 77 readiness checks 78 looking beyond the cluster 79 cloud integration 81 advanced details 82 endpoints 82 manual service discovery 83 kube-proxy and cluster ips 84 table of contents | vii cluster ip environment variables 85 connecting with other environments 86 cleanup 86 summary 86 8. http load balancing with ingress. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 ingress spec versus ingress controllers 90 installing contour 91 configuring dns 92 configuring a local hosts file 92 using ingress 92 simplest usage 93 using hostnames 94 using paths 95 cleaning up 96 advanced ingress topics and gotchas 96 running multiple ingress controllers 97 multiple ingress objects 97 ingress and namespaces 97 path rewriting 98 serving tls 98 alternate ingress implementations 99 the future of ingress 100 summary 101 9. replicasets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 reconciliation loops 104 relating pods and replicasets 104 adopting existing containers 105 quarantining containers 105 designing with replicasets 105 replicaset spec 106 pod templates 106 labels 107 creating a replicaset 107 inspecting a replicaset 108 finding a replicaset from a pod 108 finding a set of pods for a replicaset 108 scaling replicasets 109 imperative scaling with kubectl scale 109 declaratively scaling with kubectl apply 109 autoscaling a replicaset 110 viii | table of contents deleting replicasets 111 summary 112 10. deployments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 your first deployment 114 deployment internals 114 creating deployments 116 managing deployments 117 updating deployments 118 scaling a deployment 118 updating a container image 119 rollout history 120 deployment strategies 123 recreate strategy 123 rollingupdate strategy 123 slowing rollouts to ensure service health 126 deleting a deployment 128 monitoring a deployment 128 summary 129 11. daemonsets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131 daemonset scheduler 132 creating daemonsets 132 limiting daemonsets to specific nodes 134 adding labels to nodes 135 node selectors 135 updating a daemonset 136 rolling update of a daemonset 136 deleting a daemonset 137 summary 138 12. jobs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 the job object 139 job patterns 140 one shot 140 parallelism 144 work queues 146 cronjobs 150 summary 151 13. con\\x80gmaps and secrets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 configmaps 153 table of contents | ix creating configmaps 153 using a configmap 154 secrets 157 creating secrets 158 consuming secrets 159 private docker registries 160 naming constraints 161 managing configmaps and secrets 162 listing 162 creating 163 updating 163 summary 165 14. role-based access control for kubernetes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167 role-based access control 168 identity in kubernetes 168 understanding roles and role bindings 169 roles and role bindings in kubernetes 169 techniques for managing rbac 172 testing authorization with can-i 172 managing rbac in source control 172 advanced topics 172 aggregating clusterroles 173 using groups for bindings 173 summary 175 15. integrating storage solutions and kubernetes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177 importing external services 178 services without selectors 179 limitations of external services: health checking 181 running reliable singletons 181 running a mysql singleton 181 dynamic volume provisioning 185 kubernetes-native storage with statefulsets 186 properties of statefulsets 187 manually replicated mongodb with statefulsets 187 automating mongodb cluster creation 189 persistent volumes and statefulsets 192 one final thing: readiness probes 193 summary 194 x | table of contents 16. extending kubernetes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195 what it means to extend kubernetes 195 points of extensibility 196 patterns for custom resources 204 just data 204 compilers 205 operators 205 getting started 205 summary 205 17. deploying real-world applications. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 jupyter 207 parse 209 prerequisites 209 building the parse-server 209 deploying the parse-server 209 testing parse 210 ghost 211 configuring ghost 211 redis 214 configuring redis 215 creating a redis service 216 deploying redis 217 playing with our redis cluster 218 summary 219 18. organizing your application. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221 principles to guide us 221 filesystems as the source of truth 222 the role of code review 222 feature gates and guards 223 managing your application in source control 224 filesystem layout 224 managing periodic versions 225 structuring your application for development, testing, and deployment 227 goals 227 progression of a release 227 parameterizing your application with templates 229 parameterizing with helm and templates 229 filesystem layout for parameterization 230 deploying your application around the world 230 architectures for worldwide deployment 230 table of contents | xi implementing worldwide deployment 232 dashboards and monitoring for worldwide deployments 233 summary 233 a. building a raspberry pi kubernetes cluster. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235 index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243 xii | table of contents preface kubernetes: a dedication kubernetes would like to thank every sysadmin who has woken up at 3 a.m. to restart a process. every developer who pushed code to production only to find that it didn’t run like it did on their laptop. every systems architect who mistakenly pointed a load test at the production service because of a leftover hostname that they hadn’t updated. it was the pain, the weird hours, and the weird errors that inspired the development of kubernetes. in a single sentence: kubernetes intends to radically simplify the task of building, deploying, and maintaining distributed systems. it has been inspired by decades of real-world experience building reliable systems and it has been designed from the ground up to make that experience if not euphoric, at least pleasant. we hope you enjoy the book! who should read this book whether you are new to distributed systems or have been deploying cloud-native sys‐ tems for years, containers and kubernetes can help you achieve new levels of velocity, agility, reliability, and efficiency. this book describes the kubernetes cluster orches‐ trator and how its tools and apis can be used to improve the development, delivery, and maintenance of distributed applications. though no previous experience with kubernetes is assumed, to make maximal use of the book you should be comfortable building and deploying server-based applications. familiarity with concepts like load balancers and network storage will be useful, though not required. likewise, experi‐ ence with linux, linux containers, and docker, though not essential, will help you make the most of this book. xiii why we wrote this book we have been involved with kubernetes since its very beginnings. it has been truly remarkable to watch it transform from a curiosity largely used in experiments to a crucial production-grade infrastructure that powers large-scale production applica‐ tions in varied fields, from machine learning to online services. as this transition occurred, it became increasingly clear that a book that captured both how to use the core concepts in kubernetes and the motivations behind the development of those concepts would be an important contribution to the state of cloud-native application development. we hope that in reading this book, you not only learn how to build reli‐ able, scalable applications on top of kubernetes but also receive insight into the core challenges of distributed systems that led to its development. why we updated this book in the few years that have passed since we wrote the first edition of this book, the kubernetes ecosystem has blossomed and evolved. kubernetes itself has had many releases, and many more tools and patterns for using kubernetes have become de facto standards. in updating the book we added material on http load balancing, role-based access control (rbac), extending the kubernetes api, how to organize your application in source control, and more. we also updated all of the existing chapters to reflect the changes and evolution in kubernetes since the first edition. we fully expect to revise this book again in a few years (and look forward to doing so) as kubernetes continues to evolve. a word on cloud-native applications today from the first programming languages, to object-oriented programming, to the development of virtualization and cloud infrastructure, the history of computer sci‐ ence is a history of the development of abstractions that hide complexity and empower you to build ever more sophisticated applications. despite this, the develop‐ ment of reliable, scalable applications is still dramatically more challenging than it ought to be. in recent years, containers and container orchestration apis like kuber‐ netes have proven to be an important abstraction that radically simplifies the devel‐ opment of reliable, scalable distributed systems. though containers and orchestrators are still in the process of entering the mainstream, they are already enabling develop‐ ers to build and deploy applications with a speed, agility, and reliability that would have seemed like science fiction only a few years ago. xiv | preface navigating this book this book is organized as follows. chapter 1 outlines the high-level benefits of kuber‐ netes without diving too deeply into the details. if you are new to kubernetes, this is a great place to start to understand why you should read the rest of the book. chapter 2 provides a detailed introduction to containers and containerized applica‐ tion development. if you’ve never really played around with docker before, this chap‐ ter will be a useful introduction. if you are already a docker expert, it will likely be mostly review. chapter 3 covers how to deploy kubernetes. while most of this book focuses on how to use kubernetes, you need to get a cluster up and running before you start using it. although running a cluster for production is out of the scope of this book, this chap‐ ter presents a couple of easy ways to create a cluster so that you can understand how to use kubernetes. chapter 4 covers a selection of common commands used to inter‐ act with a kubernetes cluster. starting with chapter 5, we dive into the details of deploying an application using kubernetes. we cover pods (chapter 5), labels and annotations (chapter 6), services (chapter 7), ingress (chapter 8), and replicasets (chapter 9). these form the core basics of what you need to deploy your service in kubernetes. we then cover deploy‐ ments (chapter 10), which tie together the lifecycle of a complete application. after those chapters, we cover some more specialized objects in kubernetes: dae‐ monsets (chapter 11), jobs (chapter 12), and configmaps and secrets (chapter 13). while these chapters are essential for many production applications, if you are just learning kubernetes you can skip them and return to them later, after you gain more experience and expertise. next we cover integrating storage into kubernetes (chapter 15). we discuss extend‐ ing kubernetes in chapter 16. finally, we conclude with some examples of how to develop and deploy real-world applications in kubernetes (chapter 17) and a discus‐ sion of how to organize your applications in source control (chapter 18). online resources you will want to install docker. you likely will also want to familiarize yourself with the docker documentation if you have not already done so. likewise, you will want to install the kubectl command-line tool. you may also want to join the kubernetes slack channel, where you will find a large community of users who are willing to talk and answer questions at nearly any hour of the day. finally, as you grow more advanced, you may want to engage with the open source kubernetes repository on github. preface | xv conventions used in this book the following typographical conventions are used in this book: italic indicates new terms, urls, email addresses, filenames, and file extensions. constant width used for program listings, as well as within paragraphs to refer to program ele‐ ments such as variable or function names, databases, data types, environment variables, statements, and keywords. constant width bold shows commands or other text that should be typed literally by the user. constant width italic shows text that should be replaced with user-supplied values or by values deter‐ mined by context. this icon signifies a tip, suggestion, or general note. this icon indicates a warning or caution. using code examples supplemental material (code examples, exercises, etc.) is available for download at https://github.com/kubernetes-up-and-running/examples. this book is here to help you get your job done. in general, if example code is offered with this book, you may use it in your programs and documentation. you do not need to contact us for permission unless you’re reproducing a significant portion of the code. for example, writing a program that uses several chunks of code from this book does not require permission. selling or distributing a cd-rom of examples from o’reilly books does require permission. answering a question by citing this book and quoting example code does not require permission. incorporating a signifi‐ cant amount of example code from this book into your product’s documentation does require permission. xvi | preface we appreciate, but do not require, attribution. an attribution usually includes the title, author, publisher, and isbn. for example: “kubernetes: up and running, 2nd edition, by brendan burns, joe beda, and kelsey hightower (o’reilly). copyright 2019 brendan burns, joe beda, and kelsey hightower, 978-1-492-04653-0.” if you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com. o’reilly online learning for almost 40 years, o’reilly media has provided technology and business training, knowledge, and insight to help compa‐ nies succeed. our unique network of experts and innovators share their knowledge and expertise through books, articles, conferences, and our online learning platform. o’reilly’s online learning platform gives you on-demand access to live training courses, indepth learning paths, interactive coding environments, and a vast collection of text and video from o’reilly and 200+ other publishers. for more information, please visit http://oreilly.com. how to contact us please address comments and questions concerning this book to the publisher: o’reilly media, inc. 1005 gravenstein highway north sebastopol, ca 95472 800-998-9938 (in the united states or canada) 707-829-0515 (international or local) 707-829-0104 (fax) we have a web page for this book, where we list errata, examples, and any additional information. you can access this page at http://bit.ly/kubernetesur2e. to comment or ask technical questions about this book, send email to bookques‐ tions@oreilly.com. for more information about our books, courses, conferences, and news, see our web‐ site at http://www.oreilly.com. find us on facebook: http://facebook.com/oreilly follow us on twitter: http://twitter.com/oreillymedia preface | xvii watch us on youtube: http://www.youtube.com/oreillymedia acknowledgments we would like to acknowledge everyone who helped us develop this book. this includes our editor virginia wilson and all of the great folks at o’reilly, as well as the technical reviewers who provided tremendous feedback that significantly improved the book. finally, we would like to thank all of our first edition readers who took the time to report errata that were found and fixed in this second edition. thank you all! we’re very grateful. xviii | preface 1 brendan burns et al., “borg, omega, and kubernetes: lessons learned from three container-management systems over a decade,” acm queue 14 (2016): 70–93, available at http://bit.ly/2virl4s. chapter 1 introduction kubernetes is an open source orchestrator for deploying containerized applications. it was originally developed by google, inspired by a decade of experience deploying scalable, reliable systems in containers via application-oriented apis.1 since its introduction in 2014, kubernetes has grown to be one of the largest and most popular open source projects in the world. it has become the standard api for building cloud-native applications, present in nearly every public cloud. kubernetes is a proven infrastructure for distributed systems that is suitable for cloud-native developers of all scales, from a cluster of raspberry pi computers to a warehouse full of the latest machines. it provides the software necessary to successfully build and deploy reliable, scalable distributed systems. you may be wondering what we mean when we say “reliable, scalable distributed sys‐ tems.” more and more services are delivered over the network via apis. these apis are often delivered by a distributed system, the various pieces that implement the api running on different machines, connected via the network and coordinating their actions via network communication. because we rely on these apis increasingly for all aspects of our daily lives (e.g., finding directions to the nearest hospital), these sys‐ tems must be highly reliable. they cannot fail, even if a part of the system crashes or otherwise stops working. likewise, they must maintain availability even during soft‐ ware rollouts or other maintenance events. finally, because more and more of the world is coming online and using such services, they must be highly scalable so that they can grow their capacity to keep up with ever-increasing usage without radical redesign of the distributed system that implements the services. 1 depending on when and why you have come to hold this book in your hands, you may have varying degrees of experience with containers, distributed systems, and kubernetes. you may be planning on building your application on top of public cloud infrastructure, in private data centers, or in some hybrid environment. regardless of what your experience is, we believe this book will enable you to make the most of your use of kubernetes. there are many reasons why people come to use containers and container apis like kubernetes, but we believe they can all be traced back to one of these benefits: • velocity • scaling (of both software and teams) • abstracting your infrastructure • efficiency in the following sections, we describe how kubernetes can help provide each of these features. velocity velocity is the key component in nearly all software development today. the software industry has evolved from shipping products as boxed cds or dvds to software that is delivered over the network via web-based services that are updated hourly. this changing landscape means that the difference between you and your competitors is often the speed with which you can develop and deploy new components and fea‐ tures, or the speed with which you can respond to innovations developed by others. it is important to note, however, that velocity is not defined in terms of simply raw speed. while your users are always looking for iterative improvement, they are more interested in a highly reliable service. once upon a time, it was ok for a service to be down for maintenance at midnight every night. but today, all users expect constant uptime, even if the software they are running is changing constantly. consequently, velocity is measured not in terms of the raw number of features you can ship per hour or day, but rather in terms of the number of things you can ship while maintaining a highly available service. in this way, containers and kubernetes can provide the tools that you need to move quickly, while staying available. the core concepts that enable this are: • immutability • declarative configuration • online self-healing systems 2 | chapter 1: introduction these ideas all interrelate to radically improve the speed with which you can reliably deploy software. the value of immutability containers and kubernetes encourage developers to build distributed systems that adhere to the principles of immutable infrastructure. with immutable infrastructure, once an artifact is created in the system it does not change via user modifications. traditionally, computers and software systems have been treated as mutable infra‐ structure. with mutable infrastructure, changes are applied as incremental updates to an existing system. these updates can occur all at once, or spread out across a long period of time. a system upgrade via the apt-get update tool is a good example of an update to a mutable system. running apt sequentially downloads any updated binaries, copies them on top of older binaries, and makes incremental updates to configuration files. with a mutable system, the current state of the infrastructure is not represented as a single artifact, but rather an accumulation of incremental updates and changes over time. on many systems these incremental updates come from not just system upgrades, but operator modifications as well. furthermore, in any system run by a large team, it is highly likely that these changes will have been performed by many different people, and in many cases will not have been recorded anywhere. in contrast, in an immutable system, rather than a series of incremental updates and changes, an entirely new, complete image is built, where the update simply replaces the entire image with the newer image in a single operation. there are no incremental changes. as you can imagine, this is a significant shift from the more traditional world of configuration management. to make this more concrete in the world of containers, consider two different ways to upgrade your software: 1. you can log in to a container, run a command to download your new software, kill the old server, and start the new one. 2. you can build a new container image, push it to a container registry, kill the exist‐ ing container, and start a new one. at first blush, these two approaches might seem largely indistinguishable. so what is it about the act of building a new container that improves reliability? the key differentiation is the artifact that you create, and the record of how you cre‐ ated it. these records make it easy to understand exactly the differences in some new version and, if something goes wrong, to determine what has changed and how to fix it. velocity | 3 additionally, building a new image rather than modifying an existing one means the old image is still around, and can quickly be used for a rollback if an error occurs. in contrast, once you copy your new binary over an existing binary, such a rollback is nearly impossible. immutable container images are at the core of everything that you will build in kubernetes. it is possible to imperatively change running containers, but this is an anti-pattern to be used only in extreme cases where there are no other options (e.g., if it is the only way to temporarily repair a mission-critical production system). and even then, the changes must also be recorded through a declarative configuration update at some later time, after the fire is out. declarative con\\x80guration immutability extends beyond containers running in your cluster to the way you describe your application to kubernetes. everything in kubernetes is a declarative configuration object that represents the desired state of the system. it is the job of kubernetes to ensure that the actual state of the world matches this desired state. much like mutable versus immutable infrastructure, declarative configuration is an alternative to imperative configuration, where the state of the world is defined by the execution of a series of instructions rather than a declaration of the desired state of the world. while imperative commands define actions, declarative configurations define state. to understand these two approaches, consider the task of producing three replicas of a piece of software. with an imperative approach, the configuration would say “run a, run b, and run c.” the corresponding declarative configuration would be “replicas equals three.” because it describes the state of the world, declarative configuration does not have to be executed to be understood. its impact is concretely declared. since the effects of declarative configuration can be understood before they are executed, declarative configuration is far less error-prone. further, the traditional tools of software devel‐ opment, such as source control, code review, and unit testing, can be used in declara‐ tive configuration in ways that are impossible for imperative instructions. the idea of storing declarative configuration in source control is often referred to as “infrastruc‐ ture as code.” the combination of declarative state stored in a version control system and the ability of kubernetes to make reality match this declarative state makes rollback of a change trivially easy. it is simply restating the previous declarative state of the system. this is usually impossible with imperative systems, because although the imperative instruc‐ tions describe how to get you from point a to point b, they rarely include the reverse instructions that can get you back. 4 | chapter 1: introduction self-healing systems kubernetes is an online, self-healing system. when it receives a desired state configu‐ ration, it does not simply take a set of actions to make the current state match the desired state a single time. it continuously takes actions to ensure that the current state matches the desired state. this means that not only will kubernetes initialize your system, but it will guard it against any failures or perturbations that might destabilize the system and affect reliability. a more traditional operator repair involves a manual series of mitigation steps, or human intervention performed in response to some sort of alert. imperative repair like this is more expensive (since it generally requires an on-call operator to be avail‐ able to enact the repair). it is also generally slower, since a human must often wake up and log in to respond. furthermore, it is less reliable because the imperative series of repair operations suffers from all of the problems of imperative management described in the previous section. self-healing systems like kubernetes both reduce the burden on operators and improve the overall reliability of the system by perform‐ ing reliable repairs more quickly. as a concrete example of this self-healing behavior, if you assert a desired state of three replicas to kubernetes, it does not just create three replicas—it continuously ensures that there are exactly three replicas. if you manually create a fourth replica, kubernetes will destroy one to bring the number back to three. if you manually destroy a replica, kubernetes will create one to again return you to the desired state. online self-healing systems improve developer velocity because the time and energy you might otherwise have spent on operations and maintenance can instead be spent on developing and testing new features. in a more advanced form of self-healing, there has been significant recent work in the operator paradigm for kubernetes. with operators, more advanced logic needed to maintain, scale, and heal a specific piece of software (mysql, for example) is enco‐ ded into an operator application that runs as a container in the cluster. the code in the operator is responsible for more targeted and advanced health detection and heal‐ ing than can be achieved via kubernetes’s generic self-healing. often this is packaged up as “operators,” which are discussed in a later section. scaling your service and your teams as your product grows, it’s inevitable that you will need to scale both your software and the teams that develop it. fortunately, kubernetes can help with both of these goals. kubernetes achieves scalability by favoring decoupled architectures. scaling your service and your teams | 5 decoupling in a decoupled architecture, each component is separated from other components by defined apis and service load balancers. apis and load balancers isolate each piece of the system from the others. apis provide a buffer between implementer and con‐ sumer, and load balancers provide a buffer between running instances of each service. decoupling components via load balancers makes it easy to scale the programs that make up your service, because increasing the size (and therefore the capacity) of the program can be done without adjusting or reconfiguring any of the other layers of your service. decoupling servers via apis makes it easier to scale the development teams because each team can focus on a single, smaller microservice with a comprehensible surface area. crisp apis between microservices limit the amount of cross-team communica‐ tion overhead required to build and deploy software. this communication overhead is often the major restricting factor when scaling teams. easy scaling for applications and clusters concretely, when you need to scale your service, the immutable, declarative nature of kubernetes makes this scaling trivial to implement. because your containers are immutable, and the number of replicas is merely a number in a declarative config, scaling your service upward is simply a matter of changing a number in a configura‐ tion file, asserting this new declarative state to kubernetes, and letting it take care of the rest. alternatively, you can set up autoscaling and let kubernetes take care of it for you. of course, that sort of scaling assumes that there are resources available in your clus‐ ter to consume. sometimes you actually need to scale up the cluster itself. again, kubernetes makes this task easier. because many machines in a cluster are entirely identical to other machines in that set and the applications themselves are decoupled from the details of the machine by containers, adding additional resources to the cluster is simply a matter of imaging a new machine of the same class and joining it into the cluster. this can be accomplished via a few simple commands or via a pre‐ baked machine image. one of the challenges of scaling machine resources is predicting their use. if you are running on physical infrastructure, the time to obtain a new machine is measured in days or weeks. on both physical and cloud infrastructure, predicting future costs is difficult because it is hard to predict the growth and scaling needs of specific applications. kubernetes can simplify forecasting future compute costs. to understand why this is true, consider scaling up three teams, a, b, and c. historically you have seen that 6 | chapter 1: introduction each team’s growth is highly variable and thus hard to predict. if you are provisioning individual machines for each service, you have no choice but to forecast based on the maximum expected growth for each service, since machines dedicated to one team cannot be used for another team. if instead you use kubernetes to decouple the teams from the specific machines they are using, you can forecast growth based on the aggregate growth of all three services. combining three variable growth rates into a single growth rate reduces statistical noise and produces a more reliable forecast of expected growth. furthermore, decoupling the teams from specific machines means that teams can share fractional parts of one another’s machines, reducing even further the overheads associated with forecasting growth of computing resources. scaling development teams with microservices as noted in a variety of research, the ideal team size is the “two-pizza team,” or roughly six to eight people. this group size often results in good knowledge sharing, fast decision making, and a common sense of purpose. larger teams tend to suffer from issues of hierarchy, poor visibility, and infighting, which hinder agility and success. however, many projects require significantly more resources to be successful and achieve their goals. consequently, there is a tension between the ideal team size for agility and the necessary team size for the product’s end goals. the common solution to this tension has been the development of decoupled, service-oriented teams that each build a single microservice. each small team is responsible for the design and delivery of a service that is consumed by other small teams. the aggregation of all of these services ultimately provides the implementation of the overall product’s surface area. kubernetes provides numerous abstractions and apis that make it easier to build these decoupled microservice architectures: • pods, or groups of containers, can group together container images developed by different teams into a single deployable unit. • kubernetes services provide load balancing, naming, and discovery to isolate one microservice from another. • namespaces provide isolation and access control, so that each microservice can control the degree to which other services interact with it. • ingress objects provide an easy-to-use frontend that can combine multiple micro‐ services into a single externalized api surface area. finally, decoupling the application container image and machine means that different microservices can colocate on the same machine without interfering with one another, reducing the overhead and cost of microservice architectures. the healthscaling your service and your teams | 7 checking and rollout features of kubernetes guarantee a consistent approach to appli‐ cation rollout and reliability that ensures that a proliferation of microservice teams does not also result in a proliferation of different approaches to service production lifecycle and operations. separation of concerns for consistency and scaling in addition to the consistency that kubernetes brings to operations, the decoupling and separation of concerns produced by the kubernetes stack lead to significantly greater consistency for the lower levels of your infrastructure. this enables you to scale infrastructure operations to manage many machines with a single small, focused team. we have talked at length about the decoupling of application container and machine/operating system (os), but an important aspect of this decoupling is that the container orchestration api becomes a crisp contract that separates the responsi‐ bilities of the application operator from the cluster orchestration operator. we call this the “not my monkey, not my circus” line. the application developer relies on the service-level agreement (sla) delivered by the container orchestration api, without worrying about the details of how this sla is achieved. likewise, the container orchestration api reliability engineer focuses on delivering the orchestration api’s sla without worrying about the applications that are running on top of it. this decoupling of concerns means that a small team running a kubernetes cluster can be responsible for supporting hundreds or even thousands of teams running applications within that cluster (figure 1-1). likewise, a small team can be responsi‐ ble for dozens (or more) of clusters running around the world. it’s important to note that the same decoupling of containers and os enables the os reliability engineers to focus on the sla of the individual machine’s os. this becomes another line of sepa‐ rate responsibility, with the kubernetes operators relying on the os sla, and the os operators worrying solely about delivering that sla. again, this enables you to scale a small team of os experts to a fleet of thousands of machines. of course, devoting even a small team to managing an os is beyond the scale of many organizations. in these environments, a managed kubernetes-as-a-service (kaas) provided by a public cloud provider is a great option. as kubernetes has become increasingly ubiquitous, kaas has become increasingly available as well, to the point where it is now offered on nearly every public cloud. of course, using a kaas has some limitations, since the operator makes decisions for you about how the kubernetes clusters are built and configured. for example, many kaas platforms dis‐ able alpha features because they can destabilize the managed cluster. 8 | chapter 1: introduction figure 1-1. an illustration of how different operations teams are decoupled using apis in addition to a fully managed kubernetes service, there is a thriving ecosystem of companies and projects that help to install and manage kubernetes. there is a full spectrum of solutions between doing it “the hard way” and a fully managed service. consequently, the decision of whether to use kaas or manage it yourself (or some‐ thing in between) is one each user needs to make based on the skills and demands of their situation. often for small organizations, kaas provides an easy-to-use solution that enables them to focus their time and energy on building the software to support their work rather than managing a cluster. for a larger organization that can afford a dedicated team for managing its kubernetes cluster, it may make sense to manage it yourself since it enables greater flexibility in terms of cluster capabilities and operations. abstracting your infrastructure the goal of the public cloud is to provide easy-to-use, self-service infrastructure for developers to consume. however, too often cloud apis are oriented around mirror‐ ing the infrastructure that it expects, not the concepts (e.g., “virtual machines” instead of “applications”) that developers want to consume. additionally, in many cases the cloud comes with particular details in implementation or services that are specific to the cloud provider. consuming these apis directly makes it difficult to run your application in multiple environments, or spread between cloud and physical environments. the move to application-oriented container apis like kubernetes has two concrete benefits. first, as we described previously, it separates developers from specific machines. this makes the machine-oriented it role easier, since machines can simply abstracting your infrastructure | 9 be added in aggregate to scale the cluster, and in the context of the cloud it also ena‐ bles a high degree of portability since developers are consuming a higher-level api that is implemented in terms of the specific cloud infrastructure apis. when your developers build their applications in terms of container images and deploy them in terms of portable kubernetes apis, transferring your application between environments, or even running in hybrid environments, is simply a matter of sending the declarative config to a new cluster. kubernetes has a number of plug-ins that can abstract you from a particular cloud. for example, kubernetes services know how to create load balancers on all major public clouds as well as several different pri‐ vate and physical infrastructures. likewise, kubernetes persistentvolumes and persistentvolumeclaims can be used to abstract your applications away from spe‐ cific storage implementations. of course, to achieve this portability you need to avoid cloud-managed services (e.g., amazon’s dynamodb, azure’s cosmosdb, or google’s cloud spanner), which means that you will be forced to deploy and manage open source storage solutions like cassandra, mysql, or mongodb. putting it all together, building on top of kubernetes’s application-oriented abstrac‐ tions ensures that the effort you put into building, deploying, and managing your application is truly portable across a wide variety of environments. efficiency in addition to the developer and it management benefits that containers and kuber‐ netes provide, there is also a concrete economic benefit to the abstraction. because developers no longer think in terms of machines, their applications can be colocated on the same machines without impacting the applications themselves. this means that tasks from multiple users can be packed tightly onto fewer machines. efficiency can be measured by the ratio of the useful work performed by a machine or process to the total amount of energy spent doing so. when it comes to deploying and managing applications, many of the available tools and processes (e.g., bash scripts, apt updates, or imperative configuration management) are somewhat ineffi‐ cient. when discussing efficiency it’s often helpful to think of both the cost of run‐ ning a server and the human cost required to manage it. running a server incurs a cost based on power usage, cooling requirements, datacenter space, and raw compute power. once a server is racked and powered on (or clicked and spun up), the meter literally starts running. any idle cpu time is money wasted. thus, it becomes part of the system administrator’s responsibilities to keep utilization at acceptable levels, which requires ongoing management. this is where containers and the kubernetes workflow come in. kubernetes provides tools that automate the distribution of applications across a cluster of machines, ensuring higher levels of utilization than are possible with traditional tooling. 10 | chapter 1: introduction a further increase in efficiency comes from the fact that a developer’s test environ‐ ment can be quickly and cheaply created as a set of containers running in a personal view of a shared kubernetes cluster (using a feature called namespaces). in the past, turning up a test cluster for a developer might have meant turning up three machines. with kubernetes it is simple to have all developers share a single test cluster, aggre‐ gating their usage onto a much smaller set of machines. reducing the overall number of machines used in turn drives up the efficiency of each system: since more of the resources (cpu, ram, etc.) on each individual machine are used, the overall cost of each container becomes much lower. reducing the cost of development instances in your stack enables development prac‐ tices that might previously have been cost-prohibitive. for example, with your appli‐ cation deployed via kubernetes it becomes conceivable to deploy and test every single commit contributed by every developer throughout your entire stack. when the cost of each deployment is measured in terms of a small number of con‐ tainers, rather than multiple complete virtual machines (vms), the cost you incur for such testing is dramatically lower. returning to the original value of kubernetes, this increased testing also increases velocity, since you have strong signals as to the relia‐ bility of your code as well as the granularity of detail required to quickly identify where a problem may have been introduced. summary kubernetes was built to radically change the way that applications are built and deployed in the cloud. fundamentally, it was designed to give developers more veloc‐ ity, efficiency, and agility. we hope the preceding sections have given you an idea of why you should deploy your applications using kubernetes. now that you are con‐ vinced of that, the following chapters will teach you how to deploy your application. summary | 11  chapter 2 creating and running containers kubernetes is a platform for creating, deploying, and managing distributed applica‐ tions. these applications come in many different shapes and sizes, but ultimately, they are all comprised of one or more programs that run on individual machines. these programs accept input, manipulate data, and then return the results. before we can even consider building a distributed system, we must first consider how to build the application container images that contain these programs and make up the pieces of our distributed system. application programs are typically comprised of a language runtime, libraries, and your source code. in many cases, your application relies on external shared libraries such as libc and libssl. these external libraries are generally shipped as shared components in the os that you have installed on a particular machine. this dependency on shared libraries causes problems when an application developed on a programmer’s laptop has a dependency on a shared library that isn’t available when the program is rolled out to the production os. even when the development and production environments share the exact same version of the os, problems can occur when developers forget to include dependent asset files inside a package that they deploy to production. the traditional methods of running multiple programs on a single machine require that all of these programs share the same versions of shared libraries on the system. if the different programs are developed by different teams or organizations, these shared dependencies add needless complexity and coupling between these teams. a program can only execute successfully if it can be reliably deployed onto the machine where it should run. too often the state of the art for deployment involves running imperative scripts, which inevitably have twisty and byzantine failure cases. 13 this makes the task of rolling out a new version of all or parts of a distributed system a labor-intensive and difficult task. in chapter 1, we argued strongly for the value of immutable images and infrastruc‐ ture. this immutability is exactly what the container image provides. as we will see, it easily solves all the problems of dependency management and encapsulation just described. when working with applications it’s often helpful to package them in a way that makes it easy to share them with others. docker, the default container runtime engine, makes it easy to package an executable and push it to a remote registry where it can later be pulled by others. at the time of writing, container registries are avail‐ able in all of the major public clouds, and services to build images in the cloud are also available in many of them. you can also run your own registry using open source or commercial systems. these registries make it easy for users to manage and deploy private images, while image-builder services provide easy integration with continuous delivery systems. for this chapter, and the remainder of the book, we are going to work with a simple example application that we built to help show this workflow in action. you can find the application on github. container images bundle a program and its dependencies into a single artifact under a root filesystem. the most popular container image format is the docker image for‐ mat, which has been standardized by the open container initiative to the oci image format. kubernetes supports both docker- and oci-compatible images via docker and other runtimes. docker images also include additional metadata used by a con‐ tainer runtime to start a running application instance based on the contents of the container image. this chapter covers the following topics: • how to package an application using the docker image format • how to start an application using the docker container runtime container images for nearly everyone, their first interaction with any container technology is with a container image. a container image is a binary package that encapsulates all of the files necessary to run a program inside of an os container. depending on how you first experiment with containers, you will either build a container image from your local filesystem or download a preexisting image from a container registry. in either case, once the container image is present on your computer, you can run that image to produce a running application inside an os container. 14 | chapter 2: creating and running containers the docker image format the most popular and widespread container image format is the docker image for‐ mat, which was developed by the docker open source project for packaging, distrib‐ uting, and running containers using the docker command. subsequently, work has begun by docker, inc., and others to standardize the container image format via the open container initiative (oci) project. while the oci standard achieved a 1.0 release milestone in mid-2017, adoption of these standards is proceeding slowly. the docker image format continues to be the de facto standard, and is made up of a series of filesystem layers. each layer adds, removes, or modifies files from the preceding layer in the filesystem. this is an example of an overlay filesystem. the overlay system is used both when packaging up the image and when the image is actually being used. during runtime, there are a variety of different concrete implementations of such file‐ systems, including aufs, overlay, and overlay2. container layering the phrases “docker image format” and “container images” may be a bit confusing. the image isn’t a single file but rather a specification for a manifest file that points to other files. the manifest and associated files are often treated by users as a unit. the level of indirection allows for more efficient storage and transmittal. associated with this format is an api for uploading and downloading images to an image registry. container images are constructed with a series of filesystem layers, where each layer inherits and modifies the layers that came before it. to help explain this in detail, let’s build some containers. note that for correctness the ordering of the layers should be bottom up, but for ease of understanding we take the opposite approach: . └── container a: a base operating system only, such as debian  └── container b: build upon #a, by adding ruby v2.1.10  └── container c: build upon #a, by adding golang v1.6 at this point we have three containers: a, b, and c. b and c are forked from a and share nothing besides the base container’s files. taking it further, we can build on top of b by adding rails (version 4.2.6). we may also want to support a legacy application that requires an older version of rails (e.g., version 3.2.x). we can build a container image to support that application based on b also, planning to someday migrate the app to version 4: . (continuing from above) └── container b: build upon #a, by adding ruby v2.1.10  └── container d: build upon #b, by adding rails v4.2.6  └── container e: build upon #b, by adding rails v3.2.x container images | 15 conceptually, each container image layer builds upon a previous one. each parent reference is a pointer. while the example here is a simple set of containers, other realworld containers can be part of a larger extensive directed acyclic graph. container images are typically combined with a container configuration file, which provides instructions on how to set up the container environment and execute an application entry point. the container configuration often includes information on how to set up networking, namespace isolation, resource constraints (cgroups), and what syscall restrictions should be placed on a running container instance. the container root filesystem and configuration file are typically bundled using the docker image format. containers fall into two main categories: • system containers • application containers system containers seek to mimic virtual machines and often run a full boot process. they often include a set of system services typically found in a vm, such as ssh, cron, and syslog. when docker was new, these types of containers were much more com‐ mon. over time, they have come to be seen as poor practice and application contain‐ ers have gained favor. application containers differ from system containers in that they commonly run a single program. while running a single program per container might seem like an unnecessary constraint, it provides the perfect level of granularity for composing scal‐ able applications and is a design philosophy that is leveraged heavily by pods. we will examine how pods work in detail in chapter 5. building application images with docker in general, container orchestration systems like kubernetes are focused on building and deploying distributed systems made up of application containers. consequently, we will focus on application containers for the remainder of this chapter. docker\\x80les a dockerfile can be used to automate the creation of a docker container image. let’s start by building an application image for a simple node.js program. this exam‐ ple would be very similar for many other dynamic languages, like python or ruby. 16 | chapter 2: creating and running containers the simplest of npm/node/express apps has two files: package.json (example 2-1) and server.js (example 2-2). put these in a directory and then run npm install express --save to establish a dependency on express and install it. example 2-1. package.json {  \"name\": \"simple-node\",  \"version\": \"1.0.0\",  \"description\": \"a sample simple application for kubernetes up & running\",  \"main\": \"server.js\",  \"scripts\": {  \"start\": \"node server.js\"  },  \"author\": \"\" } example 2-2. server.js var express = require(\\'express\\'); var app = express(); app.get(\\'/\\', function (req, res) {  res.send(\\'hello world!\\'); }); app.listen(3000, function () {  console.log(\\'listening on port 3000!\\');  console.log(\\' http://localhost:3000\\'); }); to package this up as a docker image we need to create two additional files: .docker‐ ignore (example 2-3) and the dockerfile (example 2-4). the dockerfile is a recipe for how to build the container image, while .dockerignore defines the set of files that should be ignored when copying files into the image. a full description of the syntax of the dockerfile is available on the docker website. example 2-3. .dockerignore nodemodules example 2-4. dockerfile # start from a node.js 10 (lts) image from node:10 # specify the directory inside the image in which all commands will run workdir /usr/src/app building application images with docker | 17 # copy package files and install dependencies copy package*.json ./ run npm install # copy all of the app files into the image copy . . # the default command to run when starting the container cmd  every dockerfile builds on other container images. this line specifies that we are starting from the node:10 image on the docker hub. this is a preconfigured image with node.js 10. this line sets the work directory, in the container image, for all following commands. these two lines initialize the dependencies for node.js. first we copy the package files into the image. this will include package.json and package-lock.json. the run command then runs the correct command in the container to install the neces‐ sary dependencies. now we copy the rest of the program files into the image. this will include every‐ thing except nodemodules, as that is excluded via the .dockerignore file. finally, we specify the command that should be run when the container is run. run the following command to create the simple-node docker image: $ docker build -t simple-node . when you want to run this image, you can do it with the following command. you can navigate to http://localhost:3000 to access the program running in the container: $ docker run --rm -p 3000:3000 simple-node at this point our simple-node image lives in the local docker registry where the image was built and is only accessible to a single machine. the true power of docker comes from the ability to share images across thousands of machines and the broader docker community. optimizing image sizes there are several gotchas that come when people begin to experiment with container images that lead to overly large images. the first thing to remember is that files that are removed by subsequent layers in the system are actually still present in the images; they’re just inaccessible. consider the following situation: 18 | chapter 2: creating and running containers . └── layer a: contains a large file named \\'bigfile\\'  └── layer b: removes \\'bigfile\\'  └── layer c: builds on b by adding a static binary you might think that bigfile is no longer present in this image. after all, when you run the image, it is no longer accessible. but in fact it is still present in layer a, which means that whenever you push or pull the image, bigfile is still transmitted through the network, even if you can no longer access it. another pitfall that people fall into revolves around image caching and building. remember that each layer is an independent delta from the layer below it. every time you change a layer, it changes every layer that comes after it. changing the preceding layers means that they need to be rebuilt, repushed, and repulled to deploy your image to development. to understand this more fully, consider two images: . └── layer a: contains a base os  └── layer b: adds source code server.js  └── layer c: installs the \\'node\\' package versus: . └── layer a: contains a base os  └── layer b: installs the \\'node\\' package  └── layer c: adds source code server.js it seems obvious that both of these images will behave identically, and indeed the first time they are pulled they do. however, consider what happens when server.js changes. in one case, it is only the change that needs to be pulled or pushed, but in the other case, both server.js and the layer providing the node package need to be pulled and pushed, since the node layer is dependent on the server.js layer. in general, you want to order your layers from least likely to change to most likely to change in order to optimize the image size for pushing and pulling. this is why, in example 2-4, we copy the package*.json files and install dependencies before copying the rest of the pro‐ gram files. a developer is going to update and change the program files much more often than the dependencies. image security when it comes to security, there are no shortcuts. when building images that will ultimately run in a production kubernetes cluster, be sure to follow best practices for packaging and distributing applications. for example, don’t build containers with passwords baked in—and this includes not just in the final layer, but any layers in the image. one of the counterintuitive problems introduced by container layers is that deleting a file in one layer doesn’t delete that file from preceding layers. it still takes building application images with docker | 19 up space, and it can be accessed by anyone with the right tools—an enterprising attacker can simply create an image that only consists of the layers that contain the password. secrets and images should never be mixed. if you do so, you will be hacked, and you will bring shame to your entire company or department. we all want to be on tv someday, but there are better ways to go about that. multistage image builds one of the most common ways to accidentally build large images is to do the actual program compilation as part of the construction of the application container image. compiling code as part of the image build feels natural, and it is the easiest way to build a container image from your program. the trouble with doing this is that it leaves all of the unnecessary development tools, which are usually quite large, lying around inside of your image and slowing down your deployments. to resolve this problem, docker introduced multistage builds. with multistage builds, rather than producing a single image, a docker file can actually produce multiple images. each image is considered a stage. artifacts can be copied from preceding stages to the current stage. to illustrate this concretely, we will look at how to build our example application, kuard. this is a somewhat complicated application that involves a react.js frontend (with its own build process) that then gets embedded into a go program. the go program runs a backend api server that the react.js frontend interacts with. a simple dockerfile might look like this: from golang:1.11-alpine # install node and npm run apk update && apk upgrade && apk add --no-cache git nodejs bash npm # get dependencies for go part of build run go get -u github.com/jteeuwen/go-bindata/... run go get github.com/tools/godep workdir /go/src/github.com/kubernetes-up-and-running/kuard # copy all sources in copy . . # this is a set of variables that the build script expects env verbose=0 env pkg=github.com/kubernetes-up-and-running/kuard env arch=amd64 env version=test 20 | chapter 2: creating and running containers # do the build. this script is part of incoming sources. run build/build.sh cmd  this dockerfile produces a container image containing a static executable, but it also contains all of the go development tools and the tools to build the react.js frontend and the source code for the application, neither of which are needed by the final application. the image, across all layers, adds up to over 500 mb. to see how we would do this with multistage builds, examine this multistage docker‐ file: # stage 1: build from golang:1.11-alpine as build # install node and npm run apk update && apk upgrade && apk add --no-cache git nodejs bash npm # get dependencies for go part of build run go get -u github.com/jteeuwen/go-bindata/... run go get github.com/tools/godep workdir /go/src/github.com/kubernetes-up-and-running/kuard # copy all sources in copy . . # this is a set of variables that the build script expects env verbose=0 env pkg=github.com/kubernetes-up-and-running/kuard env arch=amd64 env version=test # do the build. script is part of incoming sources. run build/build.sh # stage 2: deployment from alpine user nobody:nobody copy --from=build /go/bin/kuard /kuard cmd  this dockerfile produces two images. the first is the build image, which contains the go compiler, react.js toolchain, and source code for the program. the second is the deployment image, which simply contains the compiled binary. building a container image using multistage builds can reduce your final container image size by hundreds of megabytes and thus dramatically speed up your deployment times, since generally, multistage image builds | 21 deployment latency is gated on network performance. the final image produced from this dockerfile is somewhere around 20 mb. you can build and run this image with the following commands: $ docker build -t kuard . $ docker run --rm -p 8080:8080 kuard storing images in a remote registry what good is a container image if it’s only available on a single machine? kubernetes relies on the fact that images described in a pod manifest are available across every machine in the cluster. one option for getting this image to all machines in the cluster would be to export the kuard image and import it on each of them. we can’t think of anything more tedious than managing docker images this way. the process of manually importing and exporting docker images has human error writ‐ ten all over it. just say no! the standard within the docker community is to store docker images in a remote registry. there are tons of options when it comes to docker registries, and what you choose will be largely based on your needs in terms of security and collaboration features. generally speaking, the first choice you need to make regarding a registry is whether to use a private or a public registry. public registries allow anyone to download images stored in the registry, while private registries require authentication to down‐ load images. in choosing public versus private, it’s helpful to consider your use case. public registries are great for sharing images with the world, because they allow for easy, unauthenticated use of the container images. you can easily distribute your soft‐ ware as a container image and have confidence that users everywhere will have the exact same experience. in contrast, a private registry is best for storing applications that are private to your service and that you don’t want the world to use. regardless, to push an image, you need to authenticate to the registry. you can gener‐ ally do this with the docker login command, though there are some differences for certain registries. in the examples here we are pushing to the google cloud platform registry, called the google container registry (gcr); other clouds, including azure and amazon web services (aws), also have hosted container registries. for new users hosting publicly readable images, the docker hub is a great place to start. once you are logged in, you can tag the kuard image by prepending the target docker registry. you can also append another identifier that is usually used for the version or variant of that image, separated by a colon (:): 22 | chapter 2: creating and running containers $ docker tag kuard gcr.io/kuar-demo/kuard-amd64:blue then you can push the kuard image: $ docker push gcr.io/kuar-demo/kuard-amd64:blue now that the kuard image is available on a remote registry, it’s time to deploy it using docker. because we pushed it to the public docker registry, it will be available every‐ where without authentication. the docker container runtime kubernetes provides an api for describing an application deployment, but relies on a container runtime to set up an application container using the container-specific apis native to the target os. on a linux system that means configuring cgroups and namespaces. the interface to this container runtime is defined by the container run‐ time interface (cri) standard. the cri api is implemented by a number of different programs, including the containerd-cri built by docker and the cri-o implementa‐ tion contributed by red hat. running containers with docker though generally in kubernetes containers are launched by a daemon on each node called the kubelet, it’s easier to get started with containers using the docker command-line tool. the docker cli tool can be used to deploy containers. to deploy a container from the gcr.io/kuar-demo/kuard-amd64:blue image, run the following command: $ docker run -d --name kuard \\\\  --publish 8080:8080 \\\\  gcr.io/kuar-demo/kuard-amd64:blue this command starts the kuard container and maps ports 8080 on your local machine to 8080 in the container. the --publish option can be shortened to -p. this forwarding is necessary because each container gets its own ip address, so listening on localhost inside the container doesn’t cause you to listen on your machine. without the port forwarding, connections will be inaccessible to your machine. the -d option specifies that this should run in the background (daemon), while --name kuard gives the container a friendly name. exploring the kuard application kuard exposes a simple web interface, which you can load by pointing your browser at http://localhost:8080 or via the command line: $ curl http://localhost:8080 the docker container runtime | 23 kuard also exposes a number of interesting functions that we will explore later on in this book. limiting resource usage docker provides the ability to limit the amount of resources used by applications by exposing the underlying cgroup technology provided by the linux kernel. these capabilities are likewise used by kubernetes to limit the resources used by each pod. limiting memory resources one of the key benefits to running applications within a container is the ability to restrict resource utilization. this allows multiple applications to coexist on the same hardware and ensures fair usage. to limit kuard to 200 mb of memory and 1 gb of swap space, use the --memory and --memory-swap flags with the docker run command. stop and remove the current kuard container: $ docker stop kuard $ docker rm kuard then start another kuard container using the appropriate flags to limit memory usage: $ docker run -d --name kuard \\\\  --publish 8080:8080 \\\\  --memory 200m \\\\  --memory-swap 1g \\\\  gcr.io/kuar-demo/kuard-amd64:blue if the program in the container uses too much memory, it will be terminated. limiting cpu resources another critical resource on a machine is the cpu. restrict cpu utilization using the --cpu-shares flag with the docker run command: $ docker run -d --name kuard \\\\  --publish 8080:8080 \\\\  --memory 200m \\\\  --memory-swap 1g \\\\  --cpu-shares 1024 \\\\  gcr.io/kuar-demo/kuard-amd64:blue cleanup once you are done building an image, you can delete it with the docker rmi command: 24 | chapter 2: creating and running containers docker rmi <tag-name> or: docker rmi <image-id> images can either be deleted via their tag name (e.g., gcr.io/kuar-demo/kuardamd64:blue) or via their image id. as with all id values in the docker tool, the image id can be shortened as long as it remains unique. generally only three or four char‐ acters of the id are necessary. it’s important to note that unless you explicitly delete an image it will live on your system forever, even if you build a new image with an identical name. building this new image simply moves the tag to the new image; it doesn’t delete or replace the old image. consequently, as you iterate while you are creating a new image, you will often create many, many different images that end up taking up unnecessary space on your computer. to see the images currently on your machine, you can use the docker images com‐ mand. you can then delete tags you are no longer using. docker provides a tool called docker system prune for doing general cleanup. this will remove all stopped containers, all untagged images, and all unused image layers cached as part of the build process. use it carefully. a slightly more sophisticated approach is to set up a cron job to run an image garbage collector. for example, the docker-gc tool is a commonly used image garbage collector that can easily run as a recurring cron job, once per day or once per hour, depending on how many images you are creating. summary application containers provide a clean abstraction for applications, and when pack‐ aged in the docker image format, applications become easy to build, deploy, and dis‐ tribute. containers also provide isolation between applications running on the same machine, which helps avoid dependency conflicts. in future chapters we’ll see how the ability to mount external directories means we can run not only stateless applications in a container, but also applications like mysql and others that generate lots of data. summary | 25  chapter 3 deploying a kubernetes cluster now that you have successfully built an application container, the next step is to learn how to transform it into a complete, reliable, scalable distributed system. to do that, you need a working kubernetes cluster. at this point, there are cloud-based kuber‐ netes services in most public clouds that make it easy to create a cluster with a few command-line instructions. we highly recommend this approach if you are just get‐ ting started with kubernetes. even if you are ultimately planning on running kuber‐ netes on bare metal, it’s a good way to quickly get started with kubernetes, learn about kubernetes itself, and then learn how to install it on physical machines. fur‐ thermore, managing a kubernetes cluster is a complicated task in itself, and for most people it makes sense to defer this management to the cloud—especially when in most clouds the management service is free. of course, using a cloud-based solution requires paying for those cloud-based resour‐ ces as well as having an active network connection to the cloud. for these reasons, local development can be more attractive, and in that case the minikube tool provides an easy-to-use way to get a local kubernetes cluster up running in a vm on your local laptop or desktop. though this is a nice option, minikube only creates a single-node cluster, which doesn’t quite demonstrate all of the aspects of a complete kubernetes cluster. for that reason, we recommend people start with a cloud-based solution, unless it really doesn’t work for their situation. a more recent alternative is to run a docker-in-docker cluster, which can spin up a multi-node cluster on a single machine. this project is still in beta, though, so keep in mind that you may encounter unexpected issues. if you truly insist on starting on bare metal, appendix a at the end of this book gives instructions for building a cluster from a collection of raspberry pi single-board computers. these instructions use the kubeadm tool and can be adapted to other machines beyond raspberry pis. 27 installing kubernetes on a public cloud provider this chapter covers installing kubernetes on the three major cloud providers: ama‐ zon web services, microsoft azure, and the google cloud platform. if you choose to use a cloud provider to manage kubernetes, you only need to install one of these options; once you have a cluster configured and ready to go you can skip to “the kubernetes client” on page 31, unless you would prefer to install kubernetes elsewhere. google kubernetes engine the google cloud platform offers a hosted kubernetes-as-a-service called google kubernetes engine (gke). to get started with gke, you need a google cloud plat‐ form account with billing enabled and the gcloud tool installed. once you have gcloud installed, first set a default zone: $ gcloud config set compute/zone us-west1-a then you can create a cluster: $ gcloud container clusters create kuar-cluster this will take a few minutes. when the cluster is ready you can get credentials for the cluster using: $ gcloud auth application-default login if you run into trouble, you can find the complete instructions for creating a gke cluster in the google cloud platform documentation. installing kubernetes with azure kubernetes service microsoft azure offers a hosted kubernetes-as-a-service as part of the azure con‐ tainer service. the easiest way to get started with azure container service is to use the built-in azure cloud shell in the azure portal. you can activate the shell by click‐ ing the shell icon in the upper-right toolbar: the shell has the az tool automatically installed and configured to work with your azure environment. alternatively, you can install the az command-line interface (cli) on your local machine. when you have the shell up and working, you can run: 28 | chapter 3: deploying a kubernetes cluster $ az group create --name=kuar --location=westus once the resource group is created, you can create a cluster using: $ az aks create --resource-group=kuar --name=kuar-cluster this will take a few minutes. once the cluster is created, you can get credentials for the cluster with: $ az aks get-credentials --resource-group=kuar --name=kuar-cluster if you don’t already have the kubectl tool installed, you can install it using: $ az aks install-cli you can find complete instructions for installing kubernetes on azure in the azure documentation. installing kubernetes on amazon web services amazon offers a managed kubernetes service called elastic kubernetes service (eks). the easiest way to create an eks cluster is via the open source eksctl command-line tool.. once you have eksctl installed and in your path, you can run the following com‐ mand to create a cluster: $ eksctl create cluster --name kuar-cluster ... for more details on installation options (such as node size and more), view the help using this command: $ eksctl create cluster --help the cluster installation includes the right configuration for the kubectl commandline tool. if you don’t already have kubectl installed, you can follow the instructions in the documentation. installing kubernetes locally using minikube if you need a local development experience, or you don’t want to pay for cloud resources, you can install a simple single-node cluster using minikube. alternatively, if you have already installed docker desktop, it comes bundled with a single-machine installation of kubernetes. while minikube (or docker desktop) is a good simulation of a kubernetes cluster, it’s really intended for local development, learning, and experimentation. because it only runs in a vm on a single node, it doesn’t provide the reliability of a distributed kubernetes cluster. installing kubernetes locally using minikube | 29 in addition, certain features described in this book require integration with a cloud provider. these features are either not available or work in a limited way with minikube. you need to have a hypervisor installed on your machine to use minikube. for linux and macos, this is generally virtualbox. on windows, the hyper-v hypervisor is the default option. make sure you install the hypervisor before using minikube. you can find the minikube tool on github. there are binaries for linux, macos, and windows that you can download. once you have the minikube tool installed, you can create a local cluster using: $ minikube start this will create a local vm, provision kubernetes, and create a local kubectl configu‐ ration that points to that cluster. when you are done with your cluster, you can stop the vm with: $ minikube stop if you want to remove the cluster, you can run: $ minikube delete running kubernetes in docker a different approach to running a kubernetes cluster has been developed more recently, which uses docker containers to simulate multiple kubernetes nodes instead of running everything in a virtual machine. the kind project provides a great experi‐ ence for launching and managing test clusters in docker. (kind stands for kubernetes in docker.) kind is still a work in progress (pre 1.0), but is widely used by those building kubernetes for fast and easy testing. installation instructions for your platform can be found at the kind site. once you get it installed, creating a cluster is as easy as: $ kind create cluster --wait 5m \\\\ $ export kubeconfig=\"$(kind get kubeconfig-path)\" $ kubectl cluster-info $ kind delete cluster 30 | chapter 3: deploying a kubernetes cluster running kubernetes on raspberry pi if you want to experiment with a realistic kubernetes cluster but don’t want to pay a lot, a very nice kubernetes cluster can be built on top of raspberry pi computers for a relatively small cost. the details of building such a cluster are out of scope for this chapter, but they are given in appendix a at the end of this book. the kubernetes client the official kubernetes client is kubectl: a command-line tool for interacting with the kubernetes api. kubectl can be used to manage most kubernetes objects, such as pods, replicasets, and services. kubectl can also be used to explore and verify the overall health of the cluster. we’ll use the kubectl tool to explore the cluster you just created. checking cluster status the first thing you can do is check the version of the cluster that you are running: $ kubectl version this will display two different versions: the version of the local kubectl tool, as well as the version of the kubernetes api server. don’t worry if these versions are different. the kubernetes tools are backward- and forward-compatible with different versions of the kubernetes api, so long as you stay within two minor versions for both the tools and the cluster and don’t try to use newer fea‐ tures on an older cluster. kubernetes follows the semantic version‐ ing specification, where the minor version is the middle number (e.g., the 5 in 1.5.2). now that we’ve established that you can communicate with your kubernetes cluster, we’ll explore the cluster in more depth. first, you can get a simple diagnostic for the cluster. this is a good way to verify that your cluster is generally healthy: $ kubectl get componentstatuses the output should look like this: name status message error scheduler healthy ok controller-manager healthy ok etcd-0 healthy {\"health\": \"true\"} running kubernetes on raspberry pi | 31 as kubernetes changes and improves over time, the output of the kubectl command sometimes changes. don’t worry if the output doesn’t look exactly identical to what is shown in the examples in this book. you can see here the components that make up the kubernetes cluster. the controller-manager is responsible for running various controllers that regulate behavior in the cluster; for example, ensuring that all of the replicas of a service are available and healthy. the scheduler is responsible for placing different pods onto different nodes in the cluster. finally, the etcd server is the storage for the cluster where all of the api objects are stored. listing kubernetes worker nodes next, you can list out all of the nodes in your cluster: $ kubectl get nodes name status age version kubernetes ready,master 45d v1.12.1 node-1 ready 45d v1.12.1 node-2 ready 45d v1.12.1 node-3 ready 45d v1.12.1 you can see this is a four-node cluster that’s been up for 45 days. in kubernetes, nodes are separated into master nodes that contain containers like the api server, schedu‐ ler, etc., which manage the cluster, and worker nodes where your containers will run. kubernetes won’t generally schedule work onto master nodes to ensure that user workloads don’t harm the overall operation of the cluster. you can use the kubectl describe command to get more information about a spe‐ cific node, such as node-1: $ kubectl describe nodes node-1 first, you see basic information about the node: name: node-1 role: labels: beta.kubernetes.io/arch=arm  beta.kubernetes.io/os=linux  kubernetes.io/hostname=node-1 you can see that this node is running the linux os and is running on an arm pro‐ cessor. next, you see information about the operation of node-1 itself: 32 | chapter 3: deploying a kubernetes cluster conditions:  type status lastheartbeattime reason message  ---- ------ ----------------- ------ -------  outofdisk false sun, 05 feb 2017… kubelethassufficientdisk kubelet…  memorypressure false sun, 05 feb 2017… kubelethassufficientmemory kubelet…  diskpressure false sun, 05 feb 2017… kubelethasnodiskpressure kubelet…  ready true sun, 05 feb 2017… kubeletready kubelet… these statuses show that the node has sufficient disk and memory space and is reporting that it is healthy to the kubernetes master. next, there is information about the capacity of the machine: capacity:  alpha.kubernetes.io/nvidia-gpu: 0  cpu: 4  memory: 882636ki  pods: 110 allocatable:  alpha.kubernetes.io/nvidia-gpu: 0  cpu: 4  memory: 882636ki  pods: 110 then there is information about the software on the node, including the version of docker that is running, the versions of kubernetes and the linux kernel, and more: system info:  machine id: 9122895d0d494e3f97dda1e8f969c85c  system uuid: a7dbf2ce-db1e-e34a-969a-3355c36a2149  boot id: ba53d5ee-27d2-4b6a-8f19-e5f702993ec6  kernel version: 4.15.0-1037-azure  os image: ubuntu 16.04.5 lts  operating system: linux  architecture: amd64  container runtime version: docker://3.0.4  kubelet version: v1.12.6  kube-proxy version: v1.12.6 podcidr: 10.244.1.0/24 finally, there is information about the pods that are currently running on this node: non-terminated pods: (3 in total)  namespace name cpu requests cpu limits memory requests memory limits  --------- ---- ------------ ---------- --------------- -------------  kube-system kube-dns... 260m (6%) 0 (0%) 140mi (16%) 220mi (25%)  kube-system kube-fla... 0 (0%) 0 (0%) 0 (0%) 0 (0%)  kube-system kube-pro... 0 (0%) 0 (0%) 0 (0%) 0 (0%) allocated resources:  (total limits may be over 100 percent, i.e., overcommitted.  cpu requests cpu limits memory requests memory limits  ------------ ---------- --------------- -------------  260m (6%) 0 (0%) 140mi (16%) 220mi (25%) no events. the kubernetes client | 33 1 as you’ll learn in the next chapter, a namespace in kubernetes is an entity for organizing kubernetes resour‐ ces. you can think of it like a folder in a filesystem. from this output you can see the pods on the node (e.g., the kube-dns pod that sup‐ plies dns services for the cluster), the cpu and memory that each pod is requesting from the node, as well as the total resources requested. it’s worth noting here that kubernetes tracks both the requests and upper limits for resources for each pod that runs on a machine. the difference between requests and limits is described in detail in chapter 5, but in a nutshell, resources requested by a pod are guaranteed to be present on the node, while a pod’s limit is the maximum amount of a given resource that a pod can consume. a pod’s limit can be higher than its request, in which case the extra resources are supplied on a best-effort basis. they are not guaranteed to be present on the node. cluster components one of the interesting aspects of kubernetes is that many of the components that make up the kubernetes cluster are actually deployed using kubernetes itself. we’ll take a look at a few of these. these components use a number of the concepts that we’ll introduce in later chapters. all of these components run in the kube-system namespace.1 kubernetes proxy the kubernetes proxy is responsible for routing network traffic to load-balanced services in the kubernetes cluster. to do its job, the proxy must be present on every node in the cluster. kubernetes has an api object named daemonset, which you will learn about later in the book, that is used in many clusters to accomplish this. if your cluster runs the kubernetes proxy with a daemonset, you can see the proxies by running: $ kubectl get daemonsets --namespace=kube-system kube-proxy name desired current ready node-selector age kube-proxy 4 4 4 <none> 45d depending on how your cluster is set up, the daemonset for the kube-proxy may be named something else, or its possible that it won’t use a daemonset at all. regardless, the kube-proxy container should be running on all nodes in a cluster. kubernetes dns kubernetes also runs a dns server, which provides naming and discovery for the services that are defined in the cluster. this dns server also runs as a replicated ser‐ vice on the cluster. depending on the size of your cluster, you may see one or more 34 | chapter 3: deploying a kubernetes cluster dns servers running in your cluster. the dns service is run as a kubernetes deploy‐ ment, which manages these replicas: $ kubectl get deployments --namespace=kube-system core-dns name desired current up-to-date available age core-dns 1 1 1 1 45d there is also a kubernetes service that performs load balancing for the dns server: $ kubectl get services --namespace=kube-system core-dns name cluster-ip external-ip port(s) age core-dns 10.96.0.10 <none> 53/udp,53/tcp 45d this shows that the dns service for the cluster has the address 10.96.0.10. if you log in to a container in the cluster, you’ll see that this has been populated into the /etc/ resolv.conf file for the container. with kubernetes 1.12, kubernetes transitioned from the kube-dns dns server to the core-dns dns server. because of this, if you are running an older kubernetes cluster, you may see kube-dns instead. kubernetes ui the final kubernetes component is a gui. the ui is run as a single replica, but it is still managed by a kubernetes deployment for reliability and upgrades. you can see this ui server using: $ kubectl get deployments --namespace=kube-system kubernetes-dashboard name desired current up-to-date available age kubernetes-dashboard 1 1 1 1 45d the dashboard also has a service that performs load balancing for the dashboard: $ kubectl get services --namespace=kube-system kubernetes-dashboard name cluster-ip external-ip port(s) age kubernetes-dashboard 10.99.104.174 <nodes> 80:32551/tcp 45d you can use kubectl proxy to access this ui. launch the kubernetes proxy using: $ kubectl proxy this starts up a server running on localhost:8001. if you visit http://localhost: 8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/ proxy/ in your web browser, you should see the kubernetes web ui. you can use this interface to explore your cluster, as well as create new containers. the full details of this interface are outside of the scope of this book, and it is changing rapidly as the dashboard is improved. cluster components | 35 some providers don’t install the kubernetes dashboard by default, so don’t worry if you don’t see it in your cluster. documentation on how to install the dashboard for these clusters is available at https://kubernetes.io/docs/tasks/access-application-cluster/ web-ui-dashboard/. summary hopefully at this point you have a kubernetes cluster (or three) up and running and you’ve used a few commands to explore the cluster you have created. next, we’ll spend some more time exploring the command-line interface to that kubernetes cluster and teach you how to master the kubectl tool. throughout the rest of the book, you’ll be using kubectl and your test cluster to explore the various objects in the kubernetes api. 36 | chapter 3: deploying a kubernetes cluster chapter 4 common kubectl commands the kubectl command-line utility is a powerful tool, and in the following chapters you will use it to create objects and interact with the kubernetes api. before that, however, it makes sense to go over the basic kubectl commands that apply to all kubernetes objects. namespaces kubernetes uses namespaces to organize objects in the cluster. you can think of each namespace as a folder that holds a set of objects. by default, the kubectl commandline tool interacts with the default namespace. if you want to use a different name‐ space, you can pass kubectl the --namespace flag. for example, kubectl --namespace=mystuff references objects in the mystuff namespace. if you want to interact with all namespaces—for example, to list all pods in your cluster— you can pass the --all-namespaces flag. contexts if you want to change the default namespace more permanently, you can use a con‐ text. this gets recorded in a kubectl configuration file, usually located at $home/.kube/config. this configuration file also stores how to both find and authen‐ ticate to your cluster. for example, you can create a context with a different default namespace for your kubectl commands using: $ kubectl config set-context my-context --namespace=mystuff this creates a new context, but it doesn’t actually start using it yet. to use this newly created context, you can run: $ kubectl config use-context my-context 37 contexts can also be used to manage different clusters or different users for authenti‐ cating to those clusters using the --users or --clusters flags with the set-context command. viewing kubernetes api objects everything contained in kubernetes is represented by a restful resource. through‐ out this book, we refer to these resources as kubernetes objects. each kubernetes object exists at a unique http path; for example, https://your-k8s.com/api/v1/name‐ spaces/default/pods/my-pod leads to the representation of a pod in the default name‐ space named my-pod. the kubectl command makes http requests to these urls to access the kubernetes objects that reside at these paths. the most basic command for viewing kubernetes objects via kubectl is get. if you run kubectl get <resource-name> you will get a listing of all resources in the cur‐ rent namespace. if you want to get a specific resource, you can use kubectl get <resource-name> <obj-name>. by default, kubectl uses a human-readable printer for viewing the responses from the api server, but this human-readable printer removes many of the details of the objects to fit each object on one terminal line. one way to get slightly more informa‐ tion is to add the -o wide flag, which gives more details, on a longer line. if you want to view the complete object, you can also view the objects as raw json or yaml using the -o json or -o yaml flags, respectively. a common option for manipulating the output of kubectl is to remove the headers, which is often useful when combining kubectl with unix pipes (e.g., kubectl ... | awk ...). if you specify the --no-headers flag, kubectl will skip the headers at the top of the human-readable table. another common task is extracting specific fields from the object. kubectl uses the jsonpath query language to select fields in the returned object. the complete details of jsonpath are beyond the scope of this chapter, but as an example, this command will extract and print the ip address of the specified pod: $ kubectl get pods my-pod -o jsonpath --template={.status.podip} if you are interested in more detailed information about a particular object, use the describe command: $ kubectl describe <resource-name> <obj-name> this will provide a rich multiline human-readable description of the object as well as any other relevant, related objects and events in the kubernetes cluster. 38 | chapter 4: common kubectl commands creating, updating, and destroying kubernetes objects objects in the kubernetes api are represented as json or yaml files. these files are either returned by the server in response to a query or posted to the server as part of an api request. you can use these yaml or json files to create, update, or delete objects on the kubernetes server. let’s assume that you have a simple object stored in obj.yaml. you can use kubectl to create this object in kubernetes by running: $ kubectl apply -f obj.yaml notice that you don’t need to specify the resource type of the object; it’s obtained from the object file itself. similarly, after you make changes to the object, you can use the apply command again to update the object: $ kubectl apply -f obj.yaml the apply tool will only modify objects that are different from the current objects in the cluster. if the objects you are creating already exist in the cluster, it will simply exit successfully without making any changes. this makes it useful for loops where you want to ensure the state of the cluster matches the state of the filesystem. you can repeatedly use apply to reconcile state. if you want to see what the apply command will do without actually making the changes, you can use the --dry-run flag to print the objects to the terminal without actually sending them to the server. if you feel like making interactive edits instead of editing a local file, you can instead use the edit command, which will download the latest object state and then launch an editor that contains the definition: $ kubectl edit <resource-name> <obj-name> after you save the file, it will be automatically uploaded back to the kubernetes cluster. the apply command also records the history of previous configurations in an anno‐ tation within the object. you can manipulate these records with the edit-lastapplied, set-last-applied, and view-last-applied commands. for example: $ kubectl apply -f myobj.yaml view-last-applied will show you the last state that was applied to the object. creating, updating, and destroying kubernetes objects | 39 when you want to delete an object, you can simply run: $ kubectl delete -f obj.yaml it is important to note that kubectl will not prompt you to confirm the deletion. once you issue the command, the object will be deleted. likewise, you can delete an object using the resource type and name: $ kubectl delete <resource-name> <obj-name> labeling and annotating objects labels and annotations are tags for your objects. we’ll discuss the differences in chapter 6, but for now, you can update the labels and annotations on any kubernetes object using the annotate and label commands. for example, to add the color=red label to a pod named bar, you can run: $ kubectl label pods bar color=red the syntax for annotations is identical. by default, label and annotate will not let you overwrite an existing label. to do this, you need to add the --overwrite flag. if you want to remove a label, you can use the <label-name>- syntax: $ kubectl label pods bar colorthis will remove the color label from the pod named bar. debugging commands kubectl also makes a number of commands available for debugging your containers. you can use the following to see the logs for a running container: $ kubectl logs <pod-name> if you have multiple containers in your pod, you can choose the container to view using the -c flag. by default, kubectl logs lists the current logs and exits. if you instead want to con‐ tinuously stream the logs back to the terminal without exiting, you can add the -f (follow) command-line flag. you can also use the exec command to execute a command in a running container: $ kubectl exec -it <pod-name> -- bash this will provide you with an interactive shell inside the running container so that you can perform more debugging. 40 | chapter 4: common kubectl commands if you don’t have bash or some other terminal available within your container, you can always attach to the running process: $ kubectl attach -it <pod-name> this will attach to the running process. it is similar to kubectl logs but will allow you to send input to the running process, assuming that process is set up to read from standard input. you can also copy files to and from a container using the cp command: $ kubectl cp <pod-name>:</path/to/remote/file> </path/to/local/file> this will copy a file from a running container to your local machine. you can also specify directories, or reverse the syntax to copy a file from your local machine back out into the container. if you want to access your pod via the network, you can use the port-forward com‐ mand to forward network traffic from the local machine to the pod. this enables you to securely tunnel network traffic through to containers that might not be exposed anywhere on the public network. for example, the following command: $ kubectl port-forward <pod-name> 8080:80 opens up a connection that forwards traffic from the local machine on port 8080 to the remote container on port 80. you can also use the port-forward command with services by specifying services/<service-name> instead of <pod-name>, but note that if you do port-forward to a service, the requests will only ever be forwarded to a single pod in that service. they will not go through the service load balancer. finally, if you are interested in how your cluster is using resources, you can use the top command to see the list of resources in use by either nodes or pods. this command: kubectl top nodes will display the total cpu and memory in use by the nodes in terms of both absolute units (e.g., cores) and percentage of available resources (e.g., total number of cores). similarly, this command: kubectl top pods will show all pods and their resource usage. by default it only displays pods in the current namespace, but you can add the --all-namespaces flag to see resource usage by all pods in the cluster. debugging commands | 41 command autocompletion kubectl supports integration with your shell to enable tab completion for both com‐ mands and resources. depending on your environment, you may need to install the bash-completion package before you activate command autocompletion. you can do this using the appropriate package manager: # macos brew install bash-completion # centos/red hat yum install bash-completion # debian/ubuntu apt-get install bash-completion when installing on macos, make sure to follow the instructions from brew about how to activate tab completion using your ${home}/.bashprofile. once bash-completion is installed, you can temporarily activate it for your terminal using: source <(kubectl completion bash) to make this automatic for every terminal, you can add it to your ${home}/.bashrc file: echo \"source <(kubectl completion bash)\" >> ${home}/.bashrc if you use zsh you can find similar instructions online. alternative ways of viewing your cluster in addition to kubectl, there are other tools for interacting with your kubernetes cluster. for example, there are plug-ins for several editors that integrate kubernetes and the editor environment, including: • visual studio code • intellij • eclipse additionally, there is an open source mobile application that allows you to access your cluster from your phone. 42 | chapter 4: common kubectl commands summary kubectl is a powerful tool for managing your applications in your kubernetes cluster. this chapter has illustrated many of the common uses for the tool, but kubectl has a great deal of built-in help available. you can start viewing this help with: $ kubectl help or: $ kubectl help <command-name> summary | 43  chapter 5 pods in earlier chapters we discussed how you might go about containerizing your applica‐ tion, but in real-world deployments of containerized applications you will often want to colocate multiple applications into a single atomic unit, scheduled onto a single machine. a canonical example of such a deployment is illustrated in figure 5-1, which consists of a container serving web requests and a container synchronizing the filesystem with a remote git repository. figure 5-1. an example pod with two containers and a shared filesystem at first, it might seem tempting to wrap up both the web server and the git syn‐ chronizer into a single container. after closer inspection, however, the reasons for the separation become clear. first, the two different containers have significantly different requirements in terms of resource usage. take, for example, memory. because the web server is serving user requests, we want to ensure that it is always available and responsive. on the other hand, the git synchronizer isn’t really user-facing and has a “best effort” quality of service. 45 suppose that our git synchronizer has a memory leak. we need to ensure that the git synchronizer cannot use up memory that we want to use for our web server, since this can affect web server performance or even crash the server. this sort of resource isolation is exactly the sort of thing that containers are designed to accomplish. by separating the two applications into two separate containers, we can ensure reliable web server operation. of course, the two containers are quite symbiotic; it makes no sense to schedule the web server on one machine and the git synchronizer on another. consequently, kubernetes groups multiple containers into a single atomic unit called a pod. (the name goes with the whale theme of docker containers, since a pod is also a group of whales.) though the concept of such sidecars seemed controversial or con‐ fusing when it was first introduced in kubernetes, it has subse‐ quently been adopted by a variety of different applications to deploy their infrastructure. for example, several service mesh implementations use sidecars to inject network management into an application’s pod. pods in kubernetes a pod represents a collection of application containers and volumes running in the same execution environment. pods, not containers, are the smallest deployable arti‐ fact in a kubernetes cluster. this means all of the containers in a pod always land on the same machine. each container within a pod runs in its own cgroup, but they share a number of linux namespaces. applications running in the same pod share the same ip address and port space (net‐ work namespace), have the same hostname (uts namespace), and can communicate using native interprocess communication channels over system v ipc or posix message queues (ipc namespace). however, applications in different pods are iso‐ lated from each other; they have different ip addresses, different hostnames, and more. containers in different pods running on the same node might as well be on different servers. thinking with pods one of the most common questions that occurs in the adoption of kubernetes is “what should i put in a pod?” 46 | chapter 5: pods sometimes people see pods and think, “aha! a wordpress container and a mysql database container should be in the same pod.” however, this kind of pod is actually an example of an anti-pattern for pod construction. there are two reasons for this. first, wordpress and its database are not truly symbiotic. if the wordpress container and the database container land on different machines, they still can work together quite effectively, since they communicate over a network connection. secondly, you don’t necessarily want to scale wordpress and the database as a unit. wordpress itself is mostly stateless, and thus you may want to scale your wordpress frontends in response to frontend load by creating more wordpress pods. scaling a mysql data‐ base is much trickier, and you would be much more likely to increase the resources dedicated to a single mysql pod. if you group the wordpress and mysql containers together in a single pod, you are forced to use the same scaling strategy for both con‐ tainers, which doesn’t fit well. in general, the right question to ask yourself when designing pods is, “will these con‐ tainers work correctly if they land on different machines?” if the answer is “no,” a pod is the correct grouping for the containers. if the answer is “yes,” multiple pods is probably the correct solution. in the example at the beginning of this chapter, the two containers interact via a local filesystem. it would be impossible for them to operate correctly if the containers were scheduled on different machines. in the remaining sections of this chapter, we will describe how to create, introspect, manage, and delete pods in kubernetes. the pod manifest pods are described in a pod manifest. the pod manifest is just a text-file representa‐ tion of the kubernetes api object. kubernetes strongly believes in declarative configu‐ ration. declarative configuration means that you write down the desired state of the world in a configuration and then submit that configuration to a service that takes actions to ensure the desired state becomes the actual state. declarative configuration is different from imperative configura‐ tion, where you simply take a series of actions (e.g., apt-get install foo) to modify the world. years of production experience have taught us that maintaining a written record of the system’s desired state leads to a more manageable, reliable system. declara‐ tive configuration enables numerous advantages, including code review for configurations as well as documenting the current state of the world for distributed teams. additionally, it is the basis for all of the self-healing behaviors in kubernetes that keep applica‐ tions running without user action. the pod manifest | 47 the kubernetes api server accepts and processes pod manifests before storing them in persistent storage (etcd). the scheduler also uses the kubernetes api to find pods that haven’t been scheduled to a node. the scheduler then places the pods onto nodes depending on the resources and other constraints expressed in the pod manifests. multiple pods can be placed on the same machine as long as there are sufficient resources. however, scheduling multiple replicas of the same application onto the same machine is worse for reliability, since the machine is a single failure domain. consequently, the kubernetes scheduler tries to ensure that pods from the same application are distributed onto different machines for reliability in the presence of such failures. once scheduled to a node, pods don’t move and must be explicitly destroyed and rescheduled. multiple instances of a pod can be deployed by repeating the workflow described here. however, replicasets (chapter 9) are better suited for running multiple instan‐ ces of a pod. (it turns out they’re also better at running a single pod, but we’ll get into that later.) creating a pod the simplest way to create a pod is via the imperative kubectl run command. for example, to run our same kuard server, use: $ kubectl run kuard --generator=run-pod/v1 \\\\  --image=gcr.io/kuar-demo/kuard-amd64:blue you can see the status of this pod by running: $ kubectl get pods you may initially see the container as pending, but eventually you will see it transition to running, which means that the pod and its containers have been successfully created. for now, you can delete this pod by running: $ kubectl delete pods/kuard we will now move on to writing a complete pod manifest by hand. creating a pod manifest pod manifests can be written using yaml or json, but yaml is generally preferred because it is slightly more human-editable and has the ability to add comments. pod manifests (and other kubernetes api objects) should really be treated in the same way as source code, and things like comments help explain the pod to new team members who are looking at them for the first time. 48 | chapter 5: pods pod manifests include a couple of key fields and attributes: namely a metadata sec‐ tion for describing the pod and its labels, a spec section for describing volumes, and a list of containers that will run in the pod. in chapter 2 we deployed kuard using the following docker command: $ docker run -d --name kuard \\\\  --publish 8080:8080 \\\\  gcr.io/kuar-demo/kuard-amd64:blue a similar result can be achieved by instead writing example 5-1 to a file named kuard-pod.yaml and then using kubectl commands to load that manifest to kubernetes. example 5-1. kuard-pod.yaml apiversion: v1 kind: pod metadata:  name: kuard spec:  containers:  - image: gcr.io/kuar-demo/kuard-amd64:blue  name: kuard  ports:  - containerport: 8080  name: http  protocol: tcp running pods in the previous section we created a pod manifest that can be used to start a pod run‐ ning kuard. use the kubectl apply command to launch a single instance of kuard: $ kubectl apply -f kuard-pod.yaml the pod manifest will be submitted to the kubernetes api server. the kubernetes system will then schedule that pod to run on a healthy node in the cluster, where it will be monitored by the kubelet daemon process. don’t worry if you don’t under‐ stand all the moving parts of kubernetes right now; we’ll get into more details throughout the book. listing pods now that we have a pod running, let’s go find out some more about it. using the kubectl command-line tool, we can list all pods running in the cluster. for now, this should only be the single pod that we created in the previous step: running pods | 49 $ kubectl get pods name ready status restarts age kuard 1/1 running 0 44s you can see the name of the pod (kuard) that we gave it in the previous yaml file. in addition to the number of ready containers (1/1), the output also shows the status, the number of times the pod was restarted, as well as the age of the pod. if you ran this command immediately after the pod was created, you might see: name ready status restarts age kuard 0/1 pending 0 1s the pending state indicates that the pod has been submitted but hasn’t been sched‐ uled yet. if a more significant error occurs (e.g., an attempt to create a pod with a container image that doesn’t exist), it will also be listed in the status field. by default, the kubectl command-line tool tries to be concise in the information it reports, but you can get more information via command-line flags. adding -o wide to any kubectl command will print out slightly more information (while still trying to keep the information to a single line). adding -o json or -o yaml will print out the complete objects in json or yaml, respectively. pod details sometimes, the single-line view is insufficient because it is too terse. additionally, kubernetes maintains numerous events about pods that are present in the event stream, not attached to the pod object. to find out more information about a pod (or any kubernetes object) you can use the kubectl describe command. for example, to describe the pod we previously cre‐ ated, you can run: $ kubectl describe pods kuard this outputs a bunch of information about the pod in different sections. at the top is basic information about the pod: name: kuard namespace: default node: node1/10.0.15.185 start time: sun, 02 jul 2017 15:00:38 -0700 labels: <none> annotations: <none> status: running ip: 192.168.199.238 controllers: <none> 50 | chapter 5: pods then there is information about the containers running in the pod: containers:  kuard:  container id: docker://055095…  image: gcr.io/kuar-demo/kuard-amd64:blue  image id: docker-pullable://gcr.io/kuar-demo/kuard-amd64@sha256:a580…  port: 8080/tcp  state: running  started: sun, 02 jul 2017 15:00:41 -0700  ready: true  restart count: 0  environment: <none>  mounts:  /var/run/secrets/kubernetes.io/serviceaccount from default-token-cg5f5 (ro) finally, there are events related to the pod, such as when it was scheduled, when its image was pulled, and if/when it had to be restarted because of failing health checks: events:  seen from subobjectpath type reason message  ---- ---- ------------- -------- ------ -------  50s default-scheduler normal scheduled success…  49s kubelet, node1 spec.containers{kuard} normal pulling pulling…  47s kubelet, node1 spec.containers{kuard} normal pulled success…  47s kubelet, node1 spec.containers{kuard} normal created created…  47s kubelet, node1 spec.containers{kuard} normal started started… deleting a pod when it is time to delete a pod, you can delete it either by name: $ kubectl delete pods/kuard or using the same file that you used to create it: $ kubectl delete -f kuard-pod.yaml when a pod is deleted, it is not immediately killed. instead, if you run kubectl get pods you will see that the pod is in the terminating state. all pods have a termina‐ tion grace period. by default, this is 30 seconds. when a pod is transitioned to terminating it no longer receives new requests. in a serving scenario, the grace period is important for reliability because it allows the pod to finish any active requests that it may be in the middle of processing before it is terminated. it’s important to note that when you delete a pod, any data stored in the containers associated with that pod will be deleted as well. if you want to persist data across mul‐ tiple instances of a pod, you need to use persistentvolumes, described at the end of this chapter. running pods | 51 accessing your pod now that your pod is running, you’re going to want to access it for a variety of rea‐ sons. you may want to load the web service that is running in the pod. you may want to view its logs to debug a problem that you are seeing, or even execute other com‐ mands in the context of the pod to help debug. the following sections detail various ways that you can interact with the code and data running inside your pod. using port forwarding later in the book, we’ll show how to expose a service to the world or other containers using load balancers—but oftentimes you simply want to access a specific pod, even if it’s not serving traffic on the internet. to achieve this, you can use the port-forwarding support built into the kubernetes api and command-line tools. when you run: $ kubectl port-forward kuard 8080:8080 a secure tunnel is created from your local machine, through the kubernetes master, to the instance of the pod running on one of the worker nodes. as long as the port-forward command is still running, you can access the pod (in this case the kuard web interface) at http://localhost:8080. getting more info with logs when your application needs debugging, it’s helpful to be able to dig deeper than describe to understand what the application is doing. kubernetes provides two com‐ mands for debugging running containers. the kubectl logs command downloads the current logs from the running instance: $ kubectl logs kuard adding the -f flag will cause you to continuously stream logs. the kubectl logs command always tries to get logs from the currently running con‐ tainer. adding the --previous flag will get logs from a previous instance of the con‐ tainer. this is useful, for example, if your containers are continuously restarting due to a problem at container startup. 52 | chapter 5: pods while using kubectl logs is useful for one-off debugging of con‐ tainers in production environments, it’s generally useful to use a log aggregation service. there are several open source log aggregation tools, like fluentd and elasticsearch, as well as numerous cloud logging providers. log aggregation services provide greater capacity for storing a longer duration of logs, as well as rich log searching and filtering capabilities. finally, they often provide the ability to aggregate logs from multiple pods into a single view. running commands in your container with exec sometimes logs are insufficient, and to truly determine what’s going on you need to execute commands in the context of the container itself. to do this you can use: $ kubectl exec kuard date you can also get an interactive session by adding the -it flags: $ kubectl exec -it kuard ash copying files to and from containers at times you may need to copy files from a remote container to a local machine for more in-depth exploration. for example, you can use a tool like wireshark to visual‐ ize tcpdump packet captures. suppose you had a file called /captures/capture3.txt inside a container in your pod. you could securely copy that file to your local machine by running: $ kubectl cp <pod-name>:/captures/capture3.txt ./capture3.txt other times you may need to copy files from your local machine into a container. let’s say you want to copy $home/config.txt to a remote container. in this case, you can run: $ kubectl cp $home/config.txt <pod-name>:/config.txt generally speaking, copying files into a container is an anti-pattern. you really should treat the contents of a container as immutable. but occasionally it’s the most immedi‐ ate way to stop the bleeding and restore your service to health, since it is quicker than building, pushing, and rolling out a new image. once the bleeding is stopped, how‐ ever, it is critically important that you immediately go and do the image build and rollout, or you are guaranteed to forget the local change that you made to your con‐ tainer and overwrite it in the subsequent regularly scheduled rollout. accessing your pod | 53 health checks when you run your application as a container in kubernetes, it is automatically kept alive for you using a process health check. this health check simply ensures that the main process of your application is always running. if it isn’t, kubernetes restarts it. however, in most cases, a simple process check is insufficient. for example, if your process has deadlocked and is unable to serve requests, a process health check will still believe that your application is healthy since its process is still running. to address this, kubernetes introduced health checks for application liveness. liveness health checks run application-specific logic (e.g., loading a web page) to ver‐ ify that the application is not just still running, but is functioning properly. since these liveness health checks are application-specific, you have to define them in your pod manifest. liveness probe once the kuard process is up and running, we need a way to confirm that it is actually healthy and shouldn’t be restarted. liveness probes are defined per container, which means each container inside a pod is health-checked separately. in example 5-2, we add a liveness probe to our kuard container, which runs an http request against the /healthy path on our container. example 5-2. kuard-pod-health.yaml apiversion: v1 kind: pod metadata:  name: kuard spec:  containers:  - image: gcr.io/kuar-demo/kuard-amd64:blue  name: kuard  livenessprobe:  httpget:  path: /healthy  port: 8080  initialdelayseconds: 5  timeoutseconds: 1  periodseconds: 10  failurethreshold: 3  ports:  - containerport: 8080  name: http  protocol: tcp 54 | chapter 5: pods the preceding pod manifest uses an httpget probe to perform an http get request against the /healthy endpoint on port 8080 of the kuard container. the probe sets an initialdelayseconds of 5, and thus will not be called until 5 seconds after all the containers in the pod are created. the probe must respond within the 1-second time‐ out, and the http status code must be equal to or greater than 200 and less than 400 to be considered successful. kubernetes will call the probe every 10 seconds. if more than three consecutive probes fail, the container will fail and restart. you can see this in action by looking at the kuard status page. create a pod using this manifest and then port-forward to that pod: $ kubectl apply -f kuard-pod-health.yaml $ kubectl port-forward kuard 8080:8080 point your browser to http://localhost:8080. click the “liveness probe” tab. you should see a table that lists all of the probes that this instance of kuard has received. if you click the “fail” link on that page, kuard will start to fail health checks. wait long enough and kubernetes will restart the container. at that point the display will reset and start over again. details of the restart can be found with kubectl describe pods kuard. the “events” section will have text similar to the following: killing container with id docker://2ac946...:pod \"kuarddefault(9ee84...)\" container \"kuard\" is unhealthy, it will be killed and re-created. while the default response to a failed liveness check is to restart the pod, the actual behavior is governed by the pod’s restartpolicy. there are three options for the restart policy: always (the default), onfailure (restart only on liveness failure or nonzero process exit code), or never. readiness probe of course, liveness isn’t the only kind of health check we want to perform. kubernetes makes a distinction between liveness and readiness. liveness determines if an applica‐ tion is running properly. containers that fail liveness checks are restarted. readiness describes when a container is ready to serve user requests. containers that fail readi‐ ness checks are removed from service load balancers. readiness probes are config‐ ured similarly to liveness probes. we explore kubernetes services in detail in chapter 7. combining the readiness and liveness probes helps ensure only healthy containers are running within the cluster. health checks | 55 types of health checks in addition to http checks, kubernetes also supports tcpsocket health checks that open a tcp socket; if the connection is successful, the probe succeeds. this style of probe is useful for non-http applications; for example, databases or other non– http-based apis. finally, kubernetes allows exec probes. these execute a script or program in the con‐ text of the container. following typical convention, if this script returns a zero exit code, the probe succeeds; otherwise, it fails. exec scripts are often useful for custom application validation logic that doesn’t fit neatly into an http call. resource management most people move into containers and orchestrators like kubernetes because of the radical improvements in image packaging and reliable deployment they provide. in addition to application-oriented primitives that simplify distributed system develop‐ ment, equally important is the ability to increase the overall utilization of the com‐ pute nodes that make up the cluster. the basic cost of operating a machine, either virtual or physical, is basically constant regardless of whether it is idle or fully loaded. consequently, ensuring that these machines are maximally active increases the effi‐ ciency of every dollar spent on infrastructure. generally speaking, we measure this efficiency with the utilization metric. utilization is defined as the amount of a resource actively being used divided by the amount of a resource that has been purchased. for example, if you purchase a one-core machine, and your application uses one-tenth of a core, then your utilization is 10%. with scheduling systems like kubernetes managing resource packing, you can drive your utilization to greater than 50%. to achieve this, you have to tell kubernetes about the resources your application requires, so that kubernetes can find the optimal packing of containers onto pur‐ chased machines. kubernetes allows users to specify two different resource metrics. resource requests specify the minimum amount of a resource required to run the application. resource limits specify the maximum amount of a resource that an application can consume. both of these resource definitions are described in greater detail in the following sections. resource requests: minimum required resources with kubernetes, a pod requests the resources required to run its containers. kuber‐ netes guarantees that these resources are available to the pod. the most commonly 56 | chapter 5: pods requested resources are cpu and memory, but kubernetes has support for other resource types as well, such as gpus and more. for example, to request that the kuard container lands on a machine with half a cpu free and gets 128 mb of memory allocated to it, we define the pod as shown in example 5-3. example 5-3. kuard-pod-resreq.yaml apiversion: v1 kind: pod metadata:  name: kuard spec:  containers:  - image: gcr.io/kuar-demo/kuard-amd64:blue  name: kuard  resources:  requests:  cpu: \"500m\"  memory: \"128mi\"  ports:  - containerport: 8080  name: http  protocol: tcp resources are requested per container, not per pod. the total resources requested by the pod is the sum of all resources reques‐ ted by all containers in the pod. the reason for this is that in many cases the different containers have very different cpu require‐ ments. for example, in the web server and data synchronizer pod, the web server is user-facing and likely needs a great deal of cpu, while the data synchronizer can make do with very little. request limit details requests are used when scheduling pods to nodes. the kubernetes scheduler will ensure that the sum of all requests of all pods on a node does not exceed the capacity of the node. therefore, a pod is guaranteed to have at least the requested resources when running on the node. importantly, “request” specifies a minimum. it does not specify a maximum cap on the resources a pod may use. to explore what this means, let’s look at an example. imagine that we have container whose code attempts to use all available cpu cores. suppose that we create a pod with this container that requests 0.5 cpu. kubernetes schedules this pod onto a machine with a total of 2 cpu cores. resource management | 57 as long as it is the only pod on the machine, it will consume all 2.0 of the available cores, despite only requesting 0.5 cpu. if a second pod with the same container and the same request of 0.5 cpu lands on the machine, then each pod will receive 1.0 cores. if a third identical pod is scheduled, each pod will receive 0.66 cores. finally, if a fourth identical pod is scheduled, each pod will receive the 0.5 core it requested, and the node will be at capacity. cpu requests are implemented using the cpu-shares functionality in the linux kernel. memory requests are handled similarly to cpu, but there is an important difference. if a container is over its memory request, the os can’t just remove memory from the process, because it’s been allocated. consequently, when the system runs out of memory, the kubelet terminates containers whose memory usage is greater than their requested memory. these containers are automatically restarted, but with less available memory on the machine for the container to consume. since resource requests guarantee resource availability to a pod, they are critical to ensuring that containers have sufficient resources in high-load situations. capping resource usage with limits in addition to setting the resources required by a pod, which establishes the mini‐ mum resources available to the pod, you can also set a maximum on a pod’s resource usage via resource limits. in our previous example we created a kuard pod that requested a minimum of 0.5 of a core and 128 mb of memory. in the pod manifest in example 5-4, we extend this configuration to add a limit of 1.0 cpu and 256 mb of memory. example 5-4. kuard-pod-reslim.yaml apiversion: v1 kind: pod metadata:  name: kuard spec:  containers:  - image: gcr.io/kuar-demo/kuard-amd64:blue  name: kuard  resources:  requests: 58 | chapter 5: pods  cpu: \"500m\"  memory: \"128mi\"  limits:  cpu: \"1000m\"  memory: \"256mi\"  ports:  - containerport: 8080  name: http  protocol: tcp when you establish limits on a container, the kernel is configured to ensure that con‐ sumption cannot exceed these limits. a container with a cpu limit of 0.5 cores will only ever get 0.5 cores, even if the cpu is otherwise idle. a container with a memory limit of 256 mb will not be allowed additional memory (e.g., malloc will fail) if its memory usage exceeds 256 mb. persisting data with volumes when a pod is deleted or a container restarts, any and all data in the container’s file‐ system is also deleted. this is often a good thing, since you don’t want to leave around cruft that happened to be written by your stateless web application. in other cases, having access to persistent disk storage is an important part of a healthy application. kubernetes models such persistent storage. using volumes with pods to add a volume to a pod manifest, there are two new stanzas to add to our configu‐ ration. the first is a new spec.volumes section. this array defines all of the volumes that may be accessed by containers in the pod manifest. it’s important to note that not all containers are required to mount all volumes defined in the pod. the second addi‐ tion is the volumemounts array in the container definition. this array defines the vol‐ umes that are mounted into a particular container, and the path where each volume should be mounted. note that two different containers in a pod can mount the same volume at different mount paths. the manifest in example 5-5 defines a single new volume named kuard-data, which the kuard container mounts to the /data path. example 5-5. kuard-pod-vol.yaml apiversion: v1 kind: pod metadata:  name: kuard spec:  volumes: persisting data with volumes | 59  - name: \"kuard-data\"  hostpath:  path: \"/var/lib/kuard\"  containers:  - image: gcr.io/kuar-demo/kuard-amd64:blue  name: kuard  volumemounts:  - mountpath: \"/data\"  name: \"kuard-data\"  ports:  - containerport: 8080  name: http  protocol: tcp different ways of using volumes with pods there are a variety of ways you can use data in your application. the following are a few, and the recommended patterns for kubernetes. communication/synchronization in the first example of a pod, we saw how two containers used a shared volume to serve a site while keeping it synchronized to a remote git location. to achieve this, the pod uses an emptydir volume. such a volume is scoped to the pod’s lifespan, but it can be shared between two containers, forming the basis for communication between our git sync and web serving containers. cache an application may use a volume that is valuable for performance, but not required for correct operation of the application. for example, perhaps the application keeps prerendered thumbnails of larger images. of course, they can be reconstructed from the original images, but that makes serving the thumbnails more expensive. you want such a cache to survive a container restart due to a health-check failure, and thus emptydir works well for the cache use case as well. persistent data sometimes you will use a volume for truly persistent data—data that is independent of the lifespan of a particular pod, and should move between nodes in the cluster if a node fails or a pod moves to a different machine for some reason. to achieve this, kubernetes supports a wide variety of remote network storage volumes, including widely supported protocols like nfs and iscsi as well as cloud provider network storage like amazon’s elastic block store, azure’s files and disk storage, as well as google’s persistent disk. 60 | chapter 5: pods mounting the host \\x80lesystem other applications don’t actually need a persistent volume, but they do need some access to the underlying host filesystem. for example, they may need access to the /dev filesystem in order to perform raw block-level access to a device on the sys‐ tem. for these cases, kubernetes supports the hostpath volume, which can mount arbitrary locations on the worker node into the container. the previous example uses the hostpath volume type. the volume created is /var/lib/ kuard on the host. persisting data using remote disks oftentimes, you want the data a pod is using to stay with the pod, even if it is restar‐ ted on a different host machine. to achieve this, you can mount a remote network storage volume into your pod. when using network-based storage, kubernetes automatically mounts and unmounts the appropriate storage whenever a pod using that volume is scheduled onto a partic‐ ular machine. there are numerous methods for mounting volumes over the network. kubernetes includes support for standard protocols such as nfs and iscsi as well as cloud pro‐ vider–based storage apis for the major cloud providers (both public and private). in many cases, the cloud providers will also create the disk for you if it doesn’t already exist. here is an example of using an nfs server: ... # rest of pod definition above here volumes:  - name: \"kuard-data\"  nfs:  server: my.nfs.server.local  path: \"/exports\" persistent volumes are a deep topic that has many different details: in particular, the manner in which persistent volumes, persistent volume claims, and dynamic volume provisioning work together. there is a more in-depth examination of the subject in chapter 15. putting it all together many applications are stateful, and as such we must preserve any data and ensure access to the underlying storage volume regardless of what machine the application runs on. as we saw earlier, this can be achieved using a persistent volume backed by network-attached storage. we also want to ensure that a healthy instance of the putting it all together | 61 application is running at all times, which means we want to make sure the container running kuard is ready before we expose it to clients. through a combination of persistent volumes, readiness and liveness probes, and resource restrictions, kubernetes provides everything needed to run stateful applica‐ tions reliably. example 5-6 pulls this all together into one manifest. example 5-6. kuard-pod-full.yaml apiversion: v1 kind: pod metadata:  name: kuard spec:  volumes:  - name: \"kuard-data\"  nfs:  server: my.nfs.server.local  path: \"/exports\"  containers:  - image: gcr.io/kuar-demo/kuard-amd64:blue  name: kuard  ports:  - containerport: 8080  name: http  protocol: tcp  resources:  requests:  cpu: \"500m\"  memory: \"128mi\"  limits:  cpu: \"1000m\"  memory: \"256mi\"  volumemounts:  - mountpath: \"/data\"  name: \"kuard-data\"  livenessprobe:  httpget:  path: /healthy  port: 8080  initialdelayseconds: 5  timeoutseconds: 1  periodseconds: 10  failurethreshold: 3  readinessprobe:  httpget:  path: /ready  port: 8080  initialdelayseconds: 30  timeoutseconds: 1 62 | chapter 5: pods  periodseconds: 10  failurethreshold: 3 summary pods represent the atomic unit of work in a kubernetes cluster. pods are comprised of one or more containers working together symbiotically. to create a pod, you write a pod manifest and submit it to the kubernetes api server by using the command-line tool or (less frequently) by making http and json calls to the server directly. once you’ve submitted the manifest to the api server, the kubernetes scheduler finds a machine where the pod can fit and schedules the pod to that machine. once sched‐ uled, the kubelet daemon on that machine is responsible for creating the containers that correspond to the pod, as well as performing any health checks defined in the pod manifest. once a pod is scheduled to a node, no rescheduling occurs if that node fails. addi‐ tionally, to create multiple replicas of the same pod you have to create and name them manually. in a later chapter we introduce the replicaset object and show how you can automate the creation of multiple identical pods and ensure that they are recreated in the event of a node machine failure. summary | 63  chapter 6 labels and annotations kubernetes was made to grow with you as your application scales in both size and complexity. with this in mind, labels and annotations were added as foundational concepts. labels and annotations let you work in sets of things that map to how you think about your application. you can organize, mark, and cross-index all of your resources to represent the groups that make the most sense for your application. labels are key/value pairs that can be attached to kubernetes objects such as pods and replicasets. they can be arbitrary, and are useful for attaching identifying informa‐ tion to kubernetes objects. labels provide the foundation for grouping objects. annotations, on the other hand, provide a storage mechanism that resembles labels: annotations are key/value pairs designed to hold nonidentifying information that can be leveraged by tools and libraries. labels labels provide identifying metadata for objects. these are fundamental qualities of the object that will be used for grouping, viewing, and operating. 65 the motivations for labels grew out of google’s experience in run‐ ning large and complex applications. there were a couple of les‐ sons that emerged from this experience. see the great site reliability book site reliability engineering by betsy beyer et al. (o’reilly) for some deeper background on how google approaches production systems. the first lesson is that production abhors a singleton. when deploying software, users will often start with a single instance. however, as the application matures, these singletons often multi‐ ply and become sets of objects. with this in mind, kubernetes uses labels to deal with sets of objects instead of single instances. the second lesson is that any hierarchy imposed by the system will fall short for many users. in addition, user groupings and hierar‐ chies are subject to change over time. for instance, a user may start out with the idea that all apps are made up of many services. how‐ ever, over time, a service may be shared across multiple apps. kubernetes labels are flexible enough to adapt to these situations and more. labels have simple syntax. they are key/value pairs, where both the key and value are represented by strings. label keys can be broken down into two parts: an optional prefix and a name, separated by a slash. the prefix, if specified, must be a dns sub‐ domain with a 253-character limit. the key name is required and must be shorter than 63 characters. names must also start and end with an alphanumeric character and permit the use of dashes (-), underscores (), and dots (.) between characters. label values are strings with a maximum length of 63 characters. the contents of the label values follow the same rules as for label keys. table 6-1 shows some valid label keys and values. table 6-1. label examples key value acme.com/app-version 1.0.0 appversion 1.0.0 app.version 1.0.0 kubernetes.io/cluster-service true when domain names are used in labels and annotations they are expected to be aligned to that particular entity in some way. for example, a project might define a canonical set of labels used to identify the various stages of application deployment (e.g., staging, canary, production). 66 | chapter 6: labels and annotations or a cloud provider might define provider-specific annotations that extend kuber‐ netes objects to activate features specific to their service. applying labels here we create a few deployments (a way to create an array of pods) with some inter‐ esting labels. we’ll take two apps (called alpaca and bandicoot) and have two envi‐ ronments for each. we will also have two different versions: 1. first, create the alpaca-prod deployment and set the ver, app, and env labels: $ kubectl run alpaca-prod \\\\  --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\  --replicas=2 \\\\  --labels=\"ver=1,app=alpaca,env=prod\" 2. next, create the alpaca-test deployment and set the ver, app, and env labels with the appropriate values: $ kubectl run alpaca-test \\\\  --image=gcr.io/kuar-demo/kuard-amd64:green \\\\  --replicas=1 \\\\  --labels=\"ver=2,app=alpaca,env=test\" 3. finally, create two deployments for bandicoot. here we name the environments prod and staging: $ kubectl run bandicoot-prod \\\\  --image=gcr.io/kuar-demo/kuard-amd64:green \\\\  --replicas=2 \\\\  --labels=\"ver=2,app=bandicoot,env=prod\" $ kubectl run bandicoot-staging \\\\  --image=gcr.io/kuar-demo/kuard-amd64:green \\\\  --replicas=1 \\\\  --labels=\"ver=2,app=bandicoot,env=staging\" at this point you should have four deployments—alpaca-prod, alpaca-test, bandicoot-prod, and bandicoot-staging: $ kubectl get deployments --show-labels name ... labels alpaca-prod ... app=alpaca,env=prod,ver=1 alpaca-test ... app=alpaca,env=test,ver=2 bandicoot-prod ... app=bandicoot,env=prod,ver=2 bandicoot-staging ... app=bandicoot,env=staging,ver=2 we can visualize this as a venn diagram based on the labels (figure 6-1). labels | 67 figure 6-1. visualization of labels applied to our deployments modifying labels labels can also be applied (or updated) on objects after they are created: $ kubectl label deployments alpaca-test \"canary=true\" there is a caveat to be aware of here. in this example, the kubectl label command will only change the label on the deployment itself; it won’t affect the objects (replicasets and pods) the deploy‐ ment creates. to change those, you’ll need to change the template embedded in the deployment (see chapter 10). you can also use the -l option to kubectl get to show a label value as a column: $ kubectl get deployments -l canary name desired current ... canary alpaca-prod 2 2 ... <none> alpaca-test 1 1 ... true bandicoot-prod 2 2 ... <none> bandicoot-staging 1 1 ... <none> you can remove a label by applying a dash suffix: $ kubectl label deployments alpaca-test \"canary-\" label selectors label selectors are used to filter kubernetes objects based on a set of labels. selectors use a simple boolean language. they are used both by end users (via tools like kubectl) and by different types of objects (such as how a replicaset relates to its pods). 68 | chapter 6: labels and annotations each deployment (via a replicaset) creates a set of pods using the labels specified in the template embedded in the deployment. this is configured by the kubectl run command. running the kubectl get pods command should return all the pods currently run‐ ning in the cluster. we should have a total of six kuard pods across our three environments: $ kubectl get pods --show-labels name ... labels alpaca-prod-3408831585-4nzfb ... app=alpaca,env=prod,ver=1,... alpaca-prod-3408831585-kga0a ... app=alpaca,env=prod,ver=1,... alpaca-test-1004512375-3r1m5 ... app=alpaca,env=test,ver=2,... bandicoot-prod-373860099-0t1gp ... app=bandicoot,env=prod,ver=2,... bandicoot-prod-373860099-k2wcf ... app=bandicoot,env=prod,ver=2,... bandicoot-staging-1839769971-3ndv ... app=bandicoot,env=staging,ver=2,... you may see a new label that we haven’t seen yet: pod-templatehash. this label is applied by the deployment so it can keep track of which pods were generated from which template versions. this allows the deployment to manage updates in a clean way, as will be covered in depth in chapter 10. if we only wanted to list pods that had the ver label set to 2, we could use the --selector flag: $ kubectl get pods --selector=\"ver=2\" name ready status restarts age alpaca-test-1004512375-3r1m5 1/1 running 0 3m bandicoot-prod-373860099-0t1gp 1/1 running 0 3m bandicoot-prod-373860099-k2wcf 1/1 running 0 3m bandicoot-staging-1839769971-3ndv5 1/1 running 0 3m if we specify two selectors separated by a comma, only the objects that satisfy both will be returned. this is a logical and operation: $ kubectl get pods --selector=\"app=bandicoot,ver=2\" name ready status restarts age bandicoot-prod-373860099-0t1gp 1/1 running 0 4m bandicoot-prod-373860099-k2wcf 1/1 running 0 4m bandicoot-staging-1839769971-3ndv5 1/1 running 0 4m we can also ask if a label is one of a set of values. here we ask for all pods where the app label is set to alpaca or bandicoot (which will be all six pods): $ kubectl get pods --selector=\"app in (alpaca,bandicoot)\" labels | 69 name ready status restarts age alpaca-prod-3408831585-4nzfb 1/1 running 0 6m alpaca-prod-3408831585-kga0a 1/1 running 0 6m alpaca-test-1004512375-3r1m5 1/1 running 0 6m bandicoot-prod-373860099-0t1gp 1/1 running 0 6m bandicoot-prod-373860099-k2wcf 1/1 running 0 6m bandicoot-staging-1839769971-3ndv5 1/1 running 0 6m finally, we can ask if a label is set at all. here we are asking for all of the deployments with the canary label set to anything: $ kubectl get deployments --selector=\"canary\" name desired current up-to-date available age alpaca-test 1 1 1 1 7m there are also “negative” versions of each of these, as shown in table 6-2. table 6-2. selector operators operator description key=value key is set to value key!=value key is not set to value key in (value1, value2) key is one of value1 or value2 key notin (value1, value2) key is not one of value1 or value2 key key is set !key key is not set for example, asking if a key, in this case canary, is not set can look like: $ kubectl get deployments --selector=\\'!canary\\' similarly, you can combine positive and negative selectors together as follows: $ kubectl get pods -l \\'ver=2,!canary\\' label selectors in api objects when a kubernetes object refers to a set of other kubernetes objects, a label selector is used. instead of a simple string as described in the previous section, we use a parsed structure. for historical reasons (kubernetes doesn’t break api compatibility!), there are two forms. most objects support a newer, more powerful set of selector operators. a selector of app=alpaca,ver in (1, 2) would be converted to this: selector:  matchlabels:  app: alpaca 70 | chapter 6: labels and annotations  matchexpressions:  - {key: ver, operator: in, values: } compact yaml syntax. this is an item in a list (matchexpressions) that is a map with three entries. the last entry (values) has a value that is a list with two items. all of the terms are evaluated as a logical and. the only way to represent the != operator is to convert it to a notin expression with a single value. the older form of specifying selectors (used in replicationcontrollers and serv‐ ices) only supports the = operator. this is a simple set of key/value pairs that must all match a target object to be selected. the selector app=alpaca,ver=1 would be represented like this: selector:  app: alpaca  ver: 1 labels in the kubernetes architecture in addition to enabling users to organize their infrastructure, labels play a critical role in linking various related kubernetes objects. kubernetes is a purposefully decoupled system. there is no hierarchy and all components operate independently. however, in many cases objects need to relate to one another, and these relationships are defined by labels and label selectors. for example, replicasets, which create and maintain multiple replicas of a pod, find the pods that they are managing via a selector. likewise, a service load balancer finds the pods it should bring traffic to via a selector query. when a pod is created, it can use a node selector to identify a particular set of nodes that it can be scheduled onto. when people want to restrict network traffic in their cluster, they use networkpolicy in conjunction with specific labels to identify pods that should or should not be allowed to communicate with each other. labels are a powerful and ubiquitous glue that holds a kubernetes application together. though your application will likely start out with a simple set of labels and queries, you should expect it to grow in size and sophistication with time. annotations annotations provide a place to store additional metadata for kubernetes objects with the sole purpose of assisting tools and libraries. they are a way for other programs driving kubernetes via an api to store some opaque data with an object. annotations can be used for the tool itself or to pass configuration information between external systems. annotations | 71 while labels are used to identify and group objects, annotations are used to provide extra information about where an object came from, how to use it, or policy around that object. there is overlap, and it is a matter of taste as to when to use an annotation or a label. when in doubt, add information to an object as an annotation and pro‐ mote it to a label if you find yourself wanting to use it in a selector. annotations are used to: • keep track of a “reason” for the latest update to an object. • communicate a specialized scheduling policy to a specialized scheduler. • extend data about the last tool to update the resource and how it was updated (used for detecting changes by other tools and doing a smart merge). • attach build, release, or image information that isn’t appropriate for labels (may include a git hash, timestamp, pr number, etc.). • enable the deployment object (chapter 10) to keep track of replicasets that it is managing for rollouts. • provide extra data to enhance the visual quality or usability of a ui. for example, objects could include a link to an icon (or a base64-encoded version of an icon). • prototype alpha functionality in kubernetes (instead of creating a first-class api field, the parameters for that functionality are encoded in an annotation). annotations are used in various places in kubernetes, with the primary use case being rolling deployments. during rolling deployments, annotations are used to track rollout status and provide the necessary information required to roll back a deploy‐ ment to a previous state. users should avoid using the kubernetes api server as a general-purpose database. annotations are good for small bits of data that are highly associated with a specific resource. if you want to store data in kubernetes but you don’t have an obvious object to associate it with, consider storing that data in some other, more appropriate database. de\\x80ning annotations annotation keys use the same format as label keys. however, because they are often used to communicate information between tools, the “namespace” part of the key is more important. example keys include deployment.kubernetes.io/revision or kubernetes.io/change-cause. the value component of an annotation is a free-form string field. while this allows maximum flexibility as users can store arbitrary data, because this is arbitrary text, there is no validation of any format. for example, it is not uncommon for a json document to be encoded as a string and stored in an annotation. it is important to 72 | chapter 6: labels and annotations note that the kubernetes server has no knowledge of the required format of annota‐ tion values. if annotations are used to pass or store data, there is no guarantee the data is valid. this can make tracking down errors more difficult. annotations are defined in the common metadata section in every kubernetes object: ... metadata:  annotations:  example.com/icon-url: \"https://example.com/icon.png\" ... annotations are very convenient and provide powerful loose cou‐ pling. however, they should be used judiciously to avoid an unty‐ ped mess of data. cleanup it is easy to clean up all of the deployments that we started in this chapter: $ kubectl delete deployments --all if you want to be more selective, you can use the --selector flag to choose which deployments to delete. summary labels are used to identify and optionally group objects in a kubernetes cluster. labels are also used in selector queries to provide flexible runtime grouping of objects such as pods. annotations provide object-scoped key/value storage of metadata that can be used by automation tooling and client libraries. annotations can also be used to hold configu‐ ration data for external tools such as third-party schedulers and monitoring tools. labels and annotations are vital to understanding how key components in a kuber‐ netes cluster work together to ensure the desired cluster state. using labels and anno‐ tations properly unlocks the true power of kubernetes’s flexibility and provides the starting point for building automation tools and deployment workflows. cleanup | 73  chapter 7 service discovery kubernetes is a very dynamic system. the system is involved in placing pods on nodes, making sure they are up and running, and rescheduling them as needed. there are ways to automatically change the number of pods based on load (such as horizontal pod autoscaling ). the apidriven nature of the system encourages others to create higher and higher levels of automation. while the dynamic nature of kubernetes makes it easy to run a lot of things, it creates problems when it comes to finding those things. most of the traditional network infrastructure wasn’t built for the level of dynamism that kubernetes presents. what is service discovery? the general name for this class of problems and solutions is service discovery. servicediscovery tools help solve the problem of finding which processes are listening at which addresses for which services. a good service-discovery system will enable users to resolve this information quickly and reliably. a good system is also low-latency; clients are updated soon after the information associated with a service changes. finally, a good service-discovery system can store a richer definition of what that ser‐ vice is. for example, perhaps there are multiple ports associated with the service. the domain name system (dns) is the traditional system of service discovery on the internet. dns is designed for relatively stable name resolution with wide and effi‐ cient caching. it is a great system for the internet but falls short in the dynamic world of kubernetes. unfortunately, many systems (for example, java, by default) look up a name in dns directly and never re-resolve. this can lead to clients caching stale mappings and talking to the wrong ip. even with short ttls and well-behaved clients, there is a 75 natural delay between when a name resolution changes and when the client notices. there are natural limits to the amount and type of information that can be returned in a typical dns query, too. things start to break past 20–30 a records for a single name. srv records solve some problems, but are often very hard to use. finally, the way that clients handle multiple ips in a dns record is usually to take the first ip address and rely on the dns server to randomize or round-robin the order of records. this is no substitute for more purpose-built load balancing. the service object real service discovery in kubernetes starts with a service object. a service object is a way to create a named label selector. as we will see, the service object does some other nice things for us, too. just as the kubectl run command is an easy way to create a kubernetes deployment, we can use kubectl expose to create a service. let’s create some deployments and services so we can see how they work: $ kubectl run alpaca-prod \\\\  --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\  --replicas=3 \\\\  --port=8080 \\\\  --labels=\"ver=1,app=alpaca,env=prod\" $ kubectl expose deployment alpaca-prod $ kubectl run bandicoot-prod \\\\  --image=gcr.io/kuar-demo/kuard-amd64:green \\\\  --replicas=2 \\\\  --port=8080 \\\\  --labels=\"ver=2,app=bandicoot,env=prod\" $ kubectl expose deployment bandicoot-prod $ kubectl get services -o wide name cluster-ip ... port(s) ... selector alpaca-prod 10.115.245.13 ... 8080/tcp ... app=alpaca,env=prod,ver=1 bandicoot-prod 10.115.242.3 ... 8080/tcp ... app=bandicoot,env=prod,ver=2 kubernetes 10.115.240.1 ... 443/tcp ... <none> after running these commands, we have three services. the ones we just created are alpaca-prod and bandicoot-prod. the kubernetes service is automatically created for you so that you can find and talk to the kubernetes api from within the app. if we look at the selector column, we see that the alpaca-prod service simply gives a name to a selector and specifies which ports to talk to for that service. the kubectl expose command will conveniently pull both the label selector and the relevant ports (8080, in this case) from the deployment definition. 76 | chapter 7: service discovery furthermore, that service is assigned a new type of virtual ip called a cluster ip. this is a special ip address the system will load-balance across all of the pods that are iden‐ tified by the selector. to interact with services, we are going to port forward to one of the alpaca pods. start and leave this command running in a terminal window. you can see the port forward working by accessing the alpaca pod at http://localhost:48858: $ alpacapod=$(kubectl get pods -l app=alpaca \\\\  -o jsonpath=\\'{.items.metadata.name}\\') $ kubectl port-forward $alpacapod 48858:8080 service dns because the cluster ip is virtual, it is stable, and it is appropriate to give it a dns address. all of the issues around clients caching dns results no longer apply. within a namespace, it is as easy as just using the service name to connect to one of the pods identified by a service. kubernetes provides a dns service exposed to pods running in the cluster. this kubernetes dns service was installed as a system component when the cluster was first created. the dns service is, itself, managed by kubernetes and is a great exam‐ ple of kubernetes building on kubernetes. the kubernetes dns service provides dns names for cluster ips. you can try this out by expanding the “dns query” section on the kuard server sta‐ tus page. query the a record for alpaca-prod. the output should look something like this: ;; opcode: query, status: noerror, id: 12071 ;; flags: qr aa rd ra; query: 1, answer: 1, authority: 0, additional: 0 ;; question section: ;alpaca-prod.default.svc.cluster.local. in a ;; answer section: alpaca-prod.default.svc.cluster.local. 30 in a 10.115.245.13 the full dns name here is alpaca-prod.default.svc.cluster.local.. let’s break this down: alpaca-prod the name of the service in question. default the namespace that this service is in. the service object | 77 svc recognizing that this is a service. this allows kubernetes to expose other types of things as dns in the future. cluster.local. the base domain name for the cluster. this is the default and what you will see for most clusters. administrators may change this to allow unique dns names across multiple clusters. when referring to a service in your own namespace you can just use the service name (alpaca-prod). you can also refer to a service in another namespace with alpacaprod.default. and, of course, you can use the fully qualified service name (alpacaprod.default.svc.cluster.local.). try each of these out in the “dns query” section of kuard. readiness checks often, when an application first starts up it isn’t ready to handle requests. there is usually some amount of initialization that can take anywhere from under a second to several minutes. one nice thing the service object does is track which of your pods are ready via a readiness check. let’s modify our deployment to add a readiness check that is attached to a pod, as we discussed in chapter 5: $ kubectl edit deployment/alpaca-prod this command will fetch the current version of the alpaca-prod deployment and bring it up in an editor. after you save and quit your editor, it’ll then write the object back to kubernetes. this is a quick way to edit an object without saving it to a yaml file. add the following section: spec:  ...  template:  ...  spec:  containers:  ...  name: alpaca-prod  readinessprobe:  httpget:  path: /ready  port: 8080  periodseconds: 2  initialdelayseconds: 0  failurethreshold: 3  successthreshold: 1 78 | chapter 7: service discovery this sets up the pods this deployment will create so that they will be checked for readiness via an http get to /ready on port 8080. this check is done every 2 sec‐ onds starting as soon as the pod comes up. if three successive checks fail, then the pod will be considered not ready. however, if only one check succeeds, the pod will again be considered ready. only ready pods are sent traffic. updating the deployment definition like this will delete and recreate the alpaca pods. as such, we need to restart our port-forward command from earlier: $ alpacapod=$(kubectl get pods -l app=alpaca \\\\  -o jsonpath=\\'{.items.metadata.name}\\') $ kubectl port-forward $alpacapod 48858:8080 point your browser to http://localhost:48858 and you should see the debug page for that instance of kuard. expand the “readiness probe” section. you should see this page update every time there is a new readiness check from the system, which should happen every 2 seconds. in another terminal window, start a watch command on the endpoints for the alpaca-prod service. endpoints are a lower-level way of finding what a service is sending traffic to and are covered later in this chapter. the --watch option here causes the kubectl command to hang around and output any updates. this is an easy way to see how a kubernetes object changes over time: $ kubectl get endpoints alpaca-prod --watch now go back to your browser and hit the “fail” link for the readiness check. you should see that the server is now returning 500s. after three of these, this server is removed from the list of endpoints for the service. hit the “succeed” link and notice that after a single readiness check the endpoint is added back. this readiness check is a way for an overloaded or sick server to signal to the system that it doesn’t want to receive traffic anymore. this is a great way to implement grace‐ ful shutdown. the server can signal that it no longer wants traffic, wait until existing connections are closed, and then cleanly exit. press ctrl-c to exit out of both the port-forward and watch commands in your terminals. looking beyond the cluster so far, everything we’ve covered in this chapter has been about exposing services inside of a cluster. oftentimes, the ips for pods are only reachable from within the cluster. at some point, we have to allow new traffic in! looking beyond the cluster | 79 the most portable way to do this is to use a feature called nodeports, which enhance a service even further. in addition to a cluster ip, the system picks a port (or the user can specify one), and every node in the cluster then forwards traffic to that port to the service. with this feature, if you can reach any node in the cluster you can contact a service. you use the nodeport without knowing where any of the pods for that service are running. this can be integrated with hardware or software load balancers to expose the service further. try this out by modifying the alpaca-prod service: $ kubectl edit service alpaca-prod change the spec.type field to nodeport. you can also do this when creating the ser‐ vice via kubectl expose by specifying --type=nodeport. the system will assign a new nodeport: $ kubectl describe service alpaca-prod name: alpaca-prod namespace: default labels: app=alpaca  env=prod  ver=1 annotations: <none> selector: app=alpaca,env=prod,ver=1 type: nodeport ip: 10.115.245.13 port: <unset> 8080/tcp nodeport: <unset> 32711/tcp endpoints: 10.112.1.66:8080,10.112.2.104:8080,10.112.2.105:8080 session affinity: none no events. here we see that the system assigned port 32711 to this service. now we can hit any of our cluster nodes on that port to access the service. if you are sitting on the same network, you can access it directly. if your cluster is in the cloud someplace, you can use ssh tunneling with something like this: $ ssh <node> -l 8080:localhost:32711 now if you point your browser to http://localhost:8080 you will be connected to that service. each request that you send to the service will be randomly directed to one of the pods that implements the service. reload the page a few times and you will see that you are randomly assigned to different pods. when you are done, exit out of the ssh session. 80 | chapter 7: service discovery cloud integration finally, if you have support from the cloud that you are running on (and your cluster is configured to take advantage of it), you can use the loadbalancer type. this builds on the nodeport type by additionally configuring the cloud to create a new load bal‐ ancer and direct it at nodes in your cluster. edit the alpaca-prod service again (kubectl edit service alpaca-prod) and change spec.type to loadbalancer. if you do a kubectl get services right away you’ll see that the external-ip col‐ umn for alpaca-prod now says <pending>. wait a bit and you should see a public address assigned by your cloud. you can look in the console for your cloud account and see the configuration work that kubernetes did for you: $ kubectl describe service alpaca-prod name: alpaca-prod namespace: default labels: app=alpaca  env=prod  ver=1 selector: app=alpaca,env=prod,ver=1 type: loadbalancer ip: 10.115.245.13 loadbalancer ingress: 104.196.248.204 port: <unset> 8080/tcp nodeport: <unset> 32711/tcp endpoints: 10.112.1.66:8080,10.112.2.104:8080,10.112.2.105:8080 session affinity: none events:  firstseen ... reason message  --------- ... ------ -------  3m ... type nodeport -> loadbalancer  3m ... creatingloadbalancer creating load balancer  2m ... createdloadbalancer created load balancer here we see that we have an address of 104.196.248.204 now assigned to the alpacaprod service. open up your browser and try! this example is from a cluster launched and managed on the goo‐ gle cloud platform via gke. however, the way a load balancer is configured is specific to a cloud. in addition, some clouds have dns-based load balancers (e.g., aws elb). in this case you’ll see a hostname here instead of an ip. also, depending on the cloud pro‐ vider, it may still take a little while for the load balancer to be fully operational. cloud integration | 81 creating a cloud-based load balancer can take some time. don’t be surprised if it takes a few minutes on most cloud providers. advanced details kubernetes is built to be an extensible system. as such, there are layers that allow for more advanced integrations. understanding the details of how a sophisticated con‐ cept like services is implemented may help you troubleshoot or create more advanced integrations. this section goes a bit below the surface. endpoints some applications (and the system itself) want to be able to use services without using a cluster ip. this is done with another type of object called an endpoints object. for every service object, kubernetes creates a buddy endpoints object that contains the ip addresses for that service: $ kubectl describe endpoints alpaca-prod name: alpaca-prod namespace: default labels: app=alpaca  env=prod  ver=1 subsets:  addresses: 10.112.1.54,10.112.2.84,10.112.2.85  notreadyaddresses: <none>  ports:  name port protocol  ---- ---- --------  <unset> 8080 tcp no events. to use a service, an advanced application can talk to the kubernetes api directly to look up endpoints and call them. the kubernetes api even has the capability to “watch” objects and be notified as soon as they change. in this way, a client can react immediately as soon as the ips associated with a service change. let’s demonstrate this. in a terminal window, start the following command and leave it running: $ kubectl get endpoints alpaca-prod --watch it will output the current state of the endpoint and then “hang”: name endpoints age alpaca-prod 10.112.1.54:8080,10.112.2.84:8080,10.112.2.85:8080 1m 82 | chapter 7: service discovery now open up another terminal window and delete and recreate the deployment back‐ ing alpaca-prod: $ kubectl delete deployment alpaca-prod $ kubectl run alpaca-prod \\\\  --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\  --replicas=3 \\\\  --port=8080 \\\\  --labels=\"ver=1,app=alpaca,env=prod\" if you look back at the output from the watched endpoint, you will see that as you deleted and re-created these pods, the output of the command reflected the most upto-date set of ip addresses associated with the service. your output will look some‐ thing like this: name endpoints age alpaca-prod 10.112.1.54:8080,10.112.2.84:8080,10.112.2.85:8080 1m alpaca-prod 10.112.1.54:8080,10.112.2.84:8080 1m alpaca-prod <none> 1m alpaca-prod 10.112.2.90:8080 1m alpaca-prod 10.112.1.57:8080,10.112.2.90:8080 1m alpaca-prod 10.112.0.28:8080,10.112.1.57:8080,10.112.2.90:8080 1m the endpoints object is great if you are writing new code that is built to run on kubernetes from the start. but most projects aren’t in this position! most existing sys‐ tems are built to work with regular old ip addresses that don’t change that often. manual service discovery kubernetes services are built on top of label selectors over pods. that means that you can use the kubernetes api to do rudimentary service discovery without using a ser‐ vice object at all! let’s demonstrate. with kubectl (and via the api) we can easily see what ips are assigned to each pod in our example deployments: $ kubectl get pods -o wide --show-labels name ... ip ... labels alpaca-prod-12334-87f8h ... 10.112.1.54 ... app=alpaca,env=prod,ver=1 alpaca-prod-12334-jssmh ... 10.112.2.84 ... app=alpaca,env=prod,ver=1 alpaca-prod-12334-tjp56 ... 10.112.2.85 ... app=alpaca,env=prod,ver=1 bandicoot-prod-5678-sbxzl ... 10.112.1.55 ... app=bandicoot,env=prod,ver=2 bandicoot-prod-5678-x0dh8 ... 10.112.2.86 ... app=bandicoot,env=prod,ver=2 this is great, but what if you have a ton of pods? you’ll probably want to filter this based on the labels applied as part of the deployment. let’s do that for just the alpaca app: advanced details | 83 $ kubectl get pods -o wide --selector=app=alpaca,env=prod name ... ip ... alpaca-prod-3408831585-bpzdz ... 10.112.1.54 ... alpaca-prod-3408831585-kncwt ... 10.112.2.84 ... alpaca-prod-3408831585-l9fsq ... 10.112.2.85 ... at this point you have the basics of service discovery! you can always use labels to identify the set of pods you are interested in, get all of the pods for those labels, and dig out the ip address. but keeping the correct set of labels to use in sync can be tricky. this is why the service object was created. kube-proxy and cluster ips cluster ips are stable virtual ips that load-balance traffic across all of the endpoints in a service. this magic is performed by a component running on every node in the cluster called the kube-proxy (figure 7-1). figure 7-1. configuring and using a cluster ip in figure 7-1, the kube-proxy watches for new services in the cluster via the api server. it then programs a set of iptables rules in the kernel of that host to rewrite the destinations of packets so they are directed at one of the endpoints for that ser‐ vice. if the set of endpoints for a service changes (due to pods coming and going or due to a failed readiness check), the set of iptables rules is rewritten. the cluster ip itself is usually assigned by the api server as the service is created. however, when creating the service, the user can specify a specific cluster ip. once set, the cluster ip cannot be modified without deleting and recreating the service object. 84 | chapter 7: service discovery the kubernetes service address range is configured using the --service-cluster-ip-range flag on the kube-apiserver binary. the service address range should not overlap with the ip subnets and ranges assigned to each docker bridge or kubernetes node. in addition, any explicit cluster ip requested must come from that range and not already be in use. cluster ip environment variables while most users should be using the dns services to find cluster ips, there are some older mechanisms that may still be in use. one of these is injecting a set of environ‐ ment variables into pods as they start up. to see this in action, let’s look at the console for the bandicoot instance of kuard. enter the following commands in your terminal: $ bandicootpod=$(kubectl get pods -l app=bandicoot \\\\  -o jsonpath=\\'{.items.metadata.name}\\') $ kubectl port-forward $bandicootpod 48858:8080 now point your browser to http://localhost:48858 to see the status page for this server. expand the “server env” section and note the set of environment variables for the alpaca service. the status page should show a table similar to table 7-1. table 7-1. service environment variables key value alpacaprodport tcp://10.115.245.13:8080 alpacaprodport8080tcp tcp://10.115.245.13:8080 alpacaprodport8080tcpaddr 10.115.245.13 alpacaprodport8080tcpport 8080 alpacaprodport8080tcpproto tcp alpacaprodservicehost 10.115.245.13 alpacaprodserviceport 8080 the two main environment variables to use are alpacaprodservicehost and alpacaprodserviceport. the other environment variables are created to be com‐ patible with (now deprecated) docker link variables. a problem with the environment variable approach is that it requires resources to be created in a specific order. the services must be created before the pods that reference them. this can introduce quite a bit of complexity when deploying a set of services that make up a larger application. in addition, using just environment variables seems strange to many users. for this reason, dns is probably a better option. advanced details | 85 connecting with other environments while it is great to have service discovery within your own cluster, many real-world applications actually require that you integrate more cloud-native applications deployed in kubernetes with applications deployed to more legacy environments. additionally, you may need to integrate a kubernetes cluster in the cloud with infra‐ structure that has been deployed on-premise. this is an area of kubernetes that is still undergoing a fair amount of exploration and development of solutions. when you are connecting kubernetes to legacy resources outside of the cluster, you can use selector-less services to declare a kubernetes ser‐ vice with a manually assigned ip address that is outside of the cluster. that way, kubernetes service discovery via dns works as expected, but the network traffic itself flows to an external resource. connecting external resources to kubernetes services is somewhat trickier. if your cloud provider supports it, the easiest thing to do is to create an “internal” load bal‐ ancer that lives in your virtual private network and can deliver traffic from a fixed ip address into the cluster. you can then use traditional dns to make this ip address available to the external resource. another option is to run the full kube-proxy on an external resource and program that machine to use the dns server in the kubernetes cluster. such a setup is significantly more difficult to get right and should really only be used in on-premise environments. there are also a variety of open source projects (for example, hashicorp’s consul) that can be used to manage connectivity between in-cluster and out-of-cluster resources. cleanup run the following command to clean up all of the objects created in this chapter: $ kubectl delete services,deployments -l app summary kubernetes is a dynamic system that challenges traditional methods of naming and connecting services over the network. the service object provides a flexible and pow‐ erful way to expose services both within the cluster and beyond. with the techniques covered here you can connect services to each other and expose them outside the cluster. 86 | chapter 7: service discovery while using the dynamic service discovery mechanisms in kubernetes introduces some new concepts and may, at first, seem complex, understanding and adapting these techniques is key to unlocking the power of kubernetes. once your application can dynamically find services and react to the dynamic placement of those applica‐ tions, you are free to stop worrying about where things are running and when they move. it is a critical piece of the puzzle to start to think about services in a logical way and let kubernetes take care of the details of container placement. summary | 87  1 the open systems interconnection (osi) model is a standard way to describe how different networking lay‐ ers build on each other. tcp and udp are considered to be layer 4, while http is layer 7. chapter 8 http load balancing with ingress a critical part of any application is getting network traffic to and from that applica‐ tion. as described in chapter 7, kubernetes has a set of capabilities to enable services to be exposed outside of the cluster. for many users and simple use cases these capa‐ bilities are sufficient. but the service object operates at layer 4 (according to the osi model1 ). this means that it only forwards tcp and udp connections and doesn’t look inside of those con‐ nections. because of this, hosting many applications on a cluster uses many different exposed services. in the case where these services are type: nodeport, you’ll have to have clients connect to a unique port per service. in the case where these services are type: loadbalancer, you’ll be allocating (often expensive or scarce) cloud resources for each service. but for http (layer 7)-based services, we can do better. when solving a similar problem in non-kubernetes situations, users often turn to the idea of “virtual hosting.” this is a mechanism to host many http sites on a single ip address. typically, the user uses a load balancer or reverse proxy to accept incoming connections on http (80) and https (443) ports. that program then parses the http connection and, based on the host header and the url path that is requested, proxies the http call to some other program. in this way, that load balancer or reverse proxy plays “traffic cop” for decoding and directing incoming connections to the right “upstream” server. kubernetes calls its http-based load-balancing system ingress. ingress is a kubernetes-native way to implement the “virtual hosting” pattern we just discussed. one of the more complex aspects of the pattern is that the user has to manage the 89 load balancer configuration file. in a dynamic environment and as the set of virtual hosts expands, this can be very complex. the kubernetes ingress system works to simplify this by (a) standardizing that configuration, (b) moving it to a standard kubernetes object, and (c) merging multiple ingress objects into a single config for the load balancer. the typical software base implementation looks something like what is depicted in figure 8-1. the ingress controller is a software system exposed outside the cluster using a service of type: loadbalancer. it then proxies requests to “upstream” servers. the configuration for how it does this is the result of reading and monitoring ingress objects. figure 8-1. the typical software ingress controller configuration ingress spec versus ingress controllers while conceptually simple, at an implementation level ingress is very different from pretty much every other regular resource object in kubernetes. specifically, it is split into a common resource specification and a controller implementation. there is no “standard” ingress controller that is built into kubernetes, so the user must install one of many optional implementations. users can create and modify ingress objects just like every other object. but, by default, there is no code running to actually act on those objects. it is up to the users (or the distribution they are using) to install and manage an outside controller. in this way, the controller is pluggable. there are multiple reasons that ingress ended up like this. first of all, there is no one single http load balancer that can universally be used. in addition to many software load balancers (both open source and proprietary), there are also load-balancing capabilities provided by cloud providers (e.g., elb on aws), and hardware-based load balancers. the second reason is that the ingress object was added to kubernetes before any of the common extensibility capabilities were added (see chapter 16). as ingress progresses, it is likely that it will evolve to use these mechanisms. 90 | chapter 8: http load balancing with ingress 2 heptio was recently acquired by vmware, so there is a chance that this url could change. however, github will forward to the new destination. installing contour while there are many available ingress controllers, for the examples here we use an ingress controller called contour. this is a controller built to configure the open source (and cncf project) load balancer called envoy. envoy is built to be dynami‐ cally configured via an api. the contour ingress controller takes care of translating the ingress objects into something that envoy can understand. the contour project is hosted at https://github.com/heptio/contour. it was created by heptio2  in collaboration with real-world custom‐ ers and is used in production settings. you can install contour with a simple one-line invocation: $ kubectl apply -f https://j.hept.io/contour-deployment-rbac note that this requires execution by a user who has cluster-admin permissions. this one line works for most configurations. it creates a namespace called heptiocontour. inside of that namespace it creates a deployment (with two replicas) and an external-facing service of type: loadbalancer. in addition, it sets up the correct per‐ missions via a service account and installs a customresourcedefinition (see chap‐ ter 16) for some extended capabilities discussed in “the future of ingress” on page 100. because it is a global install, you need to ensure that you have wide admin permis‐ sions on the cluster you are installing into. after you install it, you can fetch the external address of contour via: $ kubectl get -n heptio-contour service contour -o wide name cluster-ip external-ip port(s) ... contour 10.106.53.14 a477...amazonaws.com 80:30274/tcp ... look at the external-ip column. this can be either an ip address (for gcp and azure) or a hostname (for aws). other clouds and environments may differ. if your kubernetes cluster doesn’t support services of type: loadbalancer, you’ll have to change the yaml for installing contour to simply use type: nodeport and route traffic to machines on the cluster via a mechanism that works in your configuration. if you are using minikube, you probably won’t have anything listed for external-ip. to fix this, you need to open a separate terminal window and run minikube tunnel. installing contour | 91 this configures networking routes such that you have unique ip addresses assigned to every service of type: loadbalancer. con\\x80guring dns to make ingress work well, you need to configure dns entries to the external address for your load balancer. you can map multiple hostnames to a single external endpoint and the ingress controller will play traffic cop and direct incoming requests to the appropriate upstream service based on that hostname. for this chapter, we assume that you have a domain called example.com. you need to configure two dns entries: alpaca.example.com and bandicoot.example.com. if you have an ip address for your external load balancer, you’ll want to create a records. if you have a hostname, you’ll want to configure cname records. con\\x80guring a local hosts file if you don’t have a domain or if you are using a local solution such as minikube, you can set up a local configuration by editing your /etc/hosts file to add an ip address. you need admin/root privileges on your workstation. the location of the file may dif‐ fer on your platform, and making it take effect may require extra steps. for example, on windows the file is usually at c:\\\\windows\\\\system32\\\\drivers\\\\etc\\\\hosts, and for recent versions of macos you need to run sudo killall -hup mdnsresponder after changing the file. edit the file to add a line like the following: <ip-address> alpaca.example.com bandicoot.example.com for <ip-address>, fill in the external ip address for contour. if all you have is a host‐ name (like from aws), you can get an ip address (that may change in the future) by executing host -t a <address>. don’t forget to undo these changes when you are done! using ingress now that we have an ingress controller configured, let’s put it through its paces. first we’ll create a few upstream (also sometimes referred to as “backend”) services to play with by executing the following commands: $ kubectl run be-default \\\\  --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\  --replicas=3 \\\\  --port=8080 $ kubectl expose deployment be-default $ kubectl run alpaca \\\\ 92 | chapter 8: http load balancing with ingress  --image=gcr.io/kuar-demo/kuard-amd64:green \\\\  --replicas=3 \\\\  --port=8080 $ kubectl expose deployment alpaca $ kubectl run bandicoot \\\\  --image=gcr.io/kuar-demo/kuard-amd64:purple \\\\  --replicas=3 \\\\  --port=8080 $ kubectl expose deployment bandicoot $ kubectl get services -o wide name cluster-ip ... port(s) ... selector alpaca-prod 10.115.245.13 ... 8080/tcp ... run=alpaca bandicoot-prod 10.115.242.3 ... 8080/tcp ... run=bandicoot be-default 10.115.246.6 ... 8080/tcp ... run=be-default kubernetes 10.115.240.1 ... 443/tcp ... <none> simplest usage the simplest way to use ingress is to have it just blindly pass everything that it sees through to an upstream service. there is limited support for imperative commands to work with ingress in kubectl, so we’ll start with a yaml file (see example 8-1). example 8-1. simple-ingress.yaml apiversion: extensions/v1beta1 kind: ingress metadata:  name: simple-ingress spec:  backend:  servicename: alpaca  serviceport: 8080 create this ingress with kubectl apply: $ kubectl apply -f simple-ingress.yaml ingress.extensions/simple-ingress created you can verify that it was set up correctly using kubectl get and kubectl describe: $ kubectl get ingress name hosts address ports age simple-ingress * 80 13m $ kubectl describe ingress simple-ingress name: simple-ingress namespace: default address: default backend: be-default:8080 (172.17.0.6:8080,172.17.0.7:8080,172.17.0.8:8080) using ingress | 93 rules:  host path backends  ---- ---- --------  * * be-default:8080 (172.17.0.6:8080,172.17.0.7:8080,172.17.0.8:8080) annotations:  ... events: <none> this sets things up so that any http request that hits the ingress controller is for‐ warded on to the alpaca service. you can now access the alpaca instance of kuard on any of the raw ips/cnames of the service; in this case, either alpaca.example.com or bandicoot.example.com. this doesn’t, at this point, add much value above a simple service of type: loadba lancer. we experiment with more complex configurations in the following sections. using hostnames things start to get interesting when we start to direct traffic based on properties of the request. the most common example of this is to have the ingress system look at the http host header (which is set to the dns domain in the original url) and direct traffic based on that header. let’s add another ingress object for directing traffic to the alpaca service for any traffic directed to alpaca.example.com (see example 8-2). example 8-2. host-ingress.yaml apiversion: extensions/v1beta1 kind: ingress metadata:  name: host-ingress spec:  rules:  - host: alpaca.example.com  http:  paths:  - backend:  servicename: alpaca  serviceport: 8080 94 | chapter 8: http load balancing with ingress create this ingress with kubectl apply: $ kubectl apply -f host-ingress.yaml ingress.extensions/host-ingress created we can verify that things are set up correctly as follows: $ kubectl get ingress name hosts address ports age host-ingress alpaca.example.com 80 54s simple-ingress * 80 13m $ kubectl describe ingress host-ingress name: host-ingress namespace: default address: default backend: default-http-backend:80 (<none>) rules:  host path backends  ---- ---- --------  alpaca.example.com  / alpaca:8080 (<none>) annotations:  ... events: <none> there are a couple of things that are a bit confusing here. first, there is a reference to the default-http-backend. this is a convention that only some ingress controllers use to handle requests that aren’t handled in any other way. these controllers send those requests to a service called default-http-backend in the kube-system name‐ space. this convention is surfaced client-side in kubectl. next, there are no endpoints listed for the alpaca backend service. this is a bug in kubectl that is fixed in kubernetes v1.14. regardless, you should now be able to address the alpaca service via http:// alpaca.example.com. if instead you reach the service endpoint via other methods, you should get the default service. using paths the next interesting scenario is to direct traffic based on not just the hostname, but also the path in the http request. we can do this easily by specifying a path in the paths entry (see example 8-3). in this example we direct everything coming into http://bandicoot.example.com to the bandicoot service, but we also send http://bandi‐ coot.example.com/a to the alpaca service. this type of scenario can be used to host multiple services on different paths of a single domain. using ingress | 95 example 8-3. path-ingress.yaml apiversion: extensions/v1beta1 kind: ingress metadata:  name: path-ingress spec:  rules:  - host: bandicoot.example.com  http:  paths:  - path: \"/\"  backend:  servicename: bandicoot  serviceport: 8080  - path: \"/a/\"  backend:  servicename: alpaca  serviceport: 8080 when there are multiple paths on the same host listed in the ingress system, the longest prefix matches. so, in this example, traffic starting with /a/ is forwarded to the alpaca service, while all other traffic (starting with /) is directed to the bandicoot service. as requests get proxied to the upstream service, the path remains unmodified. that means a request to bandicoot.example.com/a/ shows up to the upstream server that is configured for that request hostname and path. the upstream service needs to be ready to serve traffic on that subpath. in this case, kuard has special code for testing, where it responds on the root path (/) along with a predefined set of subpaths (/ a/, /b/, and /c/). cleaning up to clean up, execute the following: $ kubectl delete ingress host-ingress path-ingress simple-ingress $ kubectl delete service alpaca bandicoot be-default $ kubectl delete deployment alpaca bandicoot be-default advanced ingress topics and gotchas there are some other fancy features that are supported by ingress. the level of sup‐ port for these features differs based on the ingress controller implementation, and two controllers may implement a feature in slightly different ways. many of the extended features are exposed via annotations on the ingress object. be careful, as these annotations can be hard to validate and are easy to get wrong. many 96 | chapter 8: http load balancing with ingress of these annotations apply to the entire ingress object and so can be more general than you might like. to scope the annotations down you can always split a single ingress object into multiple ingress objects. the ingress controller should read them and merge them together. running multiple ingress controllers oftentimes, you may want to run multiple ingress controllers on a single cluster. in that case, you specify which ingress object is meant for which ingress controller using the kubernetes.io/ingress.class annotation. the value should be a string that specifies which ingress controller should look at this object. the ingress controllers themselves, then, should be configured with that same string and should only respect those ingress objects with the correct annotation. if the kubernetes.io/ingress.class annotation is missing, behavior is undefined. it is likely that multiple controllers will fight to satisfy the ingress and write the status field of the ingress objects. multiple ingress objects if you specify multiple ingress objects, the ingress controllers should read them all and try to merge them into a coherent configuration. however, if you specify dupli‐ cate and conflicting configurations, the behavior is undefined. it is likely that differ‐ ent ingress controllers will behave differently. even a single implementation may do different things depending on nonobvious factors. ingress and namespaces ingress interacts with namespaces in some nonobvious ways. first, due to an abundance of security caution, an ingress object can only refer to an upstream service in the same namespace. this means that you can’t use an ingress object to point a subpath to a service in another namespace. however, multiple ingress objects in different namespaces can specify subpaths for the same host. these ingress objects are then merged together to come up with the final config for the ingress controller. this cross-namespace behavior means that it is necessary that ingress be coordinated globally across the cluster. if not coordinated carefully, an ingress object in one namespace could cause problems (and undefined behavior) in other namespaces. typically there are no restrictions built into the ingress controller around what name‐ spaces are allowed to specify what hostnames and paths. advanced users may try to enforce a policy for this using a custom admission controller. there are also advanced ingress topics and gotchas | 97 evolutions of ingress described in “the future of ingress” on page 100 that address this problem. path rewriting some ingress controller implementations support, optionally, doing path rewriting. this can be used to modify the path in the http request as it gets proxied. this is usually specified by an annotation on the ingress object and applies to all requests that are specified by that object. for example, if we were using the nginx ingress controller, we could specify an annotation of nginx.ingress.kubernetes.io/ rewrite-target: /. this can sometimes make upstream services work on a subpath even if they weren’t built to do so. there are multiple implementations that not only implement path rewriting, but also support regular expressions when specifying the path. for example, the nginx con‐ troller allows regular expressions to capture parts of the path and then use that cap‐ tured content when doing rewriting. how this is done (and what variant of regular expressions is used) is implementation-specific. path rewriting isn’t a silver bullet, though, and can often lead to bugs. many web applications assume that they can link within themselves using absolute paths. in that case, the app in question may be hosted on /subpath but have requests show up to it on /. it may then send a user to /app-path. there is then the question of whether that is an “internal” link for the app (in which case it should instead be /subpath/apppath) or a link to some other app. for this reason, it is probably best to avoid sub‐ paths if you can help it for any complicated applications. serving tls when serving websites, it is becoming increasingly necessary to do so securely using tls and https. ingress supports this (as do most ingress controllers). first, users need to specify a secret with their tls certificate and keys—something like what is outlined in example 8-4. you can also create a secret imperatively with kubectl create secret tls <secret-name> --cert <certificate-pem-file> -- key <private-key-pem-file>. example 8-4. tls-secret.yaml apiversion: v1 kind: secret metadata:  creationtimestamp: null  name: tls-secret-name type: kubernetes.io/tls data: 98 | chapter 8: http load balancing with ingress  tls.crt: <base64 encoded certificate>  tls.key: <base64 encoded private key> once you have the certificate uploaded, you can reference it in an ingress object. this specifies a list of certificates along with the hostnames that those certificates should be used for (see example 8-5). again, if multiple ingress objects specify certificates for the same hostname, the behavior is undefined. example 8-5. tls-ingress.yaml apiversion: extensions/v1beta1 kind: ingress metadata:  name: tls-ingress spec:  tls:  - hosts:  - alpaca.example.com  secretname: tls-secret-name  rules:  - host: alpaca.example.com  http:  paths:  - backend:  servicename: alpaca  serviceport: 8080 uploading and managing tls secrets can be difficult. in addition, certificates can often come at a significant cost. to help solve this problem, there is a non-profit called “let’s encrypt” running a free certificate authority that is api-driven. since it is api-driven, it is possible to set up a kubernetes cluster that automatically fetches and installs tls certificates for you. it can be tricky to set up, but when working it’s very simple to use. the missing piece is an open source project called cert-manager initiated and supported by jetstack, a uk startup. visit its github page for instruc‐ tions on installing and using cert-manager. alternate ingress implementations there are many different implementations of ingress controllers, each building on the base ingress object with unique features. it is a vibrant ecosystem. first, each cloud provider has an ingress implementation that exposes the specific cloud-based l7 load balancer for that cloud. instead of configuring a software load balancer running in a pod, these controllers take ingress objects and use them to con‐ figure, via an api, the cloud-based load balancers. this reduces the load on the clus‐ ter and management burden for the operators, but can often come at a cost. alternate ingress implementations | 99 the most popular generic ingress controller is probably the open source nginx ingress controller. be aware that there is also a commercial controller based on the proprietary nginx plus. the open source controller essentially reads ingress objects and merges them into an nginx configuration file. it then signals to the nginx process to restart with the new configuration (while responsibly serving existing inflight connections). the open nginx controller has an enormous number of fea‐ tures and options exposed via annotations. ambassador and gloo are two other envoy-based ingress controllers that are focused on being api gateways. traefik is a reverse proxy implemented in go that also can function as an ingress controller. it has a set of features and dashboards that are very developer-friendly. this just scratches the surface. the ingress ecosystem is very active and there are many new projects and commercial offerings that build on the humble ingress object in unique ways. the future of ingress as you have seen, the ingress object provides a very useful abstraction for configur‐ ing l7 load balancers—but it hasn’t scaled to all the features that users want and vari‐ ous implementations are looking to offer. many of the features in ingress are underdefined. implementations can surface these features in different ways, reducing the portability of configurations between imple‐ mentations. another problem is that it is easy to misconfigure ingress. the way that multiple objects compose together opens the door for conflicts that are resolved differently by different implementations. in addition, the way that these are merged across name‐ spaces breaks the idea of namespace isolation. ingress was also created before the idea of a service mesh (exemplified by projects such as istio and linkerd) was well known. the intersection of ingress and service meshes is still being defined. there are a lot of great ideas floating around the community. for example, istio implements the idea of a gateway that overlaps with ingress in some ways. contour introduces a new type called ingressroute that is a better-defined and more explicit version of ingress inspired by other network protocols like dns. defining the next step for ingress is a hard problem. it involves creating a common language that applies to most load balancers, while at the same time leaving room for innovation of new features and allowing different implementations to specialize in other directions. this is an active area of innovation in the kubernetes community through the net‐ work special interest group. 100 | chapter 8: http load balancing with ingress summary ingress is a unique system in kubernetes. it is simply a schema, and the implementa‐ tions of a controller for that schema must be installed and managed separately. but it is also a critical system for exposing services to users in a practical and cost-efficient way. as kubernetes continues to mature, expect to see ingress become more and more relevant. summary | 101  chapter 9 replicasets previously, we covered how to run individual containers as pods. but these pods are essentially one-off singletons. more often than not, you want multiple replicas of a container running at a particular time. there are a variety of reasons for this type of replication: redundancy multiple running instances mean failure can be tolerated. scale multiple running instances mean that more requests can be handled. sharding different replicas can handle different parts of a computation in parallel. of course, you could manually create multiple copies of a pod using multiple differ‐ ent (though largely similar) pod manifests, but doing so is both tedious and errorprone. logically, a user managing a replicated set of pods considers them as a single entity to be defined and managed. this is precisely what a replicaset is. a replicaset acts as a cluster-wide pod manager, ensuring that the right types and number of pods are running at all times. because replicasets make it easy to create and manage replicated sets of pods, they are the building blocks used to describe common application deployment patterns and provide the underpinnings of self-healing for our applications at the infrastruc‐ ture level. pods managed by replicasets are automatically rescheduled under certain failure conditions, such as node failures and network partitions. the easiest way to think of a replicaset is that it combines a cookie cutter and a desired number of cookies into a single api object. when we define a replicaset, we define a specification for the pods we want to create (the “cookie cutter”), and a 103 desired number of replicas. additionally, we need to define a way of finding pods that the replicaset should control. the actual act of managing the replicated pods is an example of a reconciliation loop. such loops are fundamental to most of the design and implementation of kubernetes. the decision to embed the definition of a pod inside a replicaset (and a deployment, and a job, and…) is one of the more interesting ones in kubernetes. in retrospect, it probably would have been a better decision to use a reference to the podtemplate object rather than embedding it directly. reconciliation loops the central concept behind a reconciliation loop is the notion of desired state versus observed or current state. desired state is the state you want. with a replicaset, it is the desired number of replicas and the definition of the pod to replicate. for example, “the desired state is that there are three replicas of a pod running the kuard server.” in contrast, the current state is the currently observed state of the system. for exam‐ ple, “there are only two kuard pods currently running.” the reconciliation loop is constantly running, observing the current state of the world and taking action to try to make the observed state match the desired state. for instance, with the previous examples, the reconciliation loop would create a new kuard pod in an effort to make the observed state match the desired state of three replicas. there are many benefits to the reconciliation loop approach to managing state. it is an inherently goal-driven, self-healing system, yet it can often be easily expressed in a few lines of code. as a concrete example of this, note that the reconciliation loop for replicasets is a single loop, yet it handles user actions to scale up or scale down the replicaset as well as node failures or nodes rejoining the cluster after being absent. we’ll see numerous examples of reconciliation loops in action throughout the rest of the book. relating pods and replicasets one of the key themes that runs through kubernetes is decoupling. in particular, it’s important that all of the core concepts of kubernetes are modular with respect to each other and that they are swappable and replaceable with other components. in this spirit, the relationship between replicasets and pods is loosely coupled. though replicasets create and manage pods, they do not own the pods they create. replica‐ 104 | chapter 9: replicasets sets use label queries to identify the set of pods they should be managing. they then use the exact same pod api that you used directly in chapter 5 to create the pods that they are managing. this notion of “coming in the front door” is another central design concept in kubernetes. in a similar decoupling, replicasets that create multi‐ ple pods and the services that load-balance to those pods are also totally separate, decoupled api objects. in addition to supporting modularity, the decoupling of pods and replicasets enables several important behaviors, discussed in the following sections. adopting existing containers despite the value placed on declarative configuration of software, there are times when it is easier to build something up imperatively. in particular, early on you may be simply deploying a single pod with a container image without a replicaset manag‐ ing it. but at some point you may want to expand your singleton container into a replicated service and create and manage an array of similar containers. you may have even defined a load balancer that is serving traffic to that single pod. if replica‐ sets owned the pods they created, then the only way to start replicating your pod would be to delete it and then relaunch it via a replicaset. this might be disruptive, as there would be a moment in time when there would be no copies of your container running. however, because replicasets are decoupled from the pods they manage, you can simply create a replicaset that will “adopt” the existing pod, and scale out additional copies of those containers. in this way, you can seamlessly move from a single imperative pod to a replicated set of pods managed by a replicaset. quarantining containers oftentimes, when a server misbehaves, pod-level health checks will automatically restart that pod. but if your health checks are incomplete, a pod can be misbehaving but still be part of the replicated set. in these situations, while it would work to simply kill the pod, that would leave your developers with only logs to debug the problem. instead, you can modify the set of labels on the sick pod. doing so will disassociate it from the replicaset (and service) so that you can debug the pod. the replicaset con‐ troller will notice that a pod is missing and create a new copy, but because the pod is still running it is available to developers for interactive debugging, which is signifi‐ cantly more valuable than debugging from logs. designing with replicasets replicasets are designed to represent a single, scalable microservice inside your architecture. the key characteristic of replicasets is that every pod that is created by the replicaset controller is entirely homogeneous. typically, these pods are then fronted by a kubernetes service load balancer, which spreads traffic across the pods designing with replicasets | 105 that make up the service. generally speaking, replicasets are designed for stateless (or nearly stateless) services. the elements created by the replicaset are interchange‐ able; when a replicaset is scaled down, an arbitrary pod is selected for deletion. your application’s behavior shouldn’t change because of such a scale-down operation. replicaset spec like all objects in kubernetes, replicasets are defined using a specification. all replicasets must have a unique name (defined using the metadata.name field), a spec section that describes the number of pods (replicas) that should be running clusterwide at any given time, and a pod template that describes the pod to be created when the defined number of replicas is not met. example 9-1 shows a minimal replicaset definition. example 9-1. kuard-rs.yaml apiversion: extensions/v1beta1 kind: replicaset metadata:  name: kuard spec:  replicas: 1  template:  metadata:  labels:  app: kuard  version: \"2\"  spec:  containers:  - name: kuard  image: \"gcr.io/kuar-demo/kuard-amd64:green\" pod templates as mentioned previously, when the number of pods in the current state is less than the number of pods in the desired state, the replicaset controller will create new pods. the pods are created using a pod template that is contained in the replicaset specification. the pods are created in exactly the same manner as when you created a pod from a yaml file in previous chapters, but instead of using a file, the kubernetes replicaset controller creates and submits a pod manifest based on the pod template directly to the api server. the following shows an example of a pod template in a replicaset: template:  metadata:  labels: 106 | chapter 9: replicasets  app: helloworld  version: v1  spec:  containers:  - name: helloworld  image: kelseyhightower/helloworld:v1  ports:  - containerport: 80 labels in any cluster of reasonable size, there are many different pods running at any given time—so how does the replicaset reconciliation loop discover the set of pods for a particular replicaset? replicasets monitor cluster state using a set of pod labels. labels are used to filter pod listings and track pods running within a cluster. when initially created, a replicaset fetches a pod listing from the kubernetes api and filters the results by labels. based on the number of pods returned by the query, the replica‐ set deletes or creates pods to meet the desired number of replicas. the labels used for filtering are defined in the replicaset spec section and are the key to understanding how replicasets work. the selector in the replicaset spec should be a proper subset of the labels in the pod template. creating a replicaset replicasets are created by submitting a replicaset object to the kubernetes api. in this section we will create a replicaset using a configuration file and the kubectl apply command. the replicaset configuration file in example 9-1 will ensure one copy of the gcr.io/ kuar-demo/kuard-amd64:green container is running at any given time. use the kubectl apply command to submit the kuard replicaset to the kubernetes api: $ kubectl apply -f kuard-rs.yaml replicaset \"kuard\" created once the kuard replicaset has been accepted, the replicaset controller will detect that there are no kuard pods running that match the desired state, and a new kuard pod will be created based on the contents of the pod template: creating a replicaset | 107 $ kubectl get pods name ready status restarts age kuard-yvzgd 1/1 running 0 11s inspecting a replicaset as with pods and other kubernetes api objects, if you are interested in further details about a replicaset, the describe command will provide much more information about its state. here is an example of using describe to obtain the details of the replicaset we previously created: $ kubectl describe rs kuard name: kuard namespace: default image(s): kuard:1.9.15 selector: app=kuard,version=2 labels: app=kuard,version=2 replicas: 1 current / 1 desired pods status: 1 running / 0 waiting / 0 succeeded / 0 failed no volumes. you can see the label selector for the replicaset, as well as the state of all of the repli‐ cas managed by the replicaset. finding a replicaset from a pod sometimes you may wonder if a pod is being managed by a replicaset, and if it is, which replicaset. to enable this kind of discovery, the replicaset controller adds an annotation to every pod that it creates. the key for the annotation is kubernetes.io/created-by. if you run the following, look for the kubernetes.io/created-by entry in the annota‐ tions section: $ kubectl get pods <pod-name> -o yaml if applicable, this will list the name of the replicaset that is managing this pod. note that such annotations are best-effort; they are only created when the pod is created by the replicaset, and can be removed by a kubernetes user at any time. finding a set of pods for a replicaset you can also determine the set of pods managed by a replicaset. first, you can get the set of labels using the kubectl describe command. in the previous example, the label selector was app=kuard,version=2. to find the pods that match this selector, use the --selector flag or the shorthand -l: $ kubectl get pods -l app=kuard,version=2 108 | chapter 9: replicasets this is exactly the same query that the replicaset executes to determine the current number of pods. scaling replicasets replicasets are scaled up or down by updating the spec.replicas key on the replicaset object stored in kubernetes. when a replicaset is scaled up, new pods are submitted to the kubernetes api using the pod template defined on the replicaset. imperative scaling with kubectl scale the easiest way to achieve this is using the scale command in kubectl. for example, to scale up to four replicas you could run: $ kubectl scale replicasets kuard --replicas=4 while such imperative commands are useful for demonstrations and quick reactions to emergency situations (e.g., in response to a sudden increase in load), it is impor‐ tant to also update any text-file configurations to match the number of replicas that you set via the imperative scale command. the reason for this becomes obvious when you consider the following scenario: alice is on call, when suddenly there is a large increase in load on the service she is managing. alice uses the scale command to increase the number of servers respond‐ ing to requests to 10, and the situation is resolved. however, alice forgets to update the replicaset configurations checked into source control. several days later, bob is pre‐ paring the weekly rollouts. bob edits the replicaset configurations stored in version control to use the new container image, but he doesn’t notice that the number of repli‐ cas in the file is currently 5, not the 10 that alice set in response to the increased load. bob proceeds with the rollout, which both updates the container image and reduces the number of replicas by half, causing an immediate overload or outage. hopefully, this illustrates the need to ensure that any imperative changes are immedi‐ ately followed by a declarative change in source control. indeed, if the need is not acute, we generally recommend only making declarative changes as described in the following section. declaratively scaling with kubectl apply in a declarative world, we make changes by editing the configuration file in version control and then applying those changes to our cluster. to scale the kuard replicaset, edit the kuard-rs.yaml configuration file and set the replicas count to 3: ... spec:  replicas: 3 ... scaling replicasets | 109 in a multiuser setting, you would like to have a documented code review of this change and eventually check the changes into version control. either way, you can then use the kubectl apply command to submit the updated kuard replicaset to the api server: $ kubectl apply -f kuard-rs.yaml replicaset \"kuard\" configured now that the updated kuard replicaset is in place, the replicaset controller will detect that the number of desired pods has changed and that it needs to take action to realize that desired state. if you used the imperative scale command in the previous section, the replicaset controller will destroy one pod to get the number to three. otherwise, it will submit two new pods to the kubernetes api using the pod template defined on the kuard replicaset. regardless, use the kubectl get pods command to list the running kuard pods. you should see output like the following: $ kubectl get pods name ready status restarts age kuard-3a2sb 1/1 running 0 26s kuard-wuq9v 1/1 running 0 26s kuard-yvzgd 1/1 running 0 2m autoscaling a replicaset while there will be times when you want to have explicit control over the number of replicas in a replicaset, often you simply want to have “enough” replicas. the defini‐ tion varies depending on the needs of the containers in the replicaset. for example, with a web server like nginx, you may want to scale due to cpu usage. for an inmemory cache, you may want to scale with memory consumption. in some cases you may want to scale in response to custom application metrics. kubernetes can handle all of these scenarios via horizontal pod autoscaling (hpa). hpa requires the presence of the heapster pod on your cluster. heapster keeps track of metrics and provides an api for consum‐ ing metrics that hpa uses when making scaling decisions. most installations of kubernetes include heapster by default. you can validate its presence by listing the pods in the kube-system name‐ space: $ kubectl get pods --namespace=kube-system you should see a pod named heapster somewhere in that list. if you do not see it, autoscaling will not work correctly. “horizontal pod autoscaling” is kind of a mouthful, and you might wonder why it is not simply called “autoscaling.” kubernetes makes a distinction between horizontal scaling, which involves creating additional replicas of a pod, and vertical scaling, 110 | chapter 9: replicasets which involves increasing the resources required for a particular pod (e.g., increasing the cpu required for the pod). vertical scaling is not currently implemented in kubernetes, but it is planned. additionally, many solutions also enable cluster autoscaling, where the number of machines in the cluster is scaled in response to resource needs, but this solution is not covered here. autoscaling based on cpu scaling based on cpu usage is the most common use case for pod autoscaling. gen‐ erally it is most useful for request-based systems that consume cpu proportionally to the number of requests they are receiving, while using a relatively static amount of memory. to scale a replicaset, you can run a command like the following: $ kubectl autoscale rs kuard --min=2 --max=5 --cpu-percent=80 this command creates an autoscaler that scales between two and five replicas with a cpu threshold of 80%. to view, modify, or delete this resource you can use the stan‐ dard kubectl commands and the horizontalpodautoscalers resource. horizontal podautoscalers is quite a bit to type, but it can be shortened to hpa: $ kubectl get hpa because of the decoupled nature of kubernetes, there is no direct link between the hpa and the replicaset. while this is great for modularity and composition, it also enables some anti-patterns. in particular, it’s a bad idea to combine both autoscaling and impera‐ tive or declarative management of the number of replicas. if both you and an autoscaler are attempting to modify the number of rep‐ licas, it’s highly likely that you will clash, resulting in unexpected behavior. deleting replicasets when a replicaset is no longer required it can be deleted using the kubectl delete command. by default, this also deletes the pods that are managed by the replicaset: $ kubectl delete rs kuard replicaset \"kuard\" deleted running the kubectl get pods command shows that all the kuard pods created by the kuard replicaset have also been deleted: $ kubectl get pods deleting replicasets | 111 if you don’t want to delete the pods that are being managed by the replicaset, you can set the --cascade flag to false to ensure only the replicaset object is deleted and not the pods: $ kubectl delete rs kuard --cascade=false summary composing pods with replicasets provides the foundation for building robust appli‐ cations with automatic failover, and makes deploying those applications a breeze by enabling scalable and sane deployment patterns. replicasets should be used for any pod you care about, even if it is a single pod! some people even default to using rep‐ licasets instead of pods. a typical cluster will have many replicasets, so apply liber‐ ally to the affected area. 112 | chapter 9: replicasets chapter 10 deployments so far, you have seen how to package your applications as containers, create replica‐ ted sets of containers, and use ingress controllers to load-balance traffic to your serv‐ ices. all of these objects (pods, replicasets, and services) are used to build a single instance of your application. however, they do little to help you manage the daily or weekly cadence of releasing new versions of your application. indeed, both pods and replicasets are expected to be tied to specific container images that don’t change. the deployment object exists to manage the release of new versions. deployments represent deployed applications in a way that transcends any particular version. additionally, deployments enable you to easily move from one version of your code to the next. this “rollout” process is specifiable and careful. it waits for a userconfigurable amount of time between upgrading individual pods. it also uses health checks to ensure that the new version of the application is operating correctly, and stops the deployment if too many failures occur. using deployments you can simply and reliably roll out new software versions without downtime or errors. the actual mechanics of the software rollout performed by a deployment is controlled by a deployment controller that runs in the kubernetes cluster itself. this means you can let a deployment proceed unattended and it will still operate correctly and safely. this makes it easy to integrate deployments with numer‐ ous continuous delivery tools and services. further, running server-side makes it safe to perform a rollout from places with poor or intermittent internet connectivity. imagine rolling out a new version of your software from your phone while riding on the subway. deployments make this possible and safe! 113 when kubernetes was first released, one of the most popular dem‐ onstrations of its power was the “rolling update,” which showed how you could use a single command to seamlessly update a run‐ ning application without any downtime and without losing requests. this original demo was based on the kubectl rollingupdate command, which is still available in the command-line tool, although its functionality has largely been subsumed by the deployment object. your first deployment like all objects in kubernetes, a deployment can be represented as a declarative yaml object that provides the details about what you want to run. in the following case, the deployment is requesting a single instance of the kuard application: apiversion: extensions/v1beta1 kind: deployment metadata:  name: kuard spec:  selector:  matchlabels:  run: kuard  replicas: 1  template:  metadata:  labels:  run: kuard  spec:  containers:  - name: kuard  image: gcr.io/kuar-demo/kuard-amd64:blue save this yaml file as kuard-deployment.yaml, then you can create it using: $ kubectl create -f kuard-deployment.yaml deployment internals let’s explore how deployments actually work. just as we learned that replicasets manage pods, deployments manage replicasets. as with all relationships in kuber‐ netes, this relationship is defined by labels and a label selector. you can see the label selector by looking at the deployment object: $ kubectl get deployments kuard \\\\  -o jsonpath --template {.spec.selector.matchlabels} map 114 | chapter 10: deployments from this you can see that the deployment is managing a replicaset with the label run=kuard. we can use this in a label selector query across replicasets to find that specific replicaset: $ kubectl get replicasets --selector=run=kuard name desired current ready age kuard-1128242161 1 1 1 13m now let’s see the relationship between a deployment and a replicaset in action. we can resize the deployment using the imperative scale command: $ kubectl scale deployments kuard --replicas=2 deployment.extensions/kuard scaled now if we list that replicaset again, we should see: $ kubectl get replicasets --selector=run=kuard name desired current ready age kuard-1128242161 2 2 2 13m scaling the deployment has also scaled the replicaset it controls. now let’s try the opposite, scaling the replicaset: $ kubectl scale replicasets kuard-1128242161 --replicas=1 replicaset \"kuard-1128242161\" scaled now get that replicaset again: $ kubectl get replicasets --selector=run=kuard name desired current ready age kuard-1128242161 2 2 2 13m that’s odd. despite our scaling the replicaset to one replica, it still has two replicas as its desired state. what’s going on? remember, kubernetes is an online, self-healing system. the top-level deployment object is managing this replicaset. when you adjust the number of replicas to one, it no longer matches the desired state of the deployment, which has replicas set to 2. the deployment controller notices this and takes action to ensure the observed state matches the desired state, in this case read‐ justing the number of replicas back to two. if you ever want to manage that replicaset directly, you need to delete the deploy‐ ment (remember to set --cascade to false, or else it will delete the replicaset and pods as well!). your first deployment | 115 creating deployments of course, as has been stated elsewhere, you should have a preference for declarative management of your kubernetes configurations. this means maintaining the state of your deployments in yaml or json files on disk. as a starting point, download this deployment into a yaml file: $ kubectl get deployments kuard --export -o yaml > kuard-deployment.yaml $ kubectl replace -f kuard-deployment.yaml --save-config if you look in the file, you will see something like this: apiversion: extensions/v1beta1 kind: deployment metadata:  annotations:  deployment.kubernetes.io/revision: \"1\"  creationtimestamp: null  generation: 1  labels:  run: kuard  name: kuard  selflink: /apis/extensions/v1beta1/namespaces/default/deployments/kuard spec:  progressdeadlineseconds: 2147483647  replicas: 2  revisionhistorylimit: 10  selector:  matchlabels:  run: kuard  strategy:  rollingupdate:  maxsurge: 1  maxunavailable: 1  type: rollingupdate  template:  metadata:  creationtimestamp: null  labels:  run: kuard  spec:  containers:  - image: gcr.io/kuar-demo/kuard-amd64:blue  imagepullpolicy: ifnotpresent  name: kuard  resources: {}  terminationmessagepath: /dev/termination-log  terminationmessagepolicy: file  dnspolicy: clusterfirst  restartpolicy: always  schedulername: default-scheduler 116 | chapter 10: deployments  securitycontext: {}  terminationgraceperiodseconds: 30 status: {} a lot of read-only and default fields were removed in the preceding listing for brevity. you also need to run kubectl replace --saveconfig. this adds an annotation so that, when applying changes in the future, kubectl will know what the last applied configuration was for smarter merging of configs. if you always use kubectl apply, this step is only required after the first time you create a deployment using kubectl create -f. the deployment spec has a very similar structure to the replicaset spec. there is a pod template, which contains a number of containers that are created for each replica managed by the deployment. in addition to the pod specification, there is also a strategy object: ...  strategy:  rollingupdate:  maxsurge: 1  maxunavailable: 1  type: rollingupdate ... the strategy object dictates the different ways in which a rollout of new software can proceed. there are two different strategies supported by deployments: recreate and rollingupdate. these are discussed in detail later in this chapter. managing deployments as with all kubernetes objects, you can get detailed information about your deploy‐ ment via the kubectl describe command: $ kubectl describe deployments kuard name: kuard namespace: default creationtimestamp: tue, 16 apr 2019 21:43:25 -0700 labels: run=kuard annotations: deployment.kubernetes.io/revision: 1 selector: run=kuard replicas: 2 desired | 2 updated | 2 total | 2 available | 0 ... strategytype: rollingupdate minreadyseconds: 0 rollingupdatestrategy: 1 max unavailable, 1 max surge managing deployments | 117 pod template:  labels: run=kuard  containers:  kuard:  image: gcr.io/kuar-demo/kuard-amd64:blue  port: <none>  host port: <none>  environment: <none>  mounts: <none>  volumes: <none> conditions:  type status reason  ---- ------ ------  available true minimumreplicasavailable oldreplicasets: <none> newreplicaset: kuard-6d69d9fc5c (2/2 replicas created) events:  type reason age from message  ---- ------ ---- ---- -------  normal scalingreplicaset 4m6s deployment-con... ...  normal scalingreplicaset 113s (x2 over 3m20s) deployment-con... ... in the output of describe there is a great deal of important information. two of the most important pieces of information in the output are oldreplicasets and newreplicaset. these fields point to the replicaset objects this deployment is currently managing. if a deployment is in the middle of a rollout, both fields will be set to a value. if a rollout is complete, oldreplicasets will be set to <none>. in addition to the describe command, there is also the kubectl rollout command for deployments. we will go into this command in more detail later on, but for now, know that you can use kubectl rollout history to obtain the history of rollouts associated with a particular deployment. if you have a current deployment in pro‐ gress, you can use kubectl rollout status to obtain the current status of a rollout. updating deployments deployments are declarative objects that describe a deployed application. the two most common operations on a deployment are scaling and application updates. scaling a deployment although we previously showed how you could imperatively scale a deployment using the kubectl scale command, the best practice is to manage your deployments declaratively via the yaml files, and then use those files to update your deployment. to scale up a deployment, you would edit your yaml file to increase the number of replicas: 118 | chapter 10: deployments ... spec:  replicas: 3 ... once you have saved and committed this change, you can update the deployment using the kubectl apply command: $ kubectl apply -f kuard-deployment.yaml this will update the desired state of the deployment, causing it to increase the size of the replicaset it manages, and eventually create a new pod managed by the deploy‐ ment: $ kubectl get deployments kuard name desired current up-to-date available age kuard 3 3 3 3 4m updating a container image the other common use case for updating a deployment is to roll out a new version of the software running in one or more containers. to do this, you should likewise edit the deployment yaml file, though in this case you are updating the container image, rather than the number of replicas: ...  containers:  - image: gcr.io/kuar-demo/kuard-amd64:green  imagepullpolicy: always ... we are also going to put an annotation in the template for the deployment to record some information about the update: ... spec:  ...  template:  metadata:  annotations:  kubernetes.io/change-cause: \"update to green kuard\" ... make sure you add this annotation to the template and not the deployment itself, since the kubectl apply command uses this field in the deployment object. also, do not update the changecause annotation when doing simple scaling operations. a modifi‐ cation of change-cause is a significant change to the template and will trigger a new rollout. updating deployments | 119 again, you can use kubectl apply to update the deployment: $ kubectl apply -f kuard-deployment.yaml after you update the deployment it will trigger a rollout, which you can then monitor via the kubectl rollout command: $ kubectl rollout status deployments kuard deployment kuard successfully rolled out you can see the old and new replicasets managed by the deployment along with the images being used. both the old and new replicasets are kept around in case you want to roll back: $ kubectl get replicasets -o wide name desired current ready ... image(s) ... kuard-1128242161 0 0 0 ... gcr.io/kuar-demo/ ... kuard-1128635377 3 3 3 ... gcr.io/kuar-demo/ ... if you are in the middle of a rollout and you want to temporarily pause it for some reason (e.g., if you start seeing weird behavior in your system and you want to inves‐ tigate), you can use the pause command: $ kubectl rollout pause deployments kuard deployment \"kuard\" paused if, after investigation, you believe the rollout can safely proceed, you can use the resume command to start up where you left off: $ kubectl rollout resume deployments kuard deployment \"kuard\" resumed rollout history kubernetes deployments maintain a history of rollouts, which can be useful both for understanding the previous state of the deployment and to roll back to a specific version. you can see the deployment history by running: $ kubectl rollout history deployment kuard deployment.extensions/kuard revision change-cause 1 <none> 2 update to green kuard the revision history is given in oldest to newest order. a unique revision number is incremented for each new rollout. so far we have two: the initial deployment, and the update of the image to kuard:1.9.10. 120 | chapter 10: deployments if you are interested in more details about a particular revision, you can add the --revision flag to view details about that specific revision: $ kubectl rollout history deployment kuard --revision=2 deployment.extensions/kuard with revision #2 pod template:  labels: pod-template-hash=54b74ddcd4  run=kuard  annotations: kubernetes.io/change-cause: update to green kuard  containers:  kuard:  image: gcr.io/kuar-demo/kuard-amd64:green  port: <none>  host port: <none>  environment: <none>  mounts: <none>  volumes: <none> let’s do one more update for this example. update the kuard version back to blue by modifying the container version number and updating the change-cause annotation. apply it with kubectl apply. our history should now have three entries: $ kubectl rollout history deployment kuard deployment.extensions/kuard revision change-cause 1 <none> 2 update to green kuard 3 update to blue kuard let’s say there is an issue with the latest release and you want to roll back while you investigate. you can simply undo the last rollout: $ kubectl rollout undo deployments kuard deployment \"kuard\" rolled back the undo command works regardless of the stage of the rollout. you can undo both partially completed and fully completed rollouts. an undo of a rollout is actually sim‐ ply a rollout in reverse (e.g., from v2 to v1, instead of from v1 to v2), and all of the same policies that control the rollout strategy apply to the undo strategy as well. you can see the deployment object simply adjusts the desired replica counts in the man‐ aged replicasets: $ kubectl get replicasets -o wide name desired current ready ... image(s) ... kuard-1128242161 0 0 0 ... gcr.io/kuar-demo/ ... kuard-1570155864 0 0 0 ... gcr.io/kuar-demo/ ... kuard-2738859366 3 3 3 ... gcr.io/kuar-demo/ ... updating deployments | 121 when using declarative files to control your production systems, you want to, as much as possible, ensure that the checked-in mani‐ fests match what is actually running in your cluster. when you do a kubectl rollout undo you are updating the production state in a way that isn’t reflected in your source control. an alternative (and perhaps preferred) way to undo a rollout is to revert your yaml file and kubectl apply the previous version. in this way, your “change tracked configuration” more closely tracks what is really running in your cluster. let’s look at our deployment history again: $ kubectl rollout history deployment kuard deployment.extensions/kuard revision change-cause 1 <none> 3 update to blue kuard 4 update to green kuard revision 2 is missing! it turns out that when you roll back to a previous revision, the deployment simply reuses the template and renumbers it so that it is the latest revi‐ sion. what was revision 2 before is now reordered into revision 4. we previously saw that you can use the kubectl rollout undo command to roll back to a previous version of a deployment. additionally, you can roll back to a spe‐ cific revision in the history using the --to-revision flag: $ kubectl rollout undo deployments kuard --to-revision=3 deployment \"kuard\" rolled back $ kubectl rollout history deployment kuard deployment.extensions/kuard revision change-cause 1 <none> 4 update to green kuard 5 update to blue kuard again, the undo took revision 3, applied it, and renumbered it as revision 5. specifying a revision of 0 is a shorthand way of specifying the previous revision. in this way, kubectl rollout undo is equivalent to kubectl rollout undo --torevision=0. by default, the complete revision history of a deployment is kept attached to the deployment object itself. over time (e.g., years) this history can grow fairly large, so it is recommended that if you have deployments that you expect to keep around for a long time you set a maximum history size for the deployment revision history, to limit the total size of the deployment object. for example, if you do a daily update 122 | chapter 10: deployments you may limit your revision history to 14, to keep a maximum of 2 weeks’ worth of revisions (if you don’t expect to need to roll back beyond 2 weeks). to accomplish this, use the revisionhistorylimit property in the deployment specification: ... spec:  # we do daily rollouts, limit the revision history to two weeks of  # releases as we don\\'t expect to roll back beyond that.  revisionhistorylimit: 14 ... deployment strategies when it comes time to change the version of software implementing your service, a kubernetes deployment supports two different rollout strategies: • recreate • rollingupdate recreate strategy the recreate strategy is the simpler of the two rollout strategies. it simply updates the replicaset it manages to use the new image and terminates all of the pods associ‐ ated with the deployment. the replicaset notices that it no longer has any replicas, and re-creates all pods using the new image. once the pods are re-created, they are running the new version. while this strategy is fast and simple, it has one major drawback—it is potentially catastrophic, and will almost certainly result in some site downtime. because of this, the recreate strategy should only be used for test deployments where a service is not user-facing and a small amount of downtime is acceptable. rollingupdate strategy the rollingupdate strategy is the generally preferable strategy for any user-facing service. while it is slower than recreate, it is also significantly more sophisticated and robust. using rollingupdate, you can roll out a new version of your service while it is still receiving user traffic, without any downtime. as you might infer from the name, the rollingupdate strategy works by updating a few pods at a time, moving incrementally until all of the pods are running the new version of your software. deployment strategies | 123 managing multiple versions of your service importantly, this means that for a period of time, both the new and the old version of your service will be receiving requests and serving traffic. this has important impli‐ cations for how you build your software. namely, it is critically important that each version of your software, and all of its clients, is capable of talking interchangeably with both a slightly older and a slightly newer version of your software. as an example of why this is important, consider the following scenario: you are in the middle of rolling out your frontend software; half of your servers are running version 1 and half are running version 2. a user makes an initial request to your service and downloads a client-side javascript library that implements your ui. this request is serviced by a version 1 server and thus the user receives the version 1 client library. this client library runs in the user’s browser and makes subsequent api requests to your service. these api requests happen to be routed to a version 2 server; thus, version 1 of your javascript client library is talking to version 2 of your api server. if you haven’t ensured compatibility between these versions, your application won’t function correctly. at first, this might seem like an extra burden. but in truth, you always had this prob‐ lem; you may just not have noticed. concretely, a user can make a request at time t just before you initiate an update. this request is serviced by a version 1 server. at t1 you update your service to version 2. at t2 the version 1 client code running on the user’s browser runs and hits an api endpoint being operated by a version 2 server. no matter how you update your software, you have to maintain backward and forward compatibility for reliable updates. the nature of the rollingupdate strategy simply makes it more clear and explicit that this is something to think about. note that this doesn’t just apply to javascript clients—the same thing is true of client libraries that are compiled into other services that make calls to your service. just because you updated doesn’t mean they have updated their client libraries. this sort of backward compatibility is critical to decoupling your service from systems that depend on your service. if you don’t formalize your apis and decouple yourself, you are forced to carefully manage your rollouts with all of the other systems that call into your service. this kind of tight coupling makes it extremely hard to produce the nec‐ essary agility to be able to push out new software every week, let alone every hour or every day. in the decoupled architecture shown in figure 10-1, the frontend is iso‐ lated from the backend via an api contract and a load balancer, whereas in the cou‐ pled architecture, a thick client compiled into the frontend is used to connect directly to the backends. 124 | chapter 10: deployments figure 10-1. diagrams of both decoupled (left) and coupled (right) application architectures con\\x80guring a rolling update rollingupdate is a fairly generic strategy; it can be used to update a variety of appli‐ cations in a variety of settings. consequently, the rolling update itself is quite configu‐ rable; you can tune its behavior to suit your particular needs. there are two parameters you can use to tune the rolling update behavior: maxunavailable and maxsurge. the maxunavailable parameter sets the maximum number of pods that can be unavailable during a rolling update. it can either be set to an absolute number (e.g., 3, meaning a maximum of three pods can be unavailable) or to a percentage (e.g., 20%, meaning a maximum of 20% of the desired number of replicas can be unavailable). generally speaking, using a percentage is a good approach for most services, since the value is correctly applicable regardless of the desired number of replicas in the deployment. however, there are times when you may want to use an absolute number (e.g., limiting the maximum unavailable pods to one). at its core, the maxunavailable parameter helps tune how quickly a rolling update proceeds. for example, if you set maxunavailable to 50%, then the rolling update will immediately scale the old replicaset down to 50% of its original size. if you have four replicas, it will scale it down to two replicas. the rolling update will then replace the removed pods by scaling the new replicaset up to two replicas, for a total of four rep‐ licas (two old, two new). it will then scale the old replicaset down to zero replicas, for a total size of two new replicas. finally, it will scale the new replicaset up to four rep‐ licas, completing the rollout. thus, with maxunavailable set to 50%, our rollout com‐ pletes in four steps, but with only 50% of our service capacity at times. deployment strategies | 125 consider what happens if we instead set maxunavailable to 25%. in this situation, each step is only performed with a single replica at a time and thus it takes twice as many steps for the rollout to complete, but availability only drops to a minimum of 75% during the rollout. this illustrates how maxunavailable allows us to trade roll‐ out speed for availability. the observant among you will note that the recreate strategy is identical to the rollingupdate strategy with maxunavailable set to 100%. using reduced capacity to achieve a successful rollout is useful either when your ser‐ vice has cyclical traffic patterns (e.g., much less traffic at night) or when you have limited resources, so scaling to larger than the current maximum number of replicas isn’t possible. however, there are situations where you don’t want to fall below 100% capacity, but you are willing to temporarily use additional resources in order to perform a rollout. in these situations, you can set the maxunavailable parameter to 0%, and instead con‐ trol the rollout using the maxsurge parameter. like maxunavailable, maxsurge can be specified either as a specific number or a percentage. the maxsurge parameter controls how many extra resources can be created to ach‐ ieve a rollout. to illustrate how this works, imagine we have a service with 10 replicas. we set maxunavailable to 0 and maxsurge to 20%. the first thing the rollout will do is scale the new replicaset up to 2 replicas, for a total of 12 (120%) in the service. it will then scale the old replicaset down to 8 replicas, for a total of 10 (8 old, 2 new) in the service. this process proceeds until the rollout is complete. at any time, the capacity of the service is guaranteed to be at least 100% and the maximum extra resources used for the rollout are limited to an additional 20% of all resources. setting maxsurge to 100% is equivalent to a blue/green deployment. the deployment controller first scales the new version up to 100% of the old version. once the new version is healthy, it immediately scales the old version down to 0%. slowing rollouts to ensure service health the purpose of a staged rollout is to ensure that the rollout results in a healthy, stable service running the new software version. to do this, the deployment controller always waits until a pod reports that it is ready before moving on to updating the next pod. 126 | chapter 10: deployments the deployment controller examines the pod’s status as determined by its readiness checks. readiness checks are part of the pod’s health probes, and they are described in detail in chapter 5. if you want to use deployments to reliably roll out your software, you have to specify readiness health checks for the containers in your pod. without these checks, the deployment controller is running blind. sometimes, however, simply noticing that a pod has become ready doesn’t give you sufficient confidence that the pod actually is behaving correctly. some error condi‐ tions only occur after a period of time. for example, you could have a serious mem‐ ory leak that takes a few minutes to show up, or you could have a bug that is only triggered by 1% of all requests. in most real-world scenarios, you want to wait a period of time to have high confidence that the new version is operating correctly before you move on to updating the next pod. for deployments, this time to wait is defined by the minreadyseconds parameter: ... spec:  minreadyseconds: 60 ... setting minreadyseconds to 60 indicates that the deployment must wait for 60 sec‐ onds after seeing a pod become healthy before moving on to updating the next pod. in addition to waiting a period of time for a pod to become healthy, you also want to set a timeout that limits how long the system will wait. suppose, for example, the new version of your service has a bug and immediately deadlocks. it will never become ready, and in the absence of a timeout, the deployment controller will stall your rollout forever. the correct behavior in such a situation is to time out the rollout. this in turn marks the rollout as failed. this failure status can be used to trigger alerting that can indicate to an operator that there is a problem with the rollout. at first blush, timing out a rollout might seem like an unnecessary complication. however, increasingly, things like rollouts are being triggered by fully automated systems with little to no human involvement. in such a situation, timing out becomes a critical exception, which can either trigger an automated rollback of the release or create a ticket/event that triggers human intervention. to set the timeout period, the deployment parameter progressdeadlineseconds is used: deployment strategies | 127 ... spec:  progressdeadlineseconds: 600 ... this example sets the progress deadline to 10 minutes. if any particular stage in the rollout fails to progress in 10 minutes, then the deployment is marked as failed, and all attempts to move the deployment forward are halted. it is important to note that this timeout is given in terms of deployment progress, not the overall length of a deployment. in this context, progress is defined as any time the deployment creates or deletes a pod. when that happens, the timeout clock is reset to zero. figure 10-2 is an illustration of the deployment lifecycle. figure 10-2. the kubernetes deployment lifecycle deleting a deployment if you ever want to delete a deployment, you can do it either with the imperative command: $ kubectl delete deployments kuard or using the declarative yaml file we created earlier: $ kubectl delete -f kuard-deployment.yaml in either case, by default, deleting a deployment deletes the entire service. it will delete not just the deployment, but also any replicasets being managed by the deployment, as well as any pods being managed by the replicasets. as with replica‐ sets, if this is not the desired behavior, you can use the --cascade=false flag to exclusively delete the deployment object. monitoring a deployment when it comes to a deployment, it is important to note that if it fails to makes pro‐ gress after a certain amount of time, the deployment will time out. when this hap‐ pens, the status of the deployment will transition to a failed state. this status can be 128 | chapter 10: deployments obtained from the status.conditions array, where there will be a condition whose type is progressing and whose status is false. a deployment in such a state has failed and will not progress further. to set how long the deployment controller should wait before transitioning into this state, use the spec.progressdeadlineseconds field. summary at the end of the day, the primary goal of kubernetes is to make it easy for you to build and deploy reliable distributed systems. this means not just instantiating the application once, but managing the regularly scheduled rollout of new versions of that software service. deployments are a critical piece of reliable rollouts and rollout management for your services. summary | 129  chapter 11 daemonsets deployments and replicasets are generally about creating a service (e.g., a web server) with multiple replicas for redundancy. but that is not the only reason you may want to replicate a set of pods within a cluster. another reason to replicate a set of pods is to schedule a single pod on every node within the cluster. generally, the moti‐ vation for replicating a pod to every node is to land some sort of agent or daemon on each node, and the kubernetes object for achieving this is the daemonset. a daemonset ensures a copy of a pod is running across a set of nodes in a kuber‐ netes cluster. daemonsets are used to deploy system daemons such as log collectors and monitoring agents, which typically must run on every node. daemonsets share similar functionality with replicasets; both create pods that are expected to be longrunning services and ensure that the desired state and the observed state of the clus‐ ter match. given the similarities between daemonsets and replicasets, it’s important to under‐ stand when to use one over the other. replicasets should be used when your applica‐ tion is completely decoupled from the node and you can run multiple copies on a given node without special consideration. daemonsets should be used when a single copy of your application must run on all or a subset of the nodes in the cluster. you should generally not use scheduling restrictions or other parameters to ensure that pods do not colocate on the same node. if you find yourself wanting a single pod per node, then a daemonset is the correct kubernetes resource to use. likewise, if you find yourself building a homogeneous replicated service to serve user traffic, then a replicaset is probably the right kubernetes resource to use. you can use labels to run daemonset pods on specific nodes; for example, you may want to run specialized intrusion-detection software on nodes that are exposed to the edge network. 131 you can also use daemonsets to install software on nodes in a cloud-based cluster. for many cloud services, an upgrade or scaling of a cluster can delete and/or recreate new virtual machines. this dynamic immutable infrastructure approach can cause problems if you want (or are required by central it) to have specific software on every node. to ensure that specific software is installed on every machine despite upgrades and scale events, a daemonset is the right approach. you can even mount the host filesystem and run scripts that install rpm/deb packages onto the host operating system. in this way, you can have a cloud-native cluster that still meets the enterprise requirements of your it department. daemonset scheduler by default a daemonset will create a copy of a pod on every node unless a node selec‐ tor is used, which will limit eligible nodes to those with a matching set of labels. dae‐ monsets determine which node a pod will run on at pod creation time by specifying the nodename field in the pod spec. as a result, pods created by daemonsets are ignored by the kubernetes scheduler. like replicasets, daemonsets are managed by a reconciliation control loop that measures the desired state (a pod is present on all nodes) with the observed state (is the pod present on a particular node?). given this information, the daemonset con‐ troller creates a pod on each node that doesn’t currently have a matching pod. if a new node is added to the cluster, then the daemonset controller notices that it is missing a pod and adds the pod to the new node. daemonsets and replicasets are a great demonstration of the value of kubernetes’s decoupled architecture. it might seem that the right design would be for a replicaset to own the pods it manages, and for pods to be subresources of a replicaset. likewise, the pods managed by a daemonset would be subresources of that daemon‐ set. however, this kind of encapsulation would require that tools for dealing with pods be written two different times, once for dae‐ monsets and once for replicasets. instead, kubernetes uses a decoupled approach where pods are top-level objects. this means that every tool you have learned for introspecting pods in the con‐ text of replicasets (e.g., kubectl logs <pod-name>) is equally applicable to pods created by daemonsets. creating daemonsets daemonsets are created by submitting a daemonset configuration to the kubernetes api server. the daemonset in example 11-1 will create a fluentd logging agent on every node in the target cluster. 132 | chapter 11: daemonsets example 11-1. fluentd.yaml apiversion: extensions/v1beta1 kind: daemonset metadata:  name: fluentd  labels:  app: fluentd spec:  template:  metadata:  labels:  app: fluentd  spec:  containers:  - name: fluentd  image: fluent/fluentd:v0.14.10  resources:  limits:  memory: 200mi  requests:  cpu: 100m  memory: 200mi  volumemounts:  - name: varlog  mountpath: /var/log  - name: varlibdockercontainers  mountpath: /var/lib/docker/containers  readonly: true  terminationgraceperiodseconds: 30  volumes:  - name: varlog  hostpath:  path: /var/log  - name: varlibdockercontainers  hostpath:  path: /var/lib/docker/containers daemonsets require a unique name across all daemonsets in a given kubernetes namespace. each daemonset must include a pod template spec, which will be used to create pods as needed. this is where the similarities between replicasets and dae‐ monsets end. unlike replicasets, daemonsets will create pods on every node in the cluster by default unless a node selector is used. once you have a valid daemonset configuration in place, you can use the kubectl apply command to submit the daemonset to the kubernetes api. in this section we will create a daemonset to ensure the fluentd http server is running on every node in our cluster: $ kubectl apply -f fluentd.yaml daemonset \"fluentd\" created creating daemonsets | 133 once the fluentd daemonset has been successfully submitted to the kubernetes api, you can query its current state using the kubectl describe command: $ kubectl describe daemonset fluentd name: fluentd image(s): fluent/fluentd:v0.14.10 selector: app=fluentd node-selector: <none> labels: app=fluentd desired number of nodes scheduled: 3 current number of nodes scheduled: 3 number of nodes misscheduled: 0 pods status: 3 running / 0 waiting / 0 succeeded / 0 failed this output indicates a fluentd pod was successfully deployed to all three nodes in our cluster. we can verify this using the kubectl get pods command with the -o flag to print the nodes where each fluentd pod was assigned: $ kubectl get pods -o wide name age node fluentd-1q6c6 13m k0-default-pool-35609c18-z7tb fluentd-mwi7h 13m k0-default-pool-35609c18-ydae fluentd-zr6l7 13m k0-default-pool-35609c18-pol3 with the fluentd daemonset in place, adding a new node to the cluster will result in a fluentd pod being deployed to that node automatically: $ kubectl get pods -o wide name age node fluentd-1q6c6 13m k0-default-pool-35609c18-z7tb fluentd-mwi7h 13m k0-default-pool-35609c18-ydae fluentd-oipmq 43s k0-default-pool-35609c18-0xnl fluentd-zr6l7 13m k0-default-pool-35609c18-pol3 this is exactly the behavior you want when managing logging daemons and other cluster-wide services. no action was required from our end; this is how the kuber‐ netes daemonset controller reconciles its observed state with our desired state. limiting daemonsets to speci\\x80c nodes the most common use case for daemonsets is to run a pod across every node in a kubernetes cluster. however, there are some cases where you want to deploy a pod to only a subset of nodes. for example, maybe you have a workload that requires a gpu or access to fast storage only available on a subset of nodes in your cluster. in cases like these, node labels can be used to tag specific nodes that meet workload requirements. 134 | chapter 11: daemonsets adding labels to nodes the first step in limiting daemonsets to specific nodes is to add the desired set of labels to a subset of nodes. this can be achieved using the kubectl label command. the following command adds the ssd=true label to a single node: $ kubectl label nodes k0-default-pool-35609c18-z7tb ssd=true node \"k0-default-pool-35609c18-z7tb\" labeled just like with other kubernetes resources, listing nodes without a label selector returns all nodes in the cluster: $ kubectl get nodes name status age k0-default-pool-35609c18-0xnl ready 23m k0-default-pool-35609c18-pol3 ready 1d k0-default-pool-35609c18-ydae ready 1d k0-default-pool-35609c18-z7tb ready 1d using a label selector, we can filter nodes based on labels. to list only the nodes that have the ssd label set to true, use the kubectl get nodes command with the --selector flag: $ kubectl get nodes --selector ssd=true name status age k0-default-pool-35609c18-z7tb ready 1d node selectors node selectors can be used to limit what nodes a pod can run on in a given kuber‐ netes cluster. node selectors are defined as part of the pod spec when creating a dae‐ monset. the daemonset configuration in example 11-2 limits nginx to running only on nodes with the ssd=true label set. example 11-2. nginx-fast-storage.yaml apiversion: extensions/v1beta1 kind: \"daemonset\" metadata:  labels:  app: nginx  ssd: \"true\"  name: nginx-fast-storage spec:  template:  metadata:  labels:  app: nginx  ssd: \"true\"  spec: limiting daemonsets to speci\\x80c nodes | 135  nodeselector:  ssd: \"true\"  containers:  - name: nginx  image: nginx:1.10.0 let’s see what happens when we submit the nginx-fast-storage daemonset to the kubernetes api: $ kubectl apply -f nginx-fast-storage.yaml daemonset \"nginx-fast-storage\" created since there is only one node with the ssd=true label, the nginx-fast-storage pod will only run on that node: $ kubectl get pods -o wide name status node nginx-fast-storage-7b90t running k0-default-pool-35609c18-z7tb adding the ssd=true label to additional nodes will cause the nginx-fast-storage pod to be deployed on those nodes. the inverse is also true: if a required label is removed from a node, the pod will be removed by the daemonset controller. removing labels from a node that are required by a daemonset’s node selector will cause the pod being managed by that daemonset to be removed from the node. updating a daemonset daemonsets are great for deploying services across an entire cluster, but what about upgrades? prior to kubernetes 1.6, the only way to update pods managed by a dae‐ monset was to update the daemonset and then manually delete each pod that was managed by the daemonset so that it would be re-created with the new configura‐ tion. with the release of kubernetes 1.6, daemonsets gained an equivalent to the deployment object that manages a daemonset rollout inside the cluster. rolling update of a daemonset daemonsets can be rolled out using the same rollingupdate strategy that deploy‐ ments use. you can configure the update strategy using the spec.updatestrat egy.type field, which should have the value rollingupdate. when a daemonset has an update strategy of rollingupdate, any change to the spec.template field (or subfields) in the daemonset will initiate a rolling update. 136 | chapter 11: daemonsets as with rolling updates of deployments (see chapter 10), the rollingupdate strategy gradually updates members of a daemonset until all of the pods are running the new configuration. there are two parameters that control the rolling update of a daemonset: • spec.minreadyseconds, which determines how long a pod must be “ready” before the rolling update proceeds to upgrade subsequent pods • spec.updatestrategy.rollingupdate.maxunavailable, which indicates how many pods may be simultaneously updated by the rolling update you will likely want to set spec.minreadyseconds to a reasonably long value, for example 30–60 seconds, to ensure that your pod is truly healthy before the rollout proceeds. the setting for spec.updatestrategy.rollingupdate.maxunavailable is more likely to be application-dependent. setting it to 1 is a safe, general-purpose strategy, but it also takes a while to complete the rollout (number of nodes × minreadysec onds). increasing the maximum unavailability will make your rollout move faster, but increases the “blast radius” of a failed rollout. the characteristics of your application and cluster environment dictate the relative values of speed versus safety. a good approach might be to set maxunavailable to 1 and only increase it if users or admin‐ istrators complain about daemonset rollout speed. once a rolling update has started, you can use the kubectl rollout commands to see the current status of a daemonset rollout. for example, kubectl rollout status daemonsets my-daemon-set will show the current rollout status of a daemonset named my-daemon-set. deleting a daemonset deleting a daemonset is pretty straightforward using the kubectl delete com‐ mand. just be sure to supply the correct name of the daemonset you would like to delete: $ kubectl delete -f fluentd.yaml deleting a daemonset will also delete all the pods being managed by that daemonset. set the --cascade flag to false to ensure only the daemonset is deleted and not the pods. deleting a daemonset | 137 summary daemonsets provide an easy-to-use abstraction for running a set of pods on every node in a kubernetes cluster, or, if the case requires it, on a subset of nodes based on labels. the daemonset provides its own controller and scheduler to ensure key serv‐ ices like monitoring agents are always up and running on the right nodes in your cluster. for some applications, you simply want to schedule a certain number of replicas; you don’t really care where they run as long as they have sufficient resources and distribu‐ tion to operate reliably. however, there is a different class of applications, like agents and monitoring applications, that need to be present on every machine in a cluster to function properly. these daemonsets aren’t really traditional serving applications, but rather add additional capabilities and features to the kubernetes cluster itself. because the daemonset is an active declarative object managed by a controller, it makes it easy to declare your intent that an agent run on every machine without explicitly placing it on every machine. this is especially useful in the context of an autoscaled kubernetes cluster where nodes may constantly be coming and going without user intervention. in such cases, the daemonset automatically adds the proper agents to each node as it is added to the cluster by the autoscaler. 138 | chapter 11: daemonsets chapter 12 jobs so far we have focused on long-running processes such as databases and web applica‐ tions. these types of workloads run until either they are upgraded or the service is no longer needed. while long-running processes make up the large majority of work‐ loads that run on a kubernetes cluster, there is often a need to run short-lived, oneoff tasks. the job object is made for handling these types of tasks. a job creates pods that run until successful termination (i.e., exit with 0). in contrast, a regular pod will continually restart regardless of its exit code. jobs are useful for things you only want to do once, such as database migrations or batch jobs. if run as a regular pod, your database migration task would run in a loop, continually repopulat‐ ing the database after every exit. in this chapter we’ll explore the most common job patterns afforded by kubernetes. we will also leverage these patterns in real-life scenarios. the job object the job object is responsible for creating and managing pods defined in a template in the job specification. these pods generally run until successful completion. the job object coordinates running a number of pods in parallel. if the pod fails before a successful termination, the job controller will create a new pod based on the pod template in the job specification. given that pods have to be scheduled, there is a chance that your job will not execute if the required resources are not found by the scheduler. also, due to the nature of distributed systems there is a small chance, during certain failure scenarios, that duplicate pods will be created for a specific task. 139 job patterns jobs are designed to manage batch-like workloads where work items are processed by one or more pods. by default, each job runs a single pod once until successful termi‐ nation. this job pattern is defined by two primary attributes of a job, namely the number of job completions and the number of pods to run in parallel. in the case of the “run once until completion” pattern, the completions and parallelism parame‐ ters are set to 1. table 12-1 highlights job patterns based on the combination of completions and parallelism for a job configuration. table 12-1. job patterns type use case behavior completions parallelism one shot database migrations a single pod running once until successful termination 1 1 parallel \\x80xed completions multiple pods processing a set of work in parallel one or more pods running one or more times until reaching a \\x80xed completion count 1+ 1+ work queue: parallel jobs multiple pods processing from a centralized work queue one or more pods running once until successful termination 1 2+ one shot one-shot jobs provide a way to run a single pod once until successful termination. while this may sound like an easy task, there is some work involved in pulling this off. first, a pod must be created and submitted to the kubernetes api. this is done using a pod template defined in the job configuration. once a job is up and running, the pod backing the job must be monitored for successful termination. a job can fail for any number of reasons, including an application error, an uncaught exception during runtime, or a node failure before the job has a chance to complete. in all cases, the job controller is responsible for recreating the pod until a successful termination occurs. there are multiple ways to create a one-shot job in kubernetes. the easiest is to use the kubectl command-line tool: $ kubectl run -i oneshot \\\\  --image=gcr.io/kuar-demo/kuard-amd64:blue \\\\  --restart=onfailure \\\\  -- --keygen-enable \\\\  --keygen-exit-on-complete \\\\  --keygen-num-to-gen 10 ... 140 | chapter 12: jobs (id 0) workload starting (id 0 1/10) item done: sha256:nasusg54xokrkjwyn+oshkupkew3mwq7occ (id 0 2/10) item done: sha256:hvkx1anns6sgf/er1lyo+zcdnb8gefgt0/8 (id 0 3/10) item done: sha256:irjclrov3mtt0p0jfsvuyhkrq1tdgr8h1jg (id 0 4/10) item done: sha256:nbqaivy/yrhmegk3ui2sahuxb/o6myo0qrk (id 0 5/10) item done: sha256:ccpboxnlxomqvr2v38yqimxgaa/w2tym+ai (id 0 6/10) item done: sha256:wey2ttidz4atjcr1iimxavczzznjrmboqp8 (id 0 7/10) item done: sha256:t3jsrct7sqwebgqg5crbmobulwk4lfdwiti (id 0 8/10) item done: sha256:e84/vze7kkyjch9ozh02mkxjgoty9phacec (id 0 9/10) item done: sha256:uomyex79qqbi1mhcifg4hdngkonlsij2k3s (id 0 10/10) item done: sha256:wcr8wigofag84bsa8f/9qhukqf+0mencady (id 0) workload exiting there are some things to note here: • the -i option to kubectl indicates that this is an interactive command. kubectl will wait until the job is running and then show the log output from the first (and in this case only) pod in the job. • --restart=onfailure is the option that tells kubectl to create a job object. • all of the options after -- are command-line arguments to the container image. these instruct our test server (kuard) to generate 10 4,096-bit ssh keys and then exit. • your output may not match this exactly. kubectl often misses the first couple of lines of output with the -i option. after the job has completed, the job object and related pod are still around. this is so that you can inspect the log output. note that this job won’t show up in kubectl get jobs unless you pass the -a flag. without this flag, kubectl hides completed jobs. delete the job before continuing: $ kubectl delete jobs oneshot the other option for creating a one-shot job is using a configuration file, as shown in example 12-1. example 12-1. job-oneshot.yaml apiversion: batch/v1 kind: job metadata:  name: oneshot spec:  template:  spec:  containers:  - name: kuard  image: gcr.io/kuar-demo/kuard-amd64:blue  imagepullpolicy: always job patterns | 141  args:  - \"--keygen-enable\"  - \"--keygen-exit-on-complete\"  - \"--keygen-num-to-gen=10\"  restartpolicy: onfailure submit the job using the kubectl apply command: $ kubectl apply -f job-oneshot.yaml job \"oneshot\" created then describe the oneshot job: $ kubectl describe jobs oneshot name: oneshot namespace: default image(s): gcr.io/kuar-demo/kuard-amd64:blue selector: controller-uid=cf87484b-e664-11e6-8222-42010a8a007b parallelism: 1 completions: 1 start time: sun, 29 jan 2017 12:52:13 -0800 labels: job=oneshot pods statuses: 0 running / 1 succeeded / 0 failed no volumes. events:  ... reason message  ... ------ -------  ... successfulcreate created pod: oneshot-4kfdt you can view the results of the job by looking at the logs of the pod that was created: $ kubectl logs oneshot-4kfdt ... serving on :8080 (id 0) workload starting (id 0 1/10) item done: sha256:+r6b4w81dbejxmcd3lhju+eignlezbpxitkn8iqhkpi (id 0 2/10) item done: sha256:mzhewajay1ka8vluslonnmk9fde5zdn7vvbs5ne8axm (id 0 3/10) item done: sha256:trteqhffljmwkqnnyggqm/ivxnyksbig8c03h0g3one (id 0 4/10) item done: sha256:tswpyh/j347il/mgqtxrrdezcoazetgzla8a3/hwbro (id 0 5/10) item done: sha256:ip8xtguj6gbwwlhqjkecvfds96b17nno21i/tnc1j9k (id 0 6/10) item done: sha256:zfnxdqvust/6zzevkyxdrg98p73c/5tm99sebperwfc (id 0 7/10) item done: sha256:th+cnl/iul/huukdmsq2xemdq8oavmhmo6iwj8zeoj0 (id 0 8/10) item done: sha256:3gfsuaalvehqcgnlbou4qd1zqqqj8j738i5r+i5xwvi (id 0 9/10) item done: sha256:5wv4l/xeihsjxwlut2fhf0sckm2g3xh3svtnbgskcxw (id 0 10/10) item done: sha256:bpqqoonwsbjzlqe9zuvrmzkz+dbjantz9hwmqhbdwli (id 0) workload exiting congratulations, your job has run successfully! 142 | chapter 12: jobs you may have noticed that we didn’t specify any labels when creat‐ ing the job object. like with other controllers (daemonsets, repli‐ casets, deployments, etc.) that use labels to identify a set of pods, unexpected behaviors can happen if a pod is reused across objects. because jobs have a finite beginning and ending, it is common for users to create many of them. this makes picking unique labels more difficult and more critical. for this reason, the job object will automatically pick a unique label and use it to identify the pods it creates. in advanced scenarios (such as swapping out a running job without killing the pods it is managing), users can choose to turn off this automatic behavior and manually specify labels and selectors. pod failure we just saw how a job can complete successfully. but what happens if something fails? let’s try that out and see what happens. let’s modify the arguments to kuard in our configuration file to cause it to fail out with a nonzero exit code after generating three keys, as shown in example 12-2. example 12-2. job-oneshot-failure1.yaml ... spec:  template:  spec:  containers:  ...  args:  - \"--keygen-enable\"  - \"--keygen-exit-on-complete\"  - \"--keygen-exit-code=1\"  - \"--keygen-num-to-gen=3\" ... now launch this with kubectl apply -f job-oneshot-failure1.yaml. let it run for a bit and then look at the pod status: $ kubectl get pod -a -l job-name=oneshot name ready status restarts age oneshot-3ddk0 0/1 crashloopbackoff 4 3m here we see that the same pod has restarted four times. kubernetes is in crashloop backoff for this pod. it is not uncommon to have a bug someplace that causes a pro‐ gram to crash as soon as it starts. in that case, kubernetes will wait a bit before job patterns | 143 restarting the pod to avoid a crash loop eating resources on the node. this is all han‐ dled local to the node by the kubelet without the job being involved at all. kill the job (kubectl delete jobs oneshot), and let’s try something else. modify the config file again and change the restartpolicy from onfailure to never. launch this with kubectl apply -f jobs-oneshot-failure2.yaml. if we let this run for a bit and then look at related pods we’ll find something interesting: $ kubectl get pod -l job-name=oneshot -a name ready status restarts age oneshot-0wm49 0/1 error 0 1m oneshot-6h9s2 0/1 error 0 39s oneshot-hkzw0 1/1 running 0 6s oneshot-k5swz 0/1 error 0 28s oneshot-m1rdw 0/1 error 0 19s oneshot-x157b 0/1 error 0 57s what we see is that we have multiple pods here that have errored out. by setting restartpolicy: never we are telling the kubelet not to restart the pod on failure, but rather just declare the pod as failed. the job object then notices and creates a replacement pod. if you aren’t careful, this’ll create a lot of “junk” in your cluster. for this reason, we suggest you use restartpolicy: onfailure so failed pods are rerun in place. clean this up with kubectl delete jobs oneshot. so far we’ve seen a program fail by exiting with a nonzero exit code. but workers can fail in other ways. specifically, they can get stuck and not make any forward progress. to help cover this case, you can use liveness probes with jobs. if the liveness probe policy determines that a pod is dead, it’ll be restarted/replaced for you. parallelism generating keys can be slow. let’s start a bunch of workers together to make key gen‐ eration faster. we’re going to use a combination of the completions and parallelism parameters. our goal is to generate 100 keys by having 10 runs of kuard with each run generating 10 keys. but we don’t want to swamp our cluster, so we’ll limit ourselves to only five pods at a time. this translates to setting completions to 10 and parallelism to 5. the config is shown in example 12-3. 144 | chapter 12: jobs example 12-3. job-parallel.yaml apiversion: batch/v1 kind: job metadata:  name: parallel  labels:  chapter: jobs spec:  parallelism: 5  completions: 10  template:  metadata:  labels:  chapter: jobs  spec:  containers:  - name: kuard  image: gcr.io/kuar-demo/kuard-amd64:blue  imagepullpolicy: always  args:  - \"--keygen-enable\"  - \"--keygen-exit-on-complete\"  - \"--keygen-num-to-gen=10\"  restartpolicy: onfailure start it up: $ kubectl apply -f job-parallel.yaml job \"parallel\" created now watch as the pods come up, do their thing, and exit. new pods are created until 10 have completed altogether. here we use the --watch flag to have kubectl stay around and list changes as they happen: $ kubectl get pods -w name ready status restarts age parallel-55tlv 1/1 running 0 5s parallel-5s7s9 1/1 running 0 5s parallel-jp7bj 1/1 running 0 5s parallel-lssmn 1/1 running 0 5s parallel-qxcxp 1/1 running 0 5s name ready status restarts age parallel-jp7bj 0/1 completed 0 26s parallel-tzp9n 0/1 pending 0 0s parallel-tzp9n 0/1 pending 0 0s parallel-tzp9n 0/1 containercreating 0 1s parallel-tzp9n 1/1 running 0 1s parallel-tzp9n 0/1 completed 0 48s parallel-x1kmr 0/1 pending 0 0s parallel-x1kmr 0/1 pending 0 0s parallel-x1kmr 0/1 containercreating 0 0s job patterns | 145 parallel-x1kmr 1/1 running 0 1s parallel-5s7s9 0/1 completed 0 1m parallel-tprfj 0/1 pending 0 0s parallel-tprfj 0/1 pending 0 0s parallel-tprfj 0/1 containercreating 0 0s parallel-tprfj 1/1 running 0 2s parallel-x1kmr 0/1 completed 0 52s parallel-bgvz5 0/1 pending 0 0s parallel-bgvz5 0/1 pending 0 0s parallel-bgvz5 0/1 containercreating 0 0s parallel-bgvz5 1/1 running 0 2s parallel-qxcxp 0/1 completed 0 2m parallel-xplw2 0/1 pending 0 1s parallel-xplw2 0/1 pending 0 1s parallel-xplw2 0/1 containercreating 0 1s parallel-xplw2 1/1 running 0 3s parallel-bgvz5 0/1 completed 0 40s parallel-55tlv 0/1 completed 0 2m parallel-lssmn 0/1 completed 0 2m feel free to study the completed jobs and check out their logs to see the fingerprints of the keys they generated. clean up by deleting the finished job object with kubectl delete job parallel. work queues a common use case for jobs is to process work from a work queue. in this scenario, some task creates a number of work items and publishes them to a work queue. a worker job can be run to process each work item until the work queue is empty (figure 12-1). figure 12-1. parallel jobs starting a work queue we start by launching a centralized work queue service. kuard has a simple memorybased work queue system built in. we will start an instance of kuard to act as a coor‐ dinator for all the work to be done. next, we create a simple replicaset to manage a singleton work queue daemon. we are using a replicaset to ensure that a new pod will get created in the face of machine failure, as shown in example 12-4. 146 | chapter 12: jobs example 12-4. rs-queue.yaml apiversion: extensions/v1beta1 kind: replicaset metadata:  labels:  app: work-queue  component: queue  chapter: jobs  name: queue spec:  replicas: 1  template:  metadata:  labels:  app: work-queue  component: queue  chapter: jobs  spec:  containers:  - name: queue  image: \"gcr.io/kuar-demo/kuard-amd64:blue\"  imagepullpolicy: always run the work queue with the following command: $ kubectl apply -f rs-queue.yaml at this point the work queue daemon should be up and running. let’s use port for‐ warding to connect to it. leave this command running in a terminal window: $ queuepod=$(kubectl get pods -l app=work-queue,component=queue \\\\  -o jsonpath=\\'{.items.metadata.name}\\') $ kubectl port-forward $queuepod 8080:8080 forwarding from 127.0.0.1:8080 -> 8080 forwarding from :8080 -> 8080 you can open your browser to http://localhost:8080 and see the kuard interface. switch to the “memq server” tab to keep an eye on what is going on. with the work queue server in place, we should expose it using a service. this will make it easy for producers and consumers to locate the work queue via dns, as example 12-5 shows. example 12-5. service-queue.yaml apiversion: v1 kind: service metadata:  labels:  app: work-queue  component: queue job patterns | 147  chapter: jobs  name: queue spec:  ports:  - port: 8080  protocol: tcp  targetport: 8080  selector:  app: work-queue  component: queue create the queue service with kubectl: $ kubectl apply -f service-queue.yaml service \"queue\" created loading up the queue we are now ready to put a bunch of work items in the queue. for the sake of simplic‐ ity we’ll just use curl to drive the api for the work queue server and insert a bunch of work items. curl will communicate to the work queue through the kubectl portforward we set up earlier, as shown in example 12-6. example 12-6. load-queue.sh # create a work queue called \\'keygen\\' curl -x put localhost:8080/memq/server/queues/keygen # create 100 work items and load up the queue. for i in work-item-{0..99}; do  curl -x post localhost:8080/memq/server/queues/keygen/enqueue \\\\  -d \"$i\" done run these commands, and you should see 100 json objects output to your terminal with a unique message identifier for each work item. you can confirm the status of the queue by looking at the “memq server” tab in the ui, or you can ask the work queue api directly: $ curl 127.0.0.1:8080/memq/server/stats {  \"kind\": \"stats\",  \"queues\": [  {  \"depth\": 100,  \"dequeued\": 0,  \"drained\": 0,  \"enqueued\": 100,  \"name\": \"keygen\"  } 148 | chapter 12: jobs  ] } now we are ready to kick off a job to consume the work queue until it’s empty. creating the consumer job this is where things get interesting! kuard is also able to act in consumer mode. we can set it up to draw work items from the work queue, create a key, and then exit once the queue is empty, as shown in example 12-7. example 12-7. job-consumers.yaml apiversion: batch/v1 kind: job metadata:  labels:  app: message-queue  component: consumer  chapter: jobs  name: consumers spec:  parallelism: 5  template:  metadata:  labels:  app: message-queue  component: consumer  chapter: jobs  spec:  containers:  - name: worker  image: \"gcr.io/kuar-demo/kuard-amd64:blue\"  imagepullpolicy: always  args:  - \"--keygen-enable\"  - \"--keygen-exit-on-complete\"  - \"--keygen-memq-server=http://queue:8080/memq/server\"  - \"--keygen-memq-queue=keygen\"  restartpolicy: onfailure here, we are telling the job to start up five pods in parallel. as the completions parameter is unset, we put the job into a worker pool mode. once the first pod exits with a zero exit code, the job will start winding down and will not start any new pods. this means that none of the workers should exit until the work is done and they are all in the process of finishing up. job patterns | 149 now, create the consumers job: $ kubectl apply -f job-consumers.yaml job \"consumers\" created once the job has been created, you can view the pods backing the job: $ kubectl get pods name ready status restarts age queue-43s87 1/1 running 0 5m consumers-6wjxc 1/1 running 0 2m consumers-7l5mh 1/1 running 0 2m consumers-hvz42 1/1 running 0 2m consumers-pc8hr 1/1 running 0 2m consumers-w20cc 1/1 running 0 2m note there are five pods running in parallel. these pods will continue to run until the work queue is empty. you can watch as it happens in the ui on the work queue server. as the queue empties, the consumer pods will exit cleanly and the consumers job will be considered complete. cleaning up using labels, we can clean up all of the stuff we created in this section: $ kubectl delete rs,svc,job -l chapter=jobs cronjobs sometimes you want to schedule a job to be run at a certain interval. to achieve this you can declare a cronjob in kubernetes, which is responsible for creating a new job object at a particular interval. the declaration of a cronjob looks like: apiversion: batch/v1beta1 kind: cronjob metadata:  name: example-cron spec:  # run every fifth hour  schedule: \"0 */5 * * *\"  jobtemplate:  spec:  template:  spec:  containers:  - name: batch-job  image: my-batch-image  restartpolicy: onfailure note the spec.schedule field, which contains the interval for the cronjob in stan‐ dard cron format. 150 | chapter 12: jobs you can save this file as cron-job.yaml, and create the cronjob with kubectl create -f cron-job.yaml. if you are interested in the current state of a cronjob, you can use kubectl describe <cron-job> to get the details. summary on a single cluster, kubernetes can handle both long-running workloads such as web applications and short-lived workloads such as batch jobs. the job abstraction allows you to model batch job patterns ranging from simple one-time tasks to parallel jobs that process many items until work has been exhausted. jobs are a low-level primitive and can be used directly for simple workloads. how‐ ever, kubernetes is built from the ground up to be extensible by higher-level objects. jobs are no exception; they can easily be used by higher-level orchestration systems to take on more complex tasks. summary | 151  chapter 13 con\\x80gmaps and secrets it is a good practice to make container images as reusable as possible. the same image should be able to be used for development, staging, and production. it is even better if the same image is general-purpose enough to be used across applications and services. testing and versioning get riskier and more complicated if images need to be recreated for each new environment. but then how do we specialize the use of that image at runtime? this is where configmaps and secrets come into play. configmaps are used to pro‐ vide configuration information for workloads. this can either be fine-grained infor‐ mation (a short string) or a composite value in the form of a file. secrets are similar to configmaps but focused on making sensitive information available to the workload. they can be used for things like credentials or tls certificates. con\\x80gmaps one way to think of a configmap is as a kubernetes object that defines a small file‐ system. another way is as a set of variables that can be used when defining the envi‐ ronment or command line for your containers. the key thing is that the configmap is combined with the pod right before it is run. this means that the container image and the pod definition itself can be reused across many apps by just changing the configmap that is used. creating con\\x80gmaps let’s jump right in and create a configmap. like many objects in kubernetes, you can create these in an immediate, imperative way, or you can create them from a manifest on disk. we’ll start with the imperative method. 153 first, suppose we have a file on disk (called my-config.txt) that we want to make avail‐ able to the pod in question, as shown in example 13-1. example 13-1. my-config.txt # this is a sample config file that i might use to configure an application parameter1 = value1 parameter2 = value2 next, let’s create a configmap with that file. we’ll also add a couple of simple key/ value pairs here. these are referred to as literal values on the command line: $ kubectl create configmap my-config \\\\  --from-file=my-config.txt \\\\  --from-literal=extra-param=extra-value \\\\  --from-literal=another-param=another-value the equivalent yaml for the configmap object we just created is: $ kubectl get configmaps my-config -o yaml apiversion: v1 data:  another-param: another-value  extra-param: extra-value  my-config.txt: |  # this is a sample config file that i might use to configure an application  parameter1 = value1  parameter2 = value2 kind: configmap metadata:  creationtimestamp: ...  name: my-config  namespace: default  resourceversion: \"13556\"  selflink: /api/v1/namespaces/default/configmaps/my-config  uid: 3641c553-f7de-11e6-98c9-06135271a273 as you can see, the configmap is really just some key/value pairs stored in an object. the interesting stuff happens when you try to use a configmap. using a con\\x80gmap there are three main ways to use a configmap: filesystem you can mount a configmap into a pod. a file is created for each entry based on the key name. the contents of that file are set to the value. 154 | chapter 13: con\\x80gmaps and secrets environment variable a configmap can be used to dynamically set the value of an environment variable. command-line argument kubernetes supports dynamically creating the command line for a container based on configmap values. let’s create a manifest for kuard that pulls all of these together, as shown in example 13-2. example 13-2. kuard-config.yaml apiversion: v1 kind: pod metadata:  name: kuard-config spec:  containers:  - name: test-container  image: gcr.io/kuar-demo/kuard-amd64:blue  imagepullpolicy: always  command:  - \"/kuard\"  - \"$(extraparam)\"  env:  - name: anotherparam  valuefrom:  configmapkeyref:  name: my-config  key: another-param  - name: extraparam  valuefrom:  configmapkeyref:  name: my-config  key: extra-param  volumemounts:  - name: config-volume  mountpath: /config  volumes:  - name: config-volume  configmap:  name: my-config  restartpolicy: never for the filesystem method, we create a new volume inside the pod and give it the name config-volume. we then define this volume to be a configmap volume and point at the configmap to mount. we have to specify where this gets mounted into the kuard container with a volumemount. in this case we are mounting it at /config. con\\x80gmaps | 155 environment variables are specified with a special valuefrom member. this refer‐ ences the configmap and the data key to use within that configmap. command-line arguments build on environment variables. kubernetes will perform the correct substitution with a special $(<env-var-name>) syntax. run this pod, and let’s port forward to examine how the app sees the world: $ kubectl apply -f kuard-config.yaml $ kubectl port-forward kuard-config 8080 now point your browser at http://localhost:8080. we can look at how we’ve injected configuration values into the program in all three ways. click the “server env” tab on the left. this will show the command line that the app was launched with along with its environment, as shown in figure 13-1. figure 13-1. kuard showing its environment 156 | chapter 13: con\\x80gmaps and secrets here we can see that we’ve added two environment variables (anotherparam and extraparam) whose values are set via the configmap. furthermore, we’ve added an argument to the command line of kuard based on the extraparam value. next, click the “file system browser” tab (figure 13-2). this lets you explore the file‐ system as the application sees it. you should see an entry called /config. this is a vol‐ ume created based on our configmap. if you navigate into that, you’ll see that a file has been created for each entry of the configmap. you’ll also see some hidden files (prepended with ..) that are used to do a clean swap of new values when the config‐ map is updated. figure 13-2. the /config directory as seen through kuard secrets while configmaps are great for most configuration data, there is certain data that is extra-sensitive. this can include passwords, security tokens, or other types of private secrets | 157 keys. collectively, we call this type of data “secrets.” kubernetes has native support for storing and handling this data with care. secrets enable container images to be created without bundling sensitive data. this allows containers to remain portable across environments. secrets are exposed to pods via explicit declaration in pod manifests and the kubernetes api. in this way, the kubernetes secrets api provides an application-centric mechanism for exposing sensitive configuration information to applications in a way that’s easy to audit and leverages native os isolation primitives. by default, kubernetes secrets are stored in plain text in the etcd storage for the cluster. depending on your requirements, this may not be sufficient security for you. in particular, anyone who has cluster administration rights in your cluster will be able to read all of the secrets in the cluster. in recent versions of kubernetes, sup‐ port has been added for encrypting the secrets with a user-supplied key, generally integrated into a cloud key store. additionally, most cloud key stores have integration with kubernetes flexible volumes, enabling you to skip kubernetes secrets entirely and rely exclu‐ sively on the cloud provider’s key store. all of these options should provide you with sufficient tools to craft a security profile that suits your needs. the remainder of this section will explore how to create and manage kubernetes secrets, and also lay out best practices for exposing secrets to pods that require them. creating secrets secrets are created using the kubernetes api or the kubectl command-line tool. secrets hold one or more data elements as a collection of key/value pairs. in this section we will create a secret to store a tls key and certificate for the kuard application that meets the storage requirements listed previously. the kuard container image does not bundle a tls certificate or key. this allows the kuard container to remain portable across environments and distributable through public docker repositories. the first step in creating a secret is to obtain the raw data we want to store. the tls key and certificate for the kuard application can be downloaded by running the fol‐ lowing commands: $ curl -o kuard.crt https://storage.googleapis.com/kuar-demo/kuard.crt $ curl -o kuard.key https://storage.googleapis.com/kuar-demo/kuard.key 158 | chapter 13: con\\x80gmaps and secrets these certificates are shared with the world and they provide no actual security. please do not use them except as a learning tool in these examples. with the kuard.crt and kuard.key files stored locally, we are ready to create a secret. create a secret named kuard-tls using the create secret command: $ kubectl create secret generic kuard-tls \\\\  --from-file=kuard.crt \\\\  --from-file=kuard.key the kuard-tls secret has been created with two data elements. run the following command to get details: $ kubectl describe secrets kuard-tls name: kuard-tls namespace: default labels: <none> annotations: <none> type: opaque data ==== kuard.crt: 1050 bytes kuard.key: 1679 bytes with the kuard-tls secret in place, we can consume it from a pod by using a secrets volume. consuming secrets secrets can be consumed using the kubernetes rest api by applications that know how to call that api directly. however, our goal is to keep applications portable. not only should they run well in kubernetes, but they should run, unmodified, on other platforms. instead of accessing secrets through the api server, we can use a secrets volume. secrets volumes secret data can be exposed to pods using the secrets volume type. secrets volumes are managed by the kubelet and are created at pod creation time. secrets are stored on tmpfs volumes (aka ram disks), and as such are not written to disk on nodes. each data element of a secret is stored in a separate file under the target mount point specified in the volume mount. the kuard-tls secret contains two data elements: secrets | 159 kuard.crt and kuard.key. mounting the kuard-tls secrets volume to /tls results in the following files: /tls/kuard.crt /tls/kuard.key the pod manifest in example 13-3 demonstrates how to declare a secrets volume, which exposes the kuard-tls secret to the kuard container under /tls. example 13-3. kuard-secret.yaml apiversion: v1 kind: pod metadata:  name: kuard-tls spec:  containers:  - name: kuard-tls  image: gcr.io/kuar-demo/kuard-amd64:blue  imagepullpolicy: always  volumemounts:  - name: tls-certs  mountpath: \"/tls\"  readonly: true  volumes:  - name: tls-certs  secret:  secretname: kuard-tls create the kuard-tls pod using kubectl and observe the log output from the run‐ ning pod: $ kubectl apply -f kuard-secret.yaml connect to the pod by running: $ kubectl port-forward kuard-tls 8443:8443 now navigate your browser to https://localhost:8443. you should see some invalid cer‐ tificate warnings as this is a self-signed certificate for kuard.example.com. if you navi‐ gate past this warning, you should see the kuard server hosted via https. use the “file system browser” tab to find the certificates on disk. private docker registries a special use case for secrets is to store access credentials for private docker regis‐ tries. kubernetes supports using images stored on private registries, but access to those images requires credentials. private images can be stored across one or more private registries. this presents a challenge for managing credentials for each private registry on every possible node in the cluster. 160 | chapter 13: con\\x80gmaps and secrets image pull secrets leverage the secrets api to automate the distribution of private reg‐ istry credentials. image pull secrets are stored just like normal secrets but are con‐ sumed through the spec.imagepullsecrets pod specification field. use the create secret docker-registry to create this special kind of secret: $ kubectl create secret docker-registry my-image-pull-secret \\\\  --docker-username=<username> \\\\  --docker-password=<password> \\\\  --docker-email=<email-address> enable access to the private repository by referencing the image pull secret in the pod manifest file, as shown in example 13-4. example 13-4. kuard-secret-ips.yaml apiversion: v1 kind: pod metadata:  name: kuard-tls spec:  containers:  - name: kuard-tls  image: gcr.io/kuar-demo/kuard-amd64:blue  imagepullpolicy: always  volumemounts:  - name: tls-certs  mountpath: \"/tls\"  readonly: true  imagepullsecrets:  - name: my-image-pull-secret  volumes:  - name: tls-certs  secret:  secretname: kuard-tls if you are repeatedly pulling from the same registry, you can add the secrets to the default service account associated with each pod to avoid having to specify the secrets in every pod you create. naming constraints the key names for data items inside of a secret or configmap are defined to map to valid environment variable names. they may begin with a dot followed by a letter or number. following characters include dots, dashes, and underscores. dots cannot be repeated and dots and underscores or dashes cannot be adjacent to each other. more formally, this means that they must conform to the regular expression naming constraints | 161 ^[.[?[a-zaz0-9[([.[?[a-za-z0-9[+[-a-za-z0-9[?)*$. some examples of valid and invalid names for configmaps and secrets are given in table 13-1. table 13-1. configmap and secret key examples valid key name invalid key name .authtoken token..properties key.pem auth file.json configfile password.txt when selecting a key name, consider that these keys can be exposed to pods via a volume mount. pick a name that is going to make sense when specified on a command line or in a config file. storing a tls key as key.pem is more clear than tls-key when configuring applications to access secrets. configmap data values are simple utf-8 text specified directly in the manifest. as of kubernetes 1.6, configmaps are unable to store binary data. secret data values hold arbitrary data encoded using base64. the use of base64 encoding makes it possible to store binary data. this does, however, make it more difficult to manage secrets that are stored in yaml files as the base64-encoded value must be put in the yaml. note that the maximum size for a configmap or secret is 1 mb. managing con\\x80gmaps and secrets secrets and configmaps are managed through the kubernetes api. the usual create, delete, get, and describe commands work for manipulating these objects. listing you can use the kubectl get secrets command to list all secrets in the current namespace: $ kubectl get secrets name type data age default-token-f5jq2 kubernetes.io/service-account-token 3 1h kuard-tls opaque 2 20m similarly, you can list all of the configmaps in a namespace: $ kubectl get configmaps name data age my-config 3 1m 162 | chapter 13: con\\x80gmaps and secrets kubectl describe can be used to get more details on a single object: $ kubectl describe configmap my-config name: my-config namespace: default labels: <none> annotations: <none> data ==== another-param: 13 bytes extra-param: 11 bytes my-config.txt: 116 bytes finally, you can see the raw data (including values in secrets!) with something like kubectl get configmap my-config -o yaml or kubectl get secret kuard-tls -o yaml. creating the easiest way to create a secret or a configmap is via kubectl create secret generic or kubectl create configmap. there are a variety of ways to specify the data items that go into the secret or configmap. these can be combined in a single command: --from-file=<filename> load from the file with the secret data key the same as the filename. --from-file=<key>=<filename> load from the file with the secret data key explicitly specified. --from-file=<directory> load all the files in the specified directory where the filename is an acceptable key name. --from-literal=<key>=<value> use the specified key/value pair directly. updating you can update a configmap or secret and have it reflected in running programs. there is no need to restart if the application is configured to reread configuration val‐ ues. this is a rare feature but might be something you put in your own applications. the following are three ways to update configmaps or secrets. managing con\\x80gmaps and secrets | 163 update from \\x80le if you have a manifest for your configmap or secret, you can just edit it directly and push a new version with kubectl replace -f <filename>. you can also use kubectl apply -f <filename> if you previously created the resource with kubectl apply. due to the way that datafiles are encoded into these objects, updating a configuration can be a bit cumbersome as there is no provision in kubectl to load data from an external file. the data must be stored directly in the yaml manifest. the most common use case is when the configmap is defined as part of a directory or list of resources and everything is created and updated together. oftentimes these manifests will be checked into source control. it is generally a bad idea to check secret yaml files into source control. it is too easy to push these files someplace public and leak your secrets. recreate and update if you store the inputs into your configmaps or secrets as separate files on disk (as opposed to embedded into yaml directly), you can use kubectl to recreate the man‐ ifest and then use it to update the object. this will look something like this: $ kubectl create secret generic kuard-tls \\\\  --from-file=kuard.crt --from-file=kuard.key \\\\  --dry-run -o yaml | kubectl replace -f - this command line first creates a new secret with the same name as our existing secret. if we just stopped there, the kubernetes api server would return an error complaining that we are trying to create a secret that already exists. instead, we tell kubectl not to actually send the data to the server but instead to dump the yaml that it would have sent to the api server to stdout. we then pipe that to kubectl replace and use -f - to tell it to read from stdin. in this way, we can update a secret from files on disk without having to manually base64-encode data. edit current version the final way to update a configmap is to use kubectl edit to bring up a version of the configmap in your editor so you can tweak it (you could also do this with a secret, but you’d be stuck managing the base64 encoding of values on your own): $ kubectl edit configmap my-config 164 | chapter 13: con\\x80gmaps and secrets you should see the configmap definition in your editor. make your desired changes and then save and close your editor. the new version of the object will be pushed to the kubernetes api server. live updates once a configmap or secret is updated using the api, it’ll be automatically pushed to all volumes that use that configmap or secret. it may take a few seconds, but the file listing and contents of the files, as seen by kuard, will be updated with these new val‐ ues. using this live update feature you can update the configuration of applications without restarting them. currently there is no built-in way to signal an application when a new version of a configmap is deployed. it is up to the application (or some helper script) to look for the config files to change and reload them. using the file browser in kuard (accessed through kubectl port-forward) is a great way to interactively play with dynamically updating secrets and configmaps. summary configmaps and secrets are a great way to provide dynamic configuration in your application. they allow you to create a container image (and pod definition) once and reuse it in different contexts. this can include using the exact same image as you move from dev to staging to production. it can also include using a single image across multiple teams and services. separating configuration from application code will make your applications more reliable and reusable. summary | 165  chapter 14 role-based access control for kubernetes at this point, nearly every kubernetes cluster you encounter has role-based access control (rbac) enabled. so you likely have at least partially encountered rbac before. perhaps you initially couldn’t access your cluster until you used some magical incantation to add a rolebinding to map a user to a role. however, even though you may have had some exposure to rbac, you may not have had a great deal of experi‐ ence understanding rbac in kubernetes, what it is for, and how to use it successfully. that is the subject of this chapter. rbac was introduced into kubernetes with version 1.5 and became generally avail‐ able in kubernetes 1.8. role-based access control provides a mechanism for restrict‐ ing both access to and actions on kubernetes apis to ensure that only appropriate users have access to apis in the cluster. rbac is a critical component to both harden access to the kubernetes cluster where you are deploying your application and (possi‐ bly more importantly) prevent unexpected accidents where one person in the wrong namespace mistakenly takes down production when they think they are destroying their test cluster. multitenant security in kubernetes is a complex, multifaceted topic worthy of its own volume. while rbac can be quite useful in lim‐ iting access to the kubernetes api, it’s important to remember that anyone who can run arbitrary code inside the kubernetes cluster can effectively obtain root privileges on the entire cluster. there are approaches that you can take to make such attacks harder and more expensive, and a correct rbac setup is part of this defense. but if you are focused on hostile multitenant security, do not believe that rbac by itself is sufficient to protect you. you must isolate the pods running in your cluster to provide effective multitenant security. generally this is done with a hypervisor isolated container, or some sort of container sandbox, or both. 167 before we dive into the details of rbac in kubernetes, it’s valuable to have a highlevel understanding of rbac as a concept, as well as authentication and authoriza‐ tion more generally. every request to kubernetes is first authenticated. authentication provides the iden‐ tity of the caller issuing the request. it could be as simple as saying that the request is unauthenticated, or it could integrate deeply with a pluggable authentication provider (e.g., azure active directory) to establish an identity within that third-party system. interestingly enough, kubernetes does not have a built-in identity store, focusing instead on integrating other identity sources within itself. once users have been properly identified, the authorization phase determines whether they are authorized to perform the request. authorization is a combination of the identity of the user, the resource (effectively the http path), and the verb or action the user is attempting to perform. if the particular user is authorized for per‐ forming that action on that resource, then the request is allowed to proceed. other‐ wise, an http 403 error is returned. more details of this process are given in the following sections. role-based access control to properly manage access in kubernetes, it’s critical to understand how identity, roles, and role bindings interact to control who can do what with what resources. at first, rbac can seem like a challenge to understand, with a series of interconnected, abstract concepts; but once understood, managing cluster access is straightforward and safe. identity in kubernetes every request that comes to kubernetes is associated with some identity. even a request with no identity is associated with the system:unauthenticated group. kubernetes makes a distinction between user identities and service account identities. service accounts are created and managed by kubernetes itself and are generally asso‐ ciated with components running inside the cluster. user accounts are all other accounts associated with actual users of the cluster, and often include automation like continuous delivery as a service that runs outside of the cluster. kubernetes uses a generic interface for authentication providers. each of the provid‐ ers supplies a username and optionally the set of groups to which the user belongs. kubernetes supports a number of different authentication providers, including: • http basic authentication (largely deprecated) • x509 client certificates 168 | chapter 14: role-based access control for kubernetes • static token files on the host • cloud authentication providers like azure active directory and aws identity and access management (iam) • authentication webhooks while most managed kubernetes installations configure authentication for you, if you are deploying your own authentication you will need to configure flags on the kubernetes api server appropriately. understanding roles and role bindings identity is just the beginning of authorization in kubernetes. once the system knows the identity of the request, it needs to determine if the request is authorized for that user. to achieve this, it uses the general concept of roles and role bindings. a role is a set of abstract capabilities. for example, the appdev role might represent the ability to create pods and services. a role binding is an assignment of a role to one or more identities. thus, binding the appdev role to the user identity alice indicates that alice has the ability to create pods and services. roles and role bindings in kubernetes in kubernetes there are two pairs of related resources that represent roles and role bindings. one pair applies to just a namespace (role and rolebinding) while the other pair applies across the cluster (clusterrole and clusterrolebinding). let’s examine role and rolebinding first. role resources are namespaced, and repre‐ sent capabilities within that single namespace. you cannot use namespaced roles for non-namespaced resources (e.g., customresourcedefinitions), and binding a role binding to a role only provides authorization within the kubernetes namespace that contains both the role and the roledefinition. as a concrete example, here is a simple role that gives an identity the ability to create and modify pods and services: kind: role apiversion: rbac.authorization.k8s.io/v1 metadata:  namespace: default  name: pod-and-services rules: - apigroups:   resources:   verbs:  role-based access control | 169 to bind this role to the user alice, we need to create a rolebinding that looks as follows. this role binding also binds the group mydevs to the same role: apiversion: rbac.authorization.k8s.io/v1 kind: rolebinding metadata:  namespace: default  name: pods-and-services subjects: - apigroup: rbac.authorization.k8s.io  kind: user  name: alice - apigroup: rbac.authorization.k8s.io  kind: group  name: mydevs roleref:  apigroup: rbac.authorization.k8s.io  kind: role  name: pod-and-services of course, sometimes you want to create a role that applies to the entire cluster, or you want to limit access to cluster-level resources. to achieve this, you use the clusterrole and clusterrolebinding resources. they are largely identical to their namespaced peers, but with larger scope. verbs for kubernetes roles roles are defined in terms of both a resource (e.g., “pods”) and a verb that describes an action that can be performed on that resource. the verbs correspond roughly to http methods. the commonly used verbs in kubernetes rbac are listed in table 14-1. table 14-1. common kubernetes rbac verbs verb http method description create post create a new resource. delete delete delete an existing resource. get get get a resource. list get list a collection of resources. patch patch modify an existing resource via a partial change. update put modify an existing resource via a complete object. watch get watch for streaming updates to a resource. proxy get connect to resource via a streaming websocket proxy. 170 | chapter 14: role-based access control for kubernetes using built-in roles of course, designing your own roles can be complicated and time-consuming. fur‐ thermore, kubernetes has a large number of well-known system identities (e.g., a scheduler) that require a known set of capabilities. consequently, kubernetes has a large number of built-in cluster roles. you can view these by running: $ kubectl get clusterroles while most of these built-in roles are for system utilities, four are designed for generic end users: • the cluster-admin role provides complete access to the entire cluster. • the admin role provides complete access to a complete namespace. • the edit role allows an end user to modify things in a namespace. • the view role allows for read-only access to a namespace. most clusters already have numerous clusterrole bindings set up, and you can view these bindings with kubectl get clusterrolebindings. auto-reconciliation of built-in roles when the kubernetes api server starts up, it automatically installs a number of default clusterroles that are defined in the code of the api server itself. this means that if you modify any built-in cluster role, those modifications are transient. whenever the api server is restarted (e.g., for an upgrade) your changes will be overwritten. to prevent this from happening, before you make any other modifications you need to add the rbac.authorization.kubernetes.io/autoupdate annotation with a value of false to the built-in clusterrole resource. if this annotation is set to false, the api server will not overwrite the modified clusterrole resource. by default, the kubernetes api server installs a cluster role that allows system:unauthenticated users access to the api server’s api discovery endpoint. for any cluster exposed to a hostile envi‐ ronment (e.g., the public internet) this is a bad idea, and there has been at least one serious security vulnerability via this exposure. consequently, if you are running a kubernetes service on the pub‐ lic internet or an other hostile environment, you should ensure that the --anonymous-auth=false flag is set on your api server. role-based access control | 171 techniques for managing rbac managing rbac for a cluster can be complicated and frustrating. possibly more con‐ cerning is that misconfigured rbac can lead to security issues. fortunately, there are several tools and techniques that make managing rbac easier. testing authorization with can-i the first useful tool is the auth can-i command for kubectl. this tool is very useful for testing if a particular user can do a particular action. you can use can-i to validate configuration settings as you configure your cluster, or you can ask users to use the tool to validate their access when filing errors or bug reports. in its simplest usage, the can-i command takes a verb and a resource. for example, this command will indicate if the current kubectl user is authorized to create pods: $ kubectl auth can-i create pods you can also test subresources like logs or port forwarding with the --subresource command-line flag: $ kubectl auth can-i get pods --subresource=logs managing rbac in source control like all resources in kubernetes, rbac resources are modeled using json or yaml. given this text-based representation it makes sense to store these resources in version control. indeed, the strong need for audit, accountability, and rollback for changes to rbac policy means that version control for rbac resources is essential. fortunately, the kubectl command-line tool comes with a reconcile command that operates somewhat like kubectl apply and will reconcile a text-based set of roles and role bindings with the current state of the cluster. you can run: $ kubectl auth reconcile -f some-rbac-config.yaml and the data in the file will be reconciled with the cluster. if you want to see changes before they are made, you can add the --dry-run flag to the command to print but not submit the changes. advanced topics once you orient to the basics of role-based access control it is relatively easy to man‐ age access to a kubernetes cluster, but when managing a large number of users or 172 | chapter 14: role-based access control for kubernetes roles, there are additional advanced capabilities you can use to manage rbac at scale. aggregating clusterroles sometimes you want to be able to define roles that are combinations of other roles. one option would be to simply clone all of the rules from one clusterrole into another clusterrole, but this is complicated and error-prone, since changes to one clusterrole aren’t automatically reflected in the other. instead, kubernetes rbac supports the usage of an aggregation rule to combine multiple roles together in a new role. this new role combines all of the capabilities of all of the aggregate roles together, and any changes to any of the constituent subroles will automatically be propogated back into the aggregate role. like with all other aggregations or groupings in kubernetes, the clusterroles to be aggregated are specified using label selectors. in this particular case, the aggregation rule field in the clusterrole resource contains a clusterroleselector field, which in turn is a label selector. all clusterrole resources that match this selector are dynamically aggregated into the rules array in the aggregate clusterrole resource. a best practice for managing clusterrole resources is to create a number of finegrained cluster roles and then aggregate them together to form higher-level or broadly defined cluster roles. this is how the built-in cluster roles are defined. for example, you can see that the built-in edit role looks like this: apiversion: rbac.authorization.k8s.io/v1 kind: clusterrole metadata:  name: edit  ... aggregationrule:  clusterroleselectors:  - matchlabels:  rbac.authorization.k8s.io/aggregate-to-edit: \"true\" ... this means that the edit role is defined to be the aggregate of all clusterrole objects that have a label of rbac.authorization.k8s.io/aggregate-to-edit set to true. using groups for bindings when managing a large number of people in different organizations with similar access to the cluster, it’s generally a best practice to use groups to manage the roles that define access to the cluster, rather than individually adding bindings to specific identities. when you bind a group to a clusterrole or a namespace role, anyone advanced topics | 173 who is a member of that group gains access to the resources and verbs defined by that role. thus, to enable any individual to gain access to the group’s role, that individual needs to be added to the group. there are several reasons why using groups is a preferred strategy for managing access at scale. the first is that in any large organization, access to the cluster is defined in terms of the team that someone is part of, rather than their specific iden‐ tity. for example, someone who is part of the frontend operations team will need access to both view and edit the resources associated with the frontends, while they may only need view/read access to resources associated with the backend. granting privileges to a group makes the association between the specific team and its capabili‐ ties clear. when granting roles to individuals, it’s much harder to clearly understand the appropriate (i.e., minimal) privileges required for each team, especially when an individual may be part of multiple different teams. additional benefits of binding roles to groups instead of individuals are simplicity and consistency. when someone joins or leaves a team, it is straightforward to simply add or remove them to or from a group in a single operation. if you instead have to remove a number of different role bindings for their identity, you may either remove too few or too many bindings, resulting in unnecessary access or preventing them from being able to do necessary actions. additionally, because there is only a single set of group role bindings to maintain, you don’t have to do lots of work to ensure that all team members have the same, consistent set of permissions. furthermore, many group systems enable “just in time” (jit) access such that people are only temporarily added to a group in response to an event (say, a page in the mid‐ dle of the night) rather than having standing access. this means that you can both audit who had access at any particular time and ensure that, in general, even a com‐ promised identity can’t have access to your production infrastructure. finally, in many cases these same groups are used to manage access to other resour‐ ces, from facilities to documents and machine logins. thus, using the same groups for access control to kubernetes dramatically simplifies management. to bind a group to a clusterrole you use a group kind for the subject in the binding: ... subjects: - apigroup: rbac.authorization.k8s.io  kind: group  name: my-great-groups-name ... in kubernetes, groups are supplied by authentication providers. there is no strong notion of a group within kubernetes, only that an identity can be part of one or more groups, and those groups can be associated with a role or clusterrole via a binding. 174 | chapter 14: role-based access control for kubernetes summary when you begin with a small cluster and a small team, it is sufficient to have every member of the team have equivalent access to the cluster. but as teams grow and products become more mission critical, limiting access to parts of the cluster is cru‐ cial. in a well-designed cluster, access is limited to the minimal set of people and capabilities needed to efficiently manage the applications in the cluster. understand‐ ing how kubernetes implements rbac and how those capabilities can be used to control access to your cluster is important for both developers and cluster adminis‐ trators. as with building out testing infrastructure, best practice is to set up proper rbac earlier rather than later. it’s far easier to start with the right foundation than to try to retrofit it later on. hopefully, the information in this chapter has provided the necessary grounding for adding rbac to your cluster. summary | 175  chapter 15 integrating storage solutions and kubernetes in many cases, decoupling state from applications and building your microservices to be as stateless as possible results in maximally reliable, manageable systems. however, nearly every system that has any complexity has state in the system some‐ where, from the records in a database to the index shards that serve results for a web search engine. at some point, you have to have data stored somewhere. integrating this data with containers and container orchestration solutions is often the most complicated aspect of building a distributed system. this complexity largely stems from the fact that the move to containerized architectures is also a move toward decoupled, immutable, and declarative application development. these pat‐ terns are relatively easy to apply to stateless web applications, but even “cloud-native” storage solutions like cassandra or mongodb involve some sort of manual or imper‐ ative steps to set up a reliable, replicated solution. as an example of this, consider setting up a replicaset in mongodb, which involves deploying the mongo daemon and then running an imperative command to identify the leader, as well as the participants in the mongo cluster. of course, these steps can be scripted, but in a containerized world it is difficult to see how to integrate such commands into a deployment. likewise, even getting dns-resolvable names for indi‐ vidual containers in a replicated set of containers is challenging. additional complexity comes from the fact that there is data gravity. most container‐ ized systems aren’t built in a vacuum; they are usually adapted from existing systems deployed onto vms, and these systems likely include data that has to be imported or migrated. 177 finally, evolution to the cloud often means that storage is an externalized cloud ser‐ vice, and in that context it can never really exist inside of the kubernetes cluster. this chapter covers a variety of approaches for integrating storage into containerized microservices in kubernetes. first, we cover how to import existing external storage solutions (either cloud services or running on vms) into kubernetes. next, we explore how to run reliable singletons inside of kubernetes that enable you to have an environment that largely matches the vms where you previously deployed storage solutions. finally, we cover statefulsets, which are still under development but repre‐ sent the future of stateful workloads in kubernetes. importing external services in many cases, you have an existing machine running in your network that has some sort of database running on it. in this situation you may not want to immediately move that database into containers and kubernetes. perhaps it is run by a different team, or you are doing a gradual move, or the task of migrating the data is simply more trouble than it’s worth. regardless of the reasons for staying put, this legacy server and service are not going to move into kubernetes—but it’s still worthwhile to represent this server in kuber‐ netes. when you do this, you get to take advantage of all of the built-in naming and service-discovery primitives provided by kubernetes. additionally, this enables you to configure all your applications so that it looks like the database that is running on a machine somewhere is actually a kubernetes service. this means that it is trivial to replace it with a database that is a kubernetes service. for example, in production, you may rely on your legacy database that is running on a machine, but for continu‐ ous testing you may deploy a test database as a transient container. since it is created and destroyed for each test run, data persistence isn’t important in the continuous testing case. representing both databases as kubernetes services enables you to main‐ tain identical configurations in both testing and production. high fidelity between test and production ensures that passing tests will lead to successful deployment in production. to see concretely how you maintain high fidelity between development and produc‐ tion, remember that all kubernetes objects are deployed into namespaces. imagine that we have test and production namespaces defined. the test service is imported using an object like: kind: service metadata:  name: my-database  # note \\'test\\' namespace here  namespace: test ... 178 | chapter 15: integrating storage solutions and kubernetes the production service looks the same, except it uses a different namespace: kind: service metadata:  name: my-database  # note \\'prod\\' namespace here  namespace: prod ... when you deploy a pod into the test namespace and it looks up the service named my-database, it will receive a pointer to my-database.test.svc.cluster.internal, which in turn points to the test database. in contrast, when a pod deployed in the prod namespace looks up the same name (my-database) it will receive a pointer to my-database.prod.svc.cluster.internal, which is the production database. thus, the same service name, in two different namespaces, resolves to two different serv‐ ices. for more details on how this works, see chapter 7. the following techniques all use database or other storage services, but these approaches can be used equally well with other services that aren’t running inside your kubernetes cluster. services without selectors when we first introduced services, we talked at length about label queries and how they were used to identify the dynamic set of pods that were the backends for a par‐ ticular service. with external services, however, there is no such label query. instead, you generally have a dns name that points to the specific server running the data‐ base. for our example, let’s assume that this server is named database.company.com. to import this external database service into kubernetes, we start by creating a ser‐ vice without a pod selector that references the dns name of the database server (example 15-1). example 15-1. dns-service.yaml kind: service apiversion: v1 metadata:  name: external-database spec:  type: externalname  externalname: database.company.com when a typical kubernetes service is created, an ip address is also created and the kubernetes dns service is populated with an a record that points to that ip address. when you create a service of type externalname, the kubernetes dns service is importing external services | 179 instead populated with a cname record that points to the external name you speci‐ fied (database.company.com in this case). when an application in the cluster does a dns lookup for the hostname external-database.svc.default.cluster, the dns protocol aliases that name to database.company.com. this then resolves to the ip address of your external database server. in this way, all containers in kubernetes believe that they are talking to a service that is backed with other containers, when in fact they are being redirected to the external database. note that this is not restricted to databases you are running on your own infrastruc‐ ture. many cloud databases and other services provide you with a dns name to use when accessing the database (e.g., my-database.databases.cloudprovider.com). you can use this dns name as the externalname. this imports the cloud-provided database into the namespace of your kubernetes cluster. sometimes, however, you don’t have a dns address for an external database service, just an ip address. in such cases, it is still possible to import this service as a kuber‐ netes service, but the operation is a little different. first, you create a service without a label selector, but also without the externalname type we used before (example 15-2). example 15-2. external-ip-service.yaml kind: service apiversion: v1 metadata:  name: external-ip-database at this point, kubernetes will allocate a virtual ip address for this service and popu‐ late an a record for it. however, because there is no selector for the service, there will be no endpoints populated for the load balancer to redirect traffic to. given that this is an external service, the user is responsible for populating the end‐ points manually with an endpoints resource (example 15-3). example 15-3. external-ip-endpoints.yaml kind: endpoints apiversion: v1 metadata:  name: external-ip-database subsets:  - addresses:  - ip: 192.168.0.1  ports:  - port: 3306 180 | chapter 15: integrating storage solutions and kubernetes if you have more than one ip address for redundancy, you can repeat them in the addresses array. once the endpoints are populated, the load balancer will start redi‐ recting traffic from your kubernetes service to the ip address endpoint(s). because the user has assumed responsibility for keeping the ip address of the server up to date, you need to either ensure that it never changes or make sure that some automated process updates the endpoints record. limitations of external services: health checking external services in kubernetes have one significant restriction: they do not perform any health checking. the user is responsible for ensuring that the endpoint or dns name supplied to kubernetes is as reliable as necessary for the application. running reliable singletons the challenge of running storage solutions in kubernetes is often that primitives like replicaset expect that every container is identical and replaceable, but for most stor‐ age solutions this isn’t the case. one option to address this is to use kubernetes primi‐ tives, but not attempt to replicate the storage. instead, simply run a single pod that runs the database or other storage solution. in this way the challenges of running replicated storage in kubernetes don’t occur, since there is no replication. at first blush, this might seem to run counter to the principles of building reliable distributed systems, but in general, it is no less reliable than running your database or storage infrastructure on a single virtual or physical machine, which is how many people currently have built their systems. indeed, in reality, if you structure the sys‐ tem properly the only thing you are sacrificing is potential downtime for upgrades or in case of machine failure. while for large-scale or mission-critical systems this may not be acceptable, for many smaller-scale applications this kind of limited downtime is a reasonable trade-off for the reduced complexity. if this is not true for you, feel free to skip this section and either import existing services as described in the previ‐ ous section, or move on to kubernetes-native statefulsets, described in the following section. for everyone else, we’ll review how to build reliable singletons for data storage. running a mysql singleton in this section, we’ll describe how to run a reliable singleton instance of the mysql database as a pod in kubernetes, and how to expose that singleton to other applica‐ tions in the cluster. to do this, we are going to create three basic objects: running reliable singletons | 181 • a persistent volume to manage the lifespan of the on-disk storage independently from the lifespan of the running mysql application • a mysql pod that will run the mysql application • a service that will expose this pod to other containers in the cluster in chapter 5 we described persistent volumes, but a quick review makes sense. a per‐ sistent volume is a storage location that has a lifetime independent of any pod or con‐ tainer. this is very useful in the case of persistent storage solutions where the on-disk representation of a database should survive even if the containers running the data‐ base application crash, or move to different machines. if the application moves to a different machine, the volume should move with it, and data should be preserved. separating the data storage out as a persistent volume makes this possible. to begin, we’ll create a persistent volume for our mysql database to use. this exam‐ ple uses nfs for maximum portability, but kubernetes supports many different per‐ sistent volume drive types. for example, there are persistent volume drivers for all major public cloud providers, as well as many private cloud providers. to use these solutions, simply replace nfs with the appropriate cloud provider volume type (e.g., azure, awselasticblockstore, or gcepersistentdisk). in all cases, this change is all you need. kubernetes knows how to create the appropriate storage disk in the respec‐ tive cloud provider. this is a great example of how kubernetes simplifies the develop‐ ment of reliable distributed systems. example 15-4 shows the persistentvolume object. example 15-4. nfs-volume.yaml apiversion: v1 kind: persistentvolume metadata:  name: database  labels:  volume: my-volume spec:  accessmodes:  - readwritemany  capacity:  storage: 1gi  nfs:  server: 192.168.0.1  path: \"/exports\" this defines an nfs persistentvolume object with 1 gb of storage space. we can create this persistent volume as usual with: $ kubectl apply -f nfs-volume.yaml 182 | chapter 15: integrating storage solutions and kubernetes now that we have a persistent volume created, we need to claim that persistent vol‐ ume for our pod. we do this with a persistentvolumeclaim object (example 15-5). example 15-5. nfs-volume-claim.yaml kind: persistentvolumeclaim apiversion: v1 metadata:  name: database spec:  accessmodes:  - readwritemany  resources:  requests:  storage: 1gi  selector:  matchlabels:  volume: my-volume the selector field uses labels to find the matching volume we defined previously. this kind of indirection may seem overly complicated, but it has a purpose—it serves to isolate our pod definition from our storage definition. you can declare volumes directly inside a pod specification, but this locks that pod specification to a particular volume provider (e.g., a specific public or private cloud). by using volume claims, you can keep your pod specifications cloud-agnostic; simply create different volumes, specific to the cloud, and use a persistentvolumeclaim to bind them together. fur‐ thermore, in many cases, the persistent volume controller will actually automatically create a volume for you—there are more details of this process in the following section. now that we’ve claimed our volume, we can use a replicaset to construct our single‐ ton pod. it might seem odd that we are using a replicaset to manage a single pod, but it is necessary for reliability. remember that once scheduled to a machine, a bare pod is bound to that machine forever. if the machine fails, then any pods that are on that machine that are not being managed by a higher-level controller like a replicaset vanish along with the machine and are not rescheduled elsewhere. consequently, to ensure that our database pod is rescheduled in the presence of machine failures, we use the higher-level replicaset controller, with a replica size of one, to manage our database (example 15-6). example 15-6. mysql-replicaset.yaml apiversion: extensions/v1 kind: replicaset metadata:  name: mysql running reliable singletons | 183  # labels so that we can bind a service to this pod  labels:  app: mysql spec:  replicas: 1  selector:  matchlabels:  app: mysql  template:  metadata:  labels:  app: mysql  spec:  containers:  - name: database  image: mysql  resources:  requests:  cpu: 1  memory: 2gi  env:  # environment variables are not a best practice for security,  # but we\\'re using them here for brevity in the example.  # see chapter 11 for better options.  - name: mysqlrootpassword  value: some-password-here  livenessprobe:  tcpsocket:  port: 3306  ports:  - containerport: 3306  volumemounts:  - name: database  # /var/lib/mysql is where mysql stores its databases  mountpath: \"/var/lib/mysql\"  volumes:  - name: database  persistentvolumeclaim:  claimname: database once we create the replicaset it will, in turn, create a pod running mysql using the persistent disk we originally created. the final step is to expose this as a kubernetes service (example 15-7). example 15-7. mysql-service.yaml apiversion: v1 kind: service metadata:  name: mysql spec: 184 | chapter 15: integrating storage solutions and kubernetes  ports:  - port: 3306  protocol: tcp  selector:  app: mysql now we have a reliable singleton mysql instance running in our cluster and exposed as a service named mysql, which we can access at the full domain name mysql.svc.default.cluster. similar instructions can be used for a variety of data stores, and if your needs are sim‐ ple and you can survive limited downtime in the face of a machine failure or when you need to upgrade the database software, a reliable singleton may be the right approach to storage for your application. dynamic volume provisioning many clusters also include dynamic volume provisioning. with dynamic volume pro‐ visioning, the cluster operator creates one or more storageclass objects. example 15-8 shows a default storage class that automatically provisions disk objects on the microsoft azure platform. example 15-8. storageclass.yaml apiversion: storage.k8s.io/v1 kind: storageclass metadata:  name: default  annotations:  storageclass.beta.kubernetes.io/is-default-class: \"true\"  labels:  kubernetes.io/cluster-service: \"true\" provisioner: kubernetes.io/azure-disk once a storage class has been created for a cluster, you can refer to this storage class in your persistent volume claim, rather than referring to any specific persistent vol‐ ume. when the dynamic provisioner sees this storage claim, it uses the appropriate volume driver to create the volume and bind it to your persistent volume claim. example 15-9 shows an example of a persistentvolumeclaim that uses the default storage class we just defined to claim a newly created persistent volume. example 15-9. dynamic-volume-claim.yaml kind: persistentvolumeclaim apiversion: v1 metadata: running reliable singletons | 185  name: my-claim  annotations:  volume.beta.kubernetes.io/storage-class: default spec:  accessmodes:  - readwriteonce  resources:  requests:  storage: 10gi the volume.beta.kubernetes.io/storage-class annotation is what links this claim back up to the storage class we created. automatic provisioning of a persistent volume is a great feature that makes it significantly easier to build and manage stateful appli‐ cations in kubernetes. however, the lifespan of these persistent volumes is dictated by the reclamation policy of the persistentvo lumeclaim and the default is to bind that lifespan to the lifespan of the pod that creates the volume. this means that if you happen to delete the pod (e.g., via a scaledown or other event), then the volume is deleted as well. while this may be what you want in certain circumstances, you need to be careful to ensure that you don’t accidentally delete your persistent volumes. persistent volumes are great for traditional applications that require storage, but if you need to develop high-availability, scalable storage in a kubernetes-native fashion, the newly released statefulset object can be used instead. we’ll describe how to deploy mongodb using statefulsets in the next section. kubernetes-native storage with statefulsets when kubernetes was first developed, there was a heavy emphasis on homogeneity for all replicas in a replicated set. in this design, no replica had an individual identity or configuration. it was up to the application developer to determine a design that could establish this identity for their application. while this approach provides a great deal of isolation for the orchestration system, it also makes it quite difficult to develop stateful applications. after significant input from the community and a great deal of experimentation with various existing state‐ ful applications, statefulsets were introduced in kubernetes version 1.5. 186 | chapter 15: integrating storage solutions and kubernetes properties of statefulsets statefulsets are replicated groups of pods, similar to replicasets. but unlike a replica‐ set, they have certain unique properties: • each replica gets a persistent hostname with a unique index (e.g., database-0, database-1, etc.). • each replica is created in order from lowest to highest index, and creation will block until the pod at the previous index is healthy and available. this also applies to scaling up. • when a statefulset is deleted, each of the managed replica pods is also deleted in order from highest to lowest. this also applies to scaling down the number of replicas. it turns out that this simple set of requirements makes it drastically easier to deploy storage applications on kubernetes. for example, the combination of stable host‐ names (e.g., database-0) and the ordering constraints mean that all replicas, other than the first one, can reliably reference database-0 for the purposes of discovery and establishing replication quorum. manually replicated mongodb with statefulsets in this section, we’ll deploy a replicated mongodb cluster. for now, the replication setup itself will be done manually to give you a feel for how statefulsets work. even‐ tually we will automate this setup as well. to start, we’ll create a replicated set of three mongodb pods using a statefulset object (example 15-10). example 15-10. mongo-simple.yaml apiversion: apps/v1 kind: statefulset metadata:  name: mongo spec:  servicename: \"mongo\"  replicas: 3  template:  metadata:  labels:  app: mongo  spec:  containers:  - name: mongodb  image: mongo:3.4.1 kubernetes-native storage with statefulsets | 187  command:  - mongod  - --replset  - rs0  ports:  - containerport: 27017  name: peer as you can see, the definition is similar to the replicaset definitions we’ve seen previ‐ ously. the only changes are in the apiversion and kind fields. create the statefulset: $ kubectl apply -f mongo-simple.yaml once created, the differences between a replicaset and a statefulset become appa‐ rent. run kubectl get pods and you will likely see: name ready status restarts age mongo-0 1/1 running 0 1m mongo-1 0/1 containercreating 0 10s there are two important differences between this and what you would see with a replicaset. the first is that each replicated pod has a numeric index (0, 1, …), instead of the random suffix that is added by the replicaset controller. the second is that the pods are being slowly created in order, not all at once as they would be with a replicaset. once the statefulset is created, we also need to create a “headless” service to manage the dns entries for the statefulset. in kubernetes a service is called “headless” if it doesn’t have a cluster virtual ip address. since with statefulsets each pod has a unique identity, it doesn’t really make sense to have a load-balancing ip address for the repli‐ cated service. you can create a headless service using clusterip: none in the service specification (example 15-11). example 15-11. mongo-service.yaml apiversion: v1 kind: service metadata:  name: mongo spec:  ports:  - port: 27017  name: peer  clusterip: none  selector:  app: mongo 188 | chapter 15: integrating storage solutions and kubernetes once you create that service, there are usually four dns entries that are populated. as usual, mongo.default.svc.cluster.local is created, but unlike with a standard service, doing a dns lookup on this hostname provides all the addresses in the state‐ fulset. in addition, entries are created for mongo-0.mongo.default.svc.cluster .local as well as mongo-1.mongo and mongo-2.mongo. each of these resolves to the specific ip address of the replica index in the statefulset. thus, with statefulsets you get well-defined, persistent names for each replica in the set. this is often very useful when you are configuring a replicated storage solution. you can see these dns entries in action by running the following commands in one of the mongo replicas: $ kubectl run -it --rm --image busybox busybox ping mongo-1.mongo next, we’re going to manually set up mongo replication using these per-pod hostnames. we’ll choose mongo-0.mongo to be our initial primary. run the mongo tool in that pod: $ kubectl exec -it mongo-0 mongo > rs.initiate( {  id: \"rs0\",  members:  });  ok this command tells mongodb to initiate the replicaset rs0 with mongo-0.mongo as the primary replica. the rs0 name is arbitrary. you can use whatever you’d like, but you’ll need to change it in the mongo.yaml statefulset definition as well. once you have initiated the mongo replicaset, you can add the remaining replicas by running the following commands in the mongo tool on the mongo-0.mongo pod: > rs.add(\"mongo-1.mongo:27017\"); > rs.add(\"mongo-2.mongo:27017\"); as you can see, we are using the replica-specific dns names to add them as replicas in our mongo cluster. at this point, we’re done. our replicated mongodb is up and running. but it’s really not as automated as we’d like it to be—in the next section, we’ll see how to use scripts to automate the setup. automating mongodb cluster creation to automate the deployment of our statefulset-based mongodb cluster, we’re going to add an additional container to our pods to perform the initialization. kubernetes-native storage with statefulsets | 189 to configure this pod without having to build a new docker image, we’re going to use a configmap to add a script into the existing mongodb image. here’s the container we’re adding: ...  - name: init-mongo  image: mongo:3.4.1  command:  - bash  - /config/init.sh  volumemounts:  - name: config  mountpath: /config  volumes:  - name: config  configmap:  name: \"mongo-init\" note that it is mounting a configmap volume whose name is mongo-init. this con‐ figmap holds a script that performs our initialization. first, the script determines whether it is running on mongo-0 or not. if it is on mongo-0, it creates the replicaset using the same command we ran imperatively previously. if it is on a different mongo replica, it waits until the replicaset exists, and then it registers itself as a member of that replicaset. example 15-12 has the complete configmap object. example 15-12. mongo-configmap.yaml apiversion: v1 kind: configmap metadata:  name: mongo-init data:  init.sh: |  #!/bin/bash  # need to wait for the readiness health check to pass so that the  # mongo names resolve. this is kind of wonky.  until ping -c 1 ${hostname}.mongo; do  echo \"waiting for dns (${hostname}.mongo)...\"  sleep 2  done  until /usr/bin/mongo --eval \\'printjson(db.serverstatus())\\'; do  echo \"connecting to local mongo...\"  sleep 2  done  echo \"connected to local.\" 190 | chapter 15: integrating storage solutions and kubernetes  host=mongo-0.mongo:27017  until /usr/bin/mongo --host=${host} --eval \\'printjson(db.serverstatus())\\'; do  echo \"connecting to remote mongo...\"  sleep 2  done  echo \"connected to remote.\"  if ]; then  until /usr/bin/mongo --host=${host} --eval=\"printjson(rs.status())\" \\\\  | grep -v \"no replset config has been received\"; do  echo \"waiting for replication set initialization\"  sleep 2  done  echo \"adding self to mongo-0\"  /usr/bin/mongo --host=${host} \\\\  --eval=\"printjson(rs.add(\\'${hostname}.mongo\\'))\"  fi  if ]; then  echo \"initializing replica set\"  /usr/bin/mongo --eval=\"printjson(rs.initiate(\\\\  {\\'id\\': \\'rs0\\', \\'members\\': [{\\'id\\': 0, \\\\  \\'host\\': \\'mongo-0.mongo:27017\\'}]}))\"  fi  echo \"initialized\"  while true; do  sleep 3600  done this script currently sleeps forever after initializing the cluster. every container in a pod has to have the same restartpolicy. since we do not want our main mongo container to be restarted, we need to have our initialization container run forever too, or else kubernetes might think our mongo pod is unhealthy. putting it all together, example 15-13 is the complete statefulset that uses the configmap. example 15-13. mongo.yaml apiversion: apps/v1 kind: statefulset metadata:  name: mongo spec:  servicename: \"mongo\"  replicas: 3  template: kubernetes-native storage with statefulsets | 191  metadata:  labels:  app: mongo  spec:  containers:  - name: mongodb  image: mongo:3.4.1  command:  - mongod  - --replset  - rs0  ports:  - containerport: 27017  name: web  # this container initializes the mongodb server, then sleeps.  - name: init-mongo  image: mongo:3.4.1  command:  - bash  - /config/init.sh  volumemounts:  - name: config  mountpath: /config  volumes:  - name: config  configmap:  name: \"mongo-init\" given all of these files, you can create a mongo cluster with: $ kubectl apply -f mongo-config-map.yaml $ kubectl apply -f mongo-service.yaml $ kubectl apply -f mongo.yaml or if you want, you can combine them all into a single yaml file where the individ‐ ual objects are separated by ---. ensure that you keep the same ordering, since the statefulset definition relies on the configmap definition existing. persistent volumes and statefulsets for persistent storage, you need to mount a persistent volume into the /data/db direc‐ tory. in the pod template, you need to update it to mount a persistent volume claim to that directory: ...  volumemounts:  - name: database  mountpath: /data/db while this approach is similar to the one we saw with reliable singletons, because the statefulset replicates more than one pod you cannot simply reference a persistent 192 | chapter 15: integrating storage solutions and kubernetes volume claim. instead, you need to add a persistent volume claim template. you can think of the claim template as being identical to the pod template, but instead of cre‐ ating pods, it creates volume claims. you need to add the following onto the bottom of your statefulset definition:  volumeclaimtemplates:  - metadata:  name: database  annotations:  volume.alpha.kubernetes.io/storage-class: anything  spec:  accessmodes:   resources:  requests:  storage: 100gi when you add a volume claim template to a statefulset definition, each time the statefulset controller creates a pod that is part of the statefulset it will create a persis‐ tent volume claim based on this template as part of that pod. in order for these replicated persistent volumes to work correctly, you either need to have autoprovisioning set up for persistent vol‐ umes, or you need to prepopulate a collection of persistent volume objects for the statefulset controller to draw from. if there are no claims that can be created, the statefulset controller will not be able to create the corresponding pods. one final thing: readiness probes the final piece in productionizing our mongodb cluster is to add liveness checks to our mongo-serving containers. as we learned in “health checks” on page 54, the liveness probe is used to determine if a container is operating correctly. for the liven‐ ess checks, we can use the mongo tool itself by adding the following to the pod tem‐ plate in the statefulset object: ...  livenessprobe:  exec:  command:  - /usr/bin/mongo  - --eval  - db.serverstatus()  initialdelayseconds: 10  timeoutseconds: 10  ... kubernetes-native storage with statefulsets | 193 summary once we have combined statefulsets, persistent volume claims, and liveness probing, we have a hardened, scalable cloud-native mongodb installation running on kuber‐ netes. while this example dealt with mongodb, the steps for creating statefulsets to manage other storage solutions are quite similar and similar patterns can be followed. 194 | chapter 15: integrating storage solutions and kubernetes chapter 16 extending kubernetes from the beginning, it was clear that kubernetes was going to be more than its core set of apis; once an application is orchestrated within the cluster, there are countless other useful tools and utilities that can be represented and deployed as api objects in the kubernetes cluster. the challenge was how to embrace this explosion of objects and use cases without having an api that sprawled without bound. to resolve this tension between extended use cases and api sprawl, significant effort was put into making the kubernetes api extensible. this extensibility meant that cluster operators could customize their clusters with the additional components that suited their needs. this extensibility enables people to augment their clusters them‐ selves, consume community-developed cluster add-ons, and even develop extensions that are bundled and sold in an ecosystem of cluster plug-ins. extensibility has also given rise to whole new patterns of managing systems, such as the operator pattern. regardless of whether you are building your own extensions or consuming operators from the ecosystem, understanding how the kubernetes api server is extended and how extensions can be built and delivered is a key component to unlocking the com‐ plete power of kubernetes and its ecosystem. as more and more advanced tools and platforms are built on top of kubernetes using these extensibility mechanisms, a working knowledge of how they operate is critical to understanding how to build applications in a modern kubernetes cluster. what it means to extend kubernetes in general, extensions to the kubernetes api server either add new functionality to a cluster or limit and tweak the ways that users can interact with their clusters. there is a rich ecosystem of plug-ins that cluster administrators can use to add additional services and capabilities to their clusters. it’s worth noting that extending the cluster 195 is a very high-privilege thing to do. it is not a capability that should be extended to arbitrary users or arbitrary code, because cluster administrator privileges are required to extend a cluster. even cluster administrators should be careful and use diligence when installing third-party tools. some extensions, like admission controllers, can be used to view all objects being created in the cluster, and could easily be used as a vec‐ tor to steal secrets or run malicious code. additionally, extending a cluster makes it different than stock kubernetes. when running on multiple clusters, it is very valua‐ ble to build tooling to maintain consistency of experience across the clusters, and this includes the extensions that are installed. points of extensibility there are many different ways to extend kubernetes, from customresourcedefini‐ tions through to container network interface (cni) plug-ins. this chapter is going to focus on the extensions to the api server via adding new resource types or admis‐ sion controllers to api requests. we will not cover the cni/csi/cri (container net‐ work interface/container storage interface/container runtime interface) extensions, as they are more commonly used by kubernetes cluster providers as opposed to the end users of kubernetes, for whom this book was written. in addition to admission controllers and api extensions, there are actually a number of ways to “extend” your cluster without ever modifying the api server at all. these include daemonsets that install automatic logging and monitoring, tools that scan your services for cross-site scripting (xss) vulnerabilities, and more. before embark‐ ing on extending your cluster yourself, however, it’s worth considering the landscape of things that are possible within the confines of the existing kubernetes apis. to help understand the role of admission controllers and customresourcedefini‐ tions, it is very helpful to understand the flow of requests through the kubernetes api server, which is shown in figure 16-1. figure 16-1. api server request flow admission controllers are called prior to the api object being written into the back‐ ing storage. admission controllers can reject or modify api requests. there are sev‐ eral admission controllers that are built into the kubernetes api server; for example, the limit range admission controller that sets default limits for pods without them. many other systems use custom admission controllers to auto-inject sidecar contain‐ ers into all pods created on the system to enable “auto-magic” experiences. 196 | chapter 16: extending kubernetes the other form of extension, which can also be used in conjunction with admission controllers, is custom resources. with custom resources, whole new api objects are added to the kubernetes api surface area. these new api objects can be added to namespaces, are subject to rbac, and can be accessed with existing tools like kubectl as well as via the kubernetes api. the following sections describe these kubernetes extension points in greater detail and give both use cases and hands-on examples of how to extend your cluster. the first thing that you do to create a custom resource is to create a customresour‐ cedefinition. this object is actually a meta-resource; that is, a resource that is the def‐ inition of another resource. as a concrete example, consider defining a new resource to represent load tests in your cluster. when a new loadtest resource is created, a load test is spun up in your kubernetes cluster and drives traffic to a service. the first step in creating this new resource is defining it through a customresource‐ definition. an example definition looks as follows: apiversion: apiextensions.k8s.io/v1beta1 kind: customresourcedefinition metadata:  name: loadtests.beta.kuar.com spec:  group: beta.kuar.com  versions:  - name: v1  served: true  storage: true  scope: namespaced  names:  plural: loadtests  singular: loadtest  kind: loadtest  shortnames:  - lt you can see that this is a kubernetes object like any other. it has a metadata subobject, and within that subobject the resource is named. however, in the case of custom resources, the name is special. it has to be the following format: <resourceplural>.<api-group>. the reason for this is to ensure that each resource definition is unique in the cluster, because the name of each customresourcedefinition has to match this pattern, and no two objects in the cluster can have the same name. we are thus guaranteed that no two customresourcedefinitions define the same resource. in addition to metadata, the customresourcedefinition has a spec subobject. this is where the resource itself is defined. in that spec object, there is an apigroup field which supplies the api group for the resource. as mentioned previously, it must points of extensibility | 197 match the suffix of the customresourcedefinition’s name. additionally, there is a list of versions for the resource. in addition to the name of the version (e.g., v1, v2, etc.), there are fields that indicate if that version is served by the api server and which ver‐ sion is used for storing data in the backing storage for the api server. the storage field must be true for only a single version for the resource. there is also a scope field to indicate if the resource is namespaced or not (the default is namespaced), and a names field that allows for the definition of the singular, plural, and kind values for the resource. it also allows the definition of convenience “short names” for the resource for use in kubectl and elsewhere. given this definition, you can create the resource in the kubernetes api server. but first, to show the true nature of dynamic resource types, try to list our loadtests resource using kubectl: $ kubectl get loadtests you’ll see that there is no such resource currently defined. now use loadtest-resource.yaml to create this resource: $ kubectl create -f loadtest-resource.yaml then get the loadtests resource again: $ kubectl get loadtests this time you’ll see that there is a loadtest resource type defined, though there are still no instances of this resource type. let’s change that by creating a new loadtest resource. as with all built-in kubernetes api objects, you can use yaml or json to define a custom resource (in this case our loadtest). see the following definition: apiversion: beta.kuar.com/v1 kind: loadtest metadata:  name: my-loadtest spec:  service: my-service  scheme: https  requestspersecond: 1000  paths:  - /index.html  - /login.html  - /shares/my-shares/ one thing that you’ll note is that we never defined the schema for the custom resource in the customresourcedefinition. it actually is possible to provide an openapi specification for a custom resource, but this complexity is generally not 198 | chapter 16: extending kubernetes worth it for simple resource types. if you do want to perform validation, you can reg‐ ister a validating admission controller, as described in the following sections. you can now use this loadtest.yaml file to create a resource just like you would with any built-in type: $ kubectl create -f loadtest.yaml now when you list the loadtests resource, you’ll see your newly created resource: $ kubectl get loadtests this may be exciting, but it doesn’t really do anything yet. sure, you can use this sim‐ ple crud (create/read/update/delete) api to manipulate the data for loadtest objects, but no actual load tests are created in response to this new api we defined. this is because there is no controller present in the cluster to react and take action when a loadtest object is defined. the loadtest custom resource is only half of the infrastructure needed to add loadtests to our cluster. the other is a piece of code that will continuously monitor the custom resources and create, modify, or delete loadtests as necessary to implement the api. just like the user of the api, the controller interacts with the api server to list loadtests and watches for any changes that might occur. this interaction between controller and api server is shown in figure 16-2. figure 16-2. customresourcedefinition interactions the code for such a controller can range from simple to complex. the simplest con‐ trollers run a for loop and repeatedly poll for new custom objects, and then take actions to create or delete the resources that implement those custom objects (e.g., the loadtest worker pods). however, this polling-based approach is inefficient: the period of the polling loop adds unnecessary latency, and the overhead of polling may add unnecessary load on the api server. a more efficient approach is to use the watch api on the api server, which provides a stream of updates when they occur, eliminating both the latency and overhead of polling. however, using this api correctly in a bug-free way is com‐ plicated. as a result, if you want to use watches, it is highly recommended that you use a well-supported mechanism such as the informer pattern exposed in the client-go library. points of extensibility | 199 now that we have created a custom resource and implemented it via a controller, we have the basic functionality of a new resource in our cluster. however, many parts of what it means to be a well-functioning resource are missing. the two most important are validation and defaulting. validation is the process of ensuring that loadtest objects sent to the api server are well formed and can be used to create load tests, while defaulting makes it easier for people to use our resources by providing auto‐ matic, commonly used values by default. we’ll now cover adding these capabilities to our custom resource. as mentioned earlier, one option for adding validation is via an openapi specifica‐ tion for our objects. this can be useful for basic validation of the presence of required fields or the absence of unknown fields. a complete openapi tutorial is beyond the scope of this book, but there are lots of resources online, including the complete kubernetes api specification. generally speaking, an api schema is actually insufficient for validation of api objects. for example, in our loadtests example, we may want to validate that the loadtest object has a valid scheme (e.g., http or https) or that requestspersecond is a nonzero positive number. to accomplish this, we will use a validating admission controller. as discussed previ‐ ously, admission controllers intercept requests to the api server before they are pro‐ cessed and can reject or modify the requests in flight. admission controllers can be added to a cluster via the dynamic admission control system. a dynamic admission controller is a simple http application. the api server connects to the admission controller via either a kubernetes service object or an arbitrary url. this means that admission controllers can optionally run outside of the cluster—for example, in a cloud provider’s function-as-a-service offering, like azure functions or aws lambda. to install our validating admission controller, we need to specify it as a kubernetes validatingwebhookconfiguration. this object specifies the endpoint where the admission controller runs, as well as the resource (in this case loadtest) and the action (in this case create) where the admission controller should be run. you can see the full definition for the validating admission controller in the following code: apiversion: admissionregistration.k8s.io/v1beta1 kind: validatingwebhookconfiguration metadata:  name: kuar-validator webhooks: - name: validator.kuar.com  rules:  - apigroups:  - \"beta.kuar.com\"  apiversions:  - v1 200 | chapter 16: extending kubernetes  operations:  - create  resources:  - loadtests  clientconfig:  # substitute the appropriate ip address for your webhook  url: https://192.168.1.233:8080  # this should be the base64-encoded ca certificate for your cluster,  # you can find it in your ${kubeconfig} file  cabundle: replaceme fortunately for security, but unfortunately for complexity, webhooks that are accessed by the kubernetes api server can only be accessed via https. this means that we need to generate a certificate to serve the webhook. the easiest way to do this is to use the cluster’s ability to generate new certificates using its own certificate authority (ca). first, we need a private key and a certificate signing request (csr). here’s a simple go program that generates these: package main import (  \"crypto/rand\"  \"crypto/rsa\"  \"crypto/x509\"  \"crypto/x509/pkix\"  \"encoding/asn1\"  \"encoding/pem\"  \"net/url\"  \"os\" ) func main() {  host := os.args  name := \"server\"  key, err := rsa.generatekey(rand.reader, 1024)  if err != nil {  panic(err)  }  keyder := x509.marshalpkcs1privatekey(key)  keyblock := pem.block{  type: \"rsa private key\",  bytes: keyder,  }  keyfile, err := os.create(name + \".key\")  if err != nil {  panic(err)  }  pem.encode(keyfile, &keyblock)  keyfile.close() points of extensibility | 201  commonname := \"myuser\"  emailaddress := \"someone@myco.com\"  org := \"my co, inc.\"  orgunit := \"widget farmers\"  city := \"seattle\"  state := \"wa\"  country := \"us\"  subject := pkix.name{  commonname: commonname,  country: string{country},  locality: string{city},  organization: string{org},  organizationalunit: string{orgunit},  province: string{state},  }  uri, err := url.parserequesturi(host)  if err != nil {  panic(err)  }  asn1, err := asn1.marshal(subject.tordnsequence())  if err != nil {  panic(err)  }  csr := x509.certificaterequest{  rawsubject: asn1,  emailaddresses: string{emailaddress},  signaturealgorithm: x509.sha256withrsa,  uris: *url.url{uri},  }  bytes, err := x509.createcertificaterequest(rand.reader, &csr, key)  if err != nil {  panic(err)  }  csrfile, err := os.create(name + \".csr\")  if err != nil {  panic(err)  }  pem.encode(csrfile, &pem.block{type: \"certificate request\", bytes: bytes})  csrfile.close() } you can run this program with: $ go run csr-gen.go <url-for-webook> and it will generate two files, server.csr and server-key.pem. 202 | chapter 16: extending kubernetes you can then create a certificate signing request for the kubernetes api server using the following yaml: apiversion: certificates.k8s.io/v1beta1 kind: certificatesigningrequest metadata:  name: validating-controller.default spec:  groups:  - system:authenticated  request: replaceme  usages:  usages:  - digital signature  - key encipherment  - key agreement  - server auth you will notice for the request field the value is replaceme; this needs to be replaced with the base64-encoded certificate signing request we produced in the preceding code: $ perl -pi -e s/replaceme/$(base64 server.csr | tr -d \\'\\\\n\\')/ \\\\ admission-controller-csr.yaml now that your certificate signing request is ready, you can send it to the api server to get the certificate: $ kubectl create -f admission-controller-csr.yaml next, you need to approve that request: $ kubectl certificate approve validating-controller.default once approved, you can download the new certificate: $ kubectl get csr validating-controller.default -o json | \\\\  jq -r .status.certificate | base64 -d > server.crt with the certificate, you are finally ready to create an ssl-based admission controller (phew!). when the admission controller code receives a request, it contains an object of type admissionreview, which contains metadata about the request as well as the body of the request itself. in our validating admission controller we have only regis‐ tered for a single resource type and a single action (create), so we don’t need to examine the request metadata. instead, we dive directly into the resource itself and validate that requestspersecond is positive and the url scheme is valid. if they aren’t, we return a json body disallowing the request. implementing an admission controller to provide defaulting is similar to the steps just described, but instead of using a validatingwebhookconfiguration you use a mutatingwebhookconfiguration, and you need to provide a jsonpatch object to mutate the request object before it is stored. points of extensibility | 203 here’s a typescript snippet that you can add to your validating admission controller to add defaulting. if the paths field in the loadtest is of length zero, add a single path for /index.html:  if (needspatch(loadtest)) {  const patch = [  { \\'op\\': \\'add\\', \\'path\\': \\'/spec/paths\\', \\'value\\':  },  ]  response = buffer.from(json.stringify(patch))  .tostring(\\'base64\\');  response = \\'jsonpatch\\';  } you can then register this webhook as a mutatingwebhookconfiguration by simply changing the kind field in the yaml object and saving the file as mutatingcontroller.yaml. then create the controller by running: $ kubectl create -f mutating-controller.yaml at this point you’ve seen a complete example of how to extend the kubernetes api server using custom resources and admission controllers. the following section describes some general patterns for various extensions. patterns for custom resources not all custom resources are identical. there are a variety of different reasons for extending the kubernetes api surface area, and the following sections discuss some general patterns you may want to consider. just data the easiest pattern for api extension is the notion of “just data.” in this pattern, you are simply using the api server for storage and retrieval of information for your application. it is important to note that you should not use the kubernetes api server for application data storage. the kubernetes api server is not designed to be a key/ value store for your app; instead, api extensions should be control or configuration objects that help you manage the deployment or runtime of your application. an example use case for the “just data” pattern might be configuration for canary deploy‐ ments of your application—for example, directing 10% of all traffic to an experimen‐ tal backend. while in theory such configuration information could also be stored in a configmap, configmaps are essentially untyped, and sometimes using a more strongly typed api extension object provides clarity and ease of use. extensions that are just data don’t need a corresponding controller to activate them, but they may have validating or mutating admission controllers to ensure that they are well formed. for example, in the canary use case a validating controller might ensure that all percentages in the canary object sum to 100%. 204 | chapter 16: extending kubernetes compilers a slightly more complicated pattern is the “compiler” or “abstraction” pattern. in this pattern the api extension object represents a higher-level abstraction that is “com‐ piled” into a combination of lower-level kubernetes objects. the loadtest extension in the previous example is an example of this compiler abstraction pattern. a user consumes the extension as a high-level concept, in this case a loadtest, but it comes into being by being deployed as a collection of kubernetes pods and services. to ach‐ ieve this, a compiled abstraction requires an api controller to be running somewhere in the cluster, to watch the current loadtests and create the “compiled” representa‐ tion (and likewise delete representations that no longer exist). in contrast to the oper‐ ator pattern described next, however, there is no online health maintenance for compiled abstractions; it is delegated down to the lower-level objects (e.g., pods). operators while compiler extensions provide easy-to-use abstractions, extensions that use the “operator” pattern provide online, proactive management of the resources created by the extensions. these extensions likely provide a higher-level abstraction (for exam‐ ple, a database) that is compiled down to a lower-level representation, but they also provide online functionality, such as snapshot backups of the database, or upgrade notifications when a new version of the software is available. to achieve this, the con‐ troller not only monitors the extension api to add or remove things as necessary, but also monitors the running state of the application supplied by the extension (e.g., a database) and takes actions to remediate unhealthy databases, take snapshots, or restore from a snapshot if a failure occurs. operators are the most complicated pat‐ tern for api extension of kubernetes, but they are also the most powerful, enabling users to get easy access to “self-driving” abstractions that are responsible not just for deployment, but also health checking and repair. getting started getting started extending the kubernetes api can be a daunting and exhausting experience. fortunately, there is a great deal of code to help you out. the kubebuilder project contains a library of code intended help you easily build reliable kubernetes api extensions. it’s a great resource to help you bootstrap your extension. summary one of the great “superpowers” of kubernetes is its ecosystem, and one of the most significant things powering this ecosystem is the extensibility of the kubernetes api. whether you’re designing your own extensions to customize your cluster or consum‐ ing off-the-shelf extensions as utilities, cluster services, or operators, api extensions summary | 205 are the key to making your cluster your own and building the right environment for the rapid development of reliable applications. 206 | chapter 16: extending kubernetes chapter 17 deploying real-world applications the previous chapters described a variety of api objects that are available in a kuber‐ netes cluster and ways in which those objects can best be used to construct reliable distributed systems. however, none of the preceding chapters really discussed how you might use the objects in practice to deploy a complete, real-world application. that is the focus of this chapter. we’ll take a look at four real-world applications: • jupyter, an open source scientific notebook • parse, an open source api server for mobile applications • ghost, a blogging and content management platform • redis, a lightweight, performant key/value store these complete examples should give you a better idea of how to structure your own deployments using kubernetes. jupyter the jupyter project is a web-based interactive scientific notebook for exploration and visualization. it is used by students and scientists around the world to build and explore data and data visualizations. because it is both simple to deploy and interest‐ ing to use, it’s a great first service to deploy on kubernetes. we begin by creating a namespace to hold the jupyter application: $ kubectl create namespace jupyter 207 and then create a deployment of size one with the program itself: apiversion: extensions/v1beta1 kind: deployment metadata:  labels:  run: jupyter  name: jupyter  namespace: jupyter spec:  replicas: 1  selector:  matchlabels:  run: jupyter  template:  metadata:  labels:  run: jupyter  spec:  containers  - image: jupyter/scipy-notebook:abdb27a6dfbb  name: jupyter  dnspolicy: clusterfirst  restartpolicy: always create a file named jupyter.yaml with these contents. once you have created this file, you can deploy it using: $ kubectl create -f jupyter.yaml now you need to wait for the container to be created. the jupyter container is quite large (2 gb at the time of writing), and thus it may take a few minutes to start up. you can wait for the container to become ready using the watch command (on macos, you’ll need to install this command using brew install watch): $ watch kubectl get pods --namespace jupyter once the jupyter container is up and running, you need to obtain the initial login token. you can do this by looking at the logs for the container: $ podname=$(kubectl get pods --namespace jupyter --no-headers | awk \\'{print $1}\\') \\\\ kubectl logs --namespace jupyter ${podname} you should then copy the token (it will look something like /? token=0195713c8e65088650fdd8b599db377b7ce6c9b10bd13766). next, set up port forwarding to the jupyter container: $ kubectl port-forward ${podname} 8888:8888 finally, you can visit http://localhost:8888/?token=<token>, inserting the token that you copied from the logs earlier. 208 | chapter 17: deploying real-world applications you should now have the jupyter dashboard loaded in your browser. you can find tutorials to get oriented to jupyter if you are so inclined on the jupyter project site. parse the parse server is a cloud api dedicated to providing easy-to-use storage for mobile applications. it provides a variety of different client libraries that make it easy to inte‐ grate with android, ios, and other mobile platforms. parse was purchased by face‐ book in 2013 and subsequently shut down. fortunately for us, a compatible server was open sourced by the core parse team and is available for us to use. this section describes how to set up parse in kubernetes. prerequisites parse uses a mongodb cluster for its storage. chapter 15 described how to set up a replicated mongodb cluster using kubernetes statefulsets. this section assumes you have a three-replica mongo cluster running in kubernetes with the names mongo-0.mongo, mongo-1.mongo, and mongo-2.mongo. these instructions also assume that you have a docker login; if you don’t have one, you can get one for free at https://docker.com. finally, we assume you have a kubernetes cluster deployed and the kubectl tool properly configured. building the parse-server the open source parse-server comes with a dockerfile by default, for easy contain‐ erization. first, clone the parse repository: $ git clone https://github.com/parseplatform/parse-server then move into that directory and build the image: $ cd parse-server $ docker build -t ${dockeruser}/parse-server . finally, push that image up to the docker hub: $ docker push ${dockeruser}/parse-server deploying the parse-server once you have the container image built, deploying the parse-server into your clus‐ ter is fairly straightforward. parse looks for three environment variables when being configured: parse | 209 parseserverapplicationid an identifier for authorizing your application parseservermasterkey an identifier that authorizes the master (root) user parseserverdatabaseuri the uri for your mongodb cluster putting this all together, you can deploy parse as a kubernetes deployment using the yaml file in example 17-1. example 17-1. parse.yaml apiversion: extensions/v1beta1 kind: deployment metadata:  name: parse-server  namespace: default spec:  replicas: 1  template:  metadata:  labels:  run: parse-server  spec:  containers:  - name: parse-server  image: ${dockeruser}/parse-server  env:  - name: parseserverdatabaseuri  value: \"mongodb://mongo-0.mongo:27017,\\\\  mongo-1.mongo:27017,mongo-2.mongo\\\\  :27017/dev?replicaset=rs0\"  - name: parseserverappid  value: my-app-id  - name: parseservermasterkey  value: my-master-key testing parse to test your deployment, you need to expose it as a kubernetes service. you can do that using the service definition in example 17-2. example 17-2. parse-service.yaml apiversion: v1 kind: service metadata: 210 | chapter 17: deploying real-world applications  name: parse-server  namespace: default spec:  ports:  - port: 1337  protocol: tcp  targetport: 1337  selector:  run: parse-server now your parse server is up and running and ready to receive requests from your mobile applications. of course, in any real application you are likely going to want to secure the connection with https. you can see the parse-server github page for more details on such a configuration. ghost ghost is a popular blogging engine with a clean interface written in javascript. it can either use a file-based sqlite database or mysql for storage. con\\x80guring ghost ghost is configured with a simple javascript file that describes the server. we will store this file as a configuration map. a simple development configuration for ghost looks like example 17-3. example 17-3. ghost-config.js var path = require(\\'path\\'),  config; config = {  development: {  url: \\'http://localhost:2368\\',  database: {  client: \\'sqlite3\\',  connection: {  filename: path.join(process.env.ghostcontent,  \\'/data/ghost-dev.db\\')  },  debug: false  },  server: {  host: \\'0.0.0.0\\',  port: \\'2368\\'  },  paths: {  contentpath: path.join(process.env.ghostcontent, \\'/\\')  } ghost | 211  } }; module.exports = config; once you have this configuration file saved to ghost-config.js, you can create a kuber‐ netes configmap object using: $ kubectl create cm --from-file ghost-config.js ghost-config this creates a configmap that is named ghost-config. as with the parse example, we will mount this configuration file as a volume inside of our container. we will deploy ghost as a deployment object, which defines this volume mount as part of the pod template (example 17-4). example 17-4. ghost.yaml apiversion: extensions/v1beta1 kind: deployment metadata:  name: ghost spec:  replicas: 1  selector:  matchlabels:  run: ghost  template:  metadata:  labels:  run: ghost  spec:  containers:  - image: ghost  name: ghost  command:  - sh  - -c  - cp /ghost-config/ghost-config.js /var/lib/ghost/config.js  && /usr/local/bin/docker-entrypoint.sh node current/index.js  volumemounts:  - mountpath: /ghost-config  name: config  volumes:  - name: config  configmap:  defaultmode: 420  name: ghost-config 212 | chapter 17: deploying real-world applications one thing to note here is that we are copying the config.js file from a different loca‐ tion into the location where ghost expects to find it, since the configmap can only mount directories, not individual files. ghost expects other files that are not in that configmap to be present in its directory, and thus we cannot simply mount the entire configmap into /var/lib/ghost. you can run this with: $ kubectl apply -f ghost.yaml once the pod is up and running, you can expose it as a service with: $ kubectl expose deployments ghost --port=2368 once the service is exposed, you can use the kubectl proxy command to access the ghost server: $ kubectl proxy then visit http://localhost:8001/api/v1/namespaces/default/services/ghost/proxy/ in your web browser to begin interacting with ghost. ghost + mysql of course, this example isn’t very scalable, or even reliable, since the contents of the blog are stored in a local file inside the container. a more scalable approach is to store the blog’s data in a mysql database. to do this, first modify config.js to include: ... database: {  client: \\'mysql\\',  connection: {  host : \\'mysql\\',  user : \\'root\\',  password : \\'root\\',  database : \\'ghostdb\\',  charset : \\'utf8\\'  }  }, ... obviously, in a real-world deployment you’ll want to change the password from root to something more secret. next, create a new ghost-config configmap object: $ kubectl create configmap ghost-config-mysql --from-file ghost-config.js then update the ghost deployment to change the name of the configmap mounted from config-map to config-map-mysql: ghost | 213 ...  - configmap:  name: ghost-config-mysql ... using the instructions from “kubernetes-native storage with statefulsets” on page 186, deploy a mysql server in your kubernetes cluster. make sure that it has a ser‐ vice named mysql defined as well. you will need to create the database in the mysql database: $ kubectl exec -it mysql-zzmlw -- mysql -u root -p enter password: welcome to the mysql monitor. commands end with ; or \\\\g. ... mysql> create database ghostdb; ... finally, perform a rollout to deploy this new configuration: $ kubectl apply -f ghost.yaml because your ghost server is now decoupled from its database, you can scale up your ghost server and it will continue to share the data across all replicas. edit ghost.yaml to set spec.replicas to 3, then run: $ kubectl apply -f ghost.yaml your ghost installation is now scaled up to three replicas. redis redis is a popular in-memory key/value store, with numerous additional features. it’s an interesting application to deploy because it is a good example of the value of the kubernetes pod abstraction. this is because a reliable redis installation actually is two programs working together. the first is redis-server, which implements the key/value store, and the other is redis-sentinel, which implements health checking and failover for a replicated redis cluster. when redis is deployed in a replicated manner, there is a single master server that can be used for both read and write operations. additionally, there are other replica servers that duplicate the data written to the master and can be used for loadbalancing read operations. any of these replicas can fail over to become the master if the original master fails. this failover is performed by the redis sentinel. in our deployment, both a redis server and a redis sentinel are colocated in the same file. 214 | chapter 17: deploying real-world applications con\\x80guring redis as before, we’re going to use kubernetes configmaps to configure our redis installa‐ tion. redis needs separate configurations for the master and slave replicas. to config‐ ure the master, create a file named master.conf that contains the code in example 17-5. example 17-5. master.conf bind 0.0.0.0 port 6379 dir /redis-data this directs redis to bind to all network interfaces on port 6379 (the default redis port) and store its files in the /redis-data directory. the slave configuration is identical, but it adds a single slaveof directive. create a file named slave.conf that contains what’s in example 17-6. example 17-6. slave.conf bind 0.0.0.0 port 6379 dir . slaveof redis-0.redis 6379 notice that we are using redis-0.redis for the name of the master. we will set up this name using a service and a statefulset. we also need a configuration for the redis sentinel. create a file named sentinel.conf with the contents of example 17-7. example 17-7. sentinel.conf bind 0.0.0.0 port 26379 sentinel monitor redis redis-0.redis 6379 2 sentinel parallel-syncs redis 1 sentinel down-after-milliseconds redis 10000 sentinel failover-timeout redis 20000 now that we have all of our configuration files, we need to create a couple of simple wrapper scripts to use in our statefulset deployment. redis | 215 the first script simply looks at the hostname for the pod and determines whether this is the master or a slave, and launches redis with the appropriate configuration. cre‐ ate a file named init.sh containing the code in example 17-8. example 17-8. init.sh #!/bin/bash if ]; then  redis-server /redis-config/master.conf else  redis-server /redis-config/slave.conf fi the other script is for the sentinel. in this case it is necessary because we need to wait for the redis-0.redis dns name to become available. create a script named sentinel.sh containing the code in example 17-9. example 17-9. sentinel.sh #!/bin/bash cp /redis-config-src/*.* /redis-config while ! ping -c 1 redis-0.redis; do  echo \\'waiting for server\\'  sleep 1 done redis-sentinel /redis-config/sentinel.conf now we need to package all of these files into a configmap object. you can do this with a single command line: $ kubectl create configmap \\\\  --from-file=slave.conf=./slave.conf \\\\  --from-file=master.conf=./master.conf \\\\  --from-file=sentinel.conf=./sentinel.conf \\\\  --from-file=init.sh=./init.sh \\\\  --from-file=sentinel.sh=./sentinel.sh \\\\  redis-config creating a redis service the next step in deploying redis is to create a kubernetes service that will provide naming and discovery for the redis replicas (e.g., redis-0.redis). to do this, we cre‐ ate a service without a cluster ip address (example 17-10). 216 | chapter 17: deploying real-world applications example 17-10. redis-service.yaml apiversion: v1 kind: service metadata:  name: redis spec:  ports:  - port: 6379  name: peer  clusterip: none  selector:  app: redis you can create this service with kubectl apply -f redis-service.yaml. don’t worry that the pods for the service don’t exist yet. kubernetes doesn’t care; it will add the right names when the pods are created. deploying redis we’re ready to deploy our redis cluster. to do this we’re going to use a statefulset. we introduced statefulsets in “manually replicated mongodb with statefulsets” on page 187, when we discussed our mongodb installation. statefulsets provide indexing (e.g., redis-0.redis) as well as ordered creation and deletion semantics (redis-0 will always be created before redis-1, and so on). they’re quite useful for stateful applications like redis, but honestly, they basically look like kubernetes deployments. example 17-11 shows what the statefulset looks like for our redis cluster. example 17-11. redis.yaml apiversion: apps/v1beta1 kind: statefulset metadata:  name: redis spec:  replicas: 3  servicename: redis  template:  metadata:  labels:  app: redis  spec:  containers:  - command:   image: redis:4.0.11-alpine  name: redis  ports:  - containerport: 6379  name: redis redis | 217  volumemounts:  - mountpath: /redis-config  name: config  - mountpath: /redis-data  name: data  - command:   image: redis:4.0.11-alpine  name: sentinel  volumemounts:  - mountpath: /redis-config-src  name: config  - mountpath: /redis-config  name: data  volumes:  - configmap:  defaultmode: 420  name: redis-config  name: config  - emptydir:  name: data  volumemounts:  - mountpath: /redis-config  name: config  - mountpath: /redis-data  name: data  - command:   image: redis:3.2.7-alpine  name: sentinel  volumemounts:  - mountpath: /redis-config  name: config you can see that there are two containers in this pod. one runs the init.sh script that we created and the main redis server, and the other is the sentinel that monitors the servers. you can also see that there are two volumes defined in the pod. one is the volume that uses our configmap to configure the two redis applications, and the other is a simple emptydir volume that is mapped into the redis server container to hold the application data so that it survives a container restart. for a more reliable redis installation, this could be a network-attached disk, as discussed in chapter 15. now that we’ve defined our redis cluster, we can create it using: $ kubectl apply -f redis.yaml playing with our redis cluster to demonstrate that we’ve actually successfully created a redis cluster, we can per‐ form some tests. 218 | chapter 17: deploying real-world applications first, we can determine which server the redis sentinel believes is the master. to do this, we can run the redis-cli command in one of the pods: $ kubectl exec redis-2 -c redis \\\\  -- redis-cli -p 26379 sentinel get-master-addr-by-name redis this should print out the ip address of the redis-0 pod. you can confirm this using kubectl get pods -o wide. next, we’ll confirm that the replication is actually working. to do this, first try to read the value foo from one of the replicas: $ kubectl exec redis-2 -c redis -- redis-cli -p 6379 get foo you should see no data in the response. next, try to write that data to a replica: $ kubectl exec redis-2 -c redis -- redis-cli -p 6379 set foo 10 readonly you can\\'t write against a read only slave. you can’t write to a replica, because it’s read-only. let’s try the same command against redis-0, which is the master: $ kubectl exec redis-0 -c redis -- redis-cli -p 6379 set foo 10 ok now try the original read from a replica: $ kubectl exec redis-2 -c redis -- redis-cli -p 6379 get foo 10 this shows that our cluster is set up correctly, and data is replicating between masters and slaves. summary in this chapter we described how to deploy a variety of applications using assorted kubernetes concepts. we saw how to put together service-based naming and discov‐ ery to deploy web frontends like ghost as well as api servers like parse, and we saw how pod abstraction makes it easy to deploy the components that make up a reliable redis cluster. regardless of whether you will actually deploy these applications to production, the examples demonstrated patterns that you can repeat to manage your applications using kubernetes. we hope that seeing the concepts we described in pre‐ vious chapters come to life in real-world examples helps you better understand how to make kubernetes work for you. summary | 219  chapter 18 organizing your application throughout this book we have described various components of an application built on top of kubernetes. we have described how to wrap programs up as containers, place those containers in pods, replicate those pods with replicasets, and roll out software each week with deployments. we have even described how to deploy stateful and real-world applications that put together a collection of these objects into a single distributed system. but we have not covered how to actually work with such an appli‐ cation in a practical way. how can you lay out, share, manage, and update the various configurations that make up your application? that is the topic for this chapter. principles to guide us before digging into the concrete details of how to structure your application, it’s worth considering the goals that drive this structure. obviously, reliability and agility are the general goals of developing a cloud-native application in kubernetes, but moving to the next level of detail, how does this actually relate to how you design the maintenance and deployment of your application? the following sections describe the various principles that we can use as a guide to design a structure that best suits these goals. the principles are: • filesystems as the source of truth • code review to ensure the quality of changes • feature flags for staged roll forward and roll back 221 filesystems as the source of truth when you first begin to explore kubernetes, as we did in the beginning of this book, you generally interact with it imperatively. you run commands like kubectl run or kubectl edit to create and modify pods or other objects running in your cluster. even when we started exploring how to write and use yaml or json files, this was presented in an ad-hoc manner, as if the file itself is just a way station on the way to modifying the state of the cluster. in reality, in a true productionized application the opposite should be true. rather than viewing the state of the cluster—the data in etcd—as the source of truth, it is optimal to view the filesystem of yaml objects as the source of truth for your application. the api objects deployed into your kubernetes cluster(s) are then a reflection of the truth stored in the filesystem. there are numerous reasons why this is the right point of view. the first and fore‐ most is that it largely enables you to treat your cluster as if it is immutable infrastruc‐ ture. as we have moved into cloud-native architectures, we have become increasingly comfortable with the notion that our applications and their containers are immutable infrastructure, but treating a cluster as such is less common. and yet, the same rea‐ sons for moving our applications to immutable infrastructure apply to our clusters. if your cluster is a snowflake made up by the ad-hoc application of various random yaml files downloaded from the internet, it is as dangerous as a virtual machine that has been built from imperative bash scripts. additionally, managing the cluster state via the filesystem makes it very easy to col‐ laborate with multiple team members. source-control systems are well understood and can easily enable multiple different people to simultaneously edit the state of the cluster while making conflicts (and the resolution of those conflicts) clear to everyone. the combination of these motivations means that it is absolutely a first principle that all applications deployed to kubernetes should first be described in files stored in a filesystem. the actual api objects are then just a projection of this filesystem into a particular cluster. the role of code review it wasn’t long ago that code review for application source code was a novel idea. but it is clear now that the notion of multiple people looking at a piece of code before it is committed to an application is a best practice for producing quality, reliable code. it is therefore surprising that the same is somewhat less true for the configurations used to deploy those applications. all of the same reasons for reviewing code apply 222 | chapter 18: organizing your application directly to application configurations also. but when you think about it, it is also obvious that code review of these configurations is critical to the reliable deployment of services. in our experience, most service outages are self-inflicted via unexpected consequences, typos, or other simple mistakes. ensuring that at least two people look at any configuration change significantly decreases the probability of such errors. consequently, the second principle of our application layout is that it must facilitate the review of every change merged into the set of files that represents the source of truth for our cluster. feature gates and guards once your application source code and your deployment configuration files are in source control, one of the most common questions that occurs is how these reposito‐ ries relate to one another. should you use the same repository for application source code as well as configuration? this can work for small projects, but in larger projects it often makes sense to separate the source code from the configuration to provide for a separation of concerns. even if the same people are responsible for both building and deploying the application, the perspectives of the builder versus the deployer are different enough that this separation of concerns makes sense. if that is the case, then how do you bridge the development of new features in source control with the deployment of those features into a production environment? this is where feature gates and guards play an important role. the idea is that when some new feature is developed, that development takes place entirely behind a feature flag or gate. this gate looks something like: if (featureflags.myflag) {  // feature implementation goes here } there are a variety of benefits to this approach. first, it enables the committing of code to the production branch long before the feature is ready to ship. this enables feature development to stay much more closely aligned with the head of a repository, and thus you avoid the horrendous merge conflicts of a long-lived branch. additionally, it means that the enabling a feature simply involves a configuration change to activate the flag. this makes it very clear what changed in the production environment, and likewise makes it very simple to roll back the activation of the fea‐ ture if it causes problems. the use of feature flags thus both simplifies debugging problems in production and ensures that disabling a feature doesn’t require a binary rollback to an older version of principles to guide us | 223 the code that would remove all of the bug fixes and other improvements made by the newer version of the code. the third principle of application layout is that code lands in source control, by default off, behind a feature flag, and is only acti‐ vated through a code-reviewed change to configuration files. managing your application in source control now that we have determined that the filesystem should represent the source of truth for your cluster, the next important question is how to actually lay out the files in the filesystem. obviously, filesystems contain hierarchical directories, and a sourcecontrol system adds concepts like tags and branches, so this section describes how to put these together to represent and manage your application. filesystem layout for the purposes of this section, we will describe how to lay out an instance of your application for a single cluster. in later sections we will describe how to parameterize this layout for multiple instances of your application. it’s worth noting that this orga‐ nization is worth getting right when you begin. much like modifying the layout of packages in source control, modifying your deployment configurations after the fact is a complicated and expensive refactor that you’ll probably never get around to. the first cardinality on which you want to organize your application is the semantic component or layer (e.g., frontend, batch work queue, etc.). though early on this might seem like overkill, since a single team manages all of these components, it sets the stage for team scaling—eventually, a different team (or subteam) may be responsi‐ ble for each of these components. thus, for an application with a frontend that uses two services, the filesystem might look like: frontend/ service-1/ service-2/ within each of these directories, the configurations for each application are stored. these are the yaml files that directly represent the current state of the cluster. it’s generally useful to include both the service name and the object type within the same file. 224 | chapter 18: organizing your application while kubernetes allows for the creation of yaml files with multi‐ ple objects in the same file, this should generally be considered an anti-pattern. the only good reason for grouping a number of objects in the same file is if they are conceptually identical. when deciding what to include in a single yaml file, consider design principles similar to those for defining a class or struct. if grouping the objects together doesn’t form a single concept, they probably shouldn’t be in a single file. thus, extending our previous example, the filesystem might look like: frontend/  frontend-deployment.yaml  frontend-service.yaml  frontend-ingress.yaml service-1/  service-1-deployment.yaml  service-1-service.yaml  service-1-configmap.yaml ... managing periodic versions the previous section described a file structure for laying out the various tiers in your application, but what about managing the releases of your application? it is very use‐ ful to be able to look back historically and see what your application deployment pre‐ viously looked like. similarly, it is very useful to be able to iterate a configuration forward while still being able to deploy a stable release configuration. consequently, it’s handy to be able to simultaneously store and maintain multiple dif‐ ferent revisions of your configuration. given the file and version control approach, there are two different approaches that you can use. the first is to use tags, branches, and source-control features. this is convenient because it maps to the same way that people manage revisions in source control, and it leads to a more simplified directory structure. the other option is to clone the configuration within the filesystem and use directories for different revisions. this approach is convenient because it makes simultaneous viewing of the configurations very straightforward. in reality, the approaches are more or less identical, and it is ultimately an aesthetic choice between the two. thus, we will discuss both approaches and let you or your team decide which you prefer. versioning with branches and tags when you use branches and tags to manage configuration revisions, the directory structure is unchanged from the example in the previous section. when you are ready for a release, you place a source-control tag (e.g., git tag v1.0) in the configu‐ managing your application in source control | 225 ration source-control system. the tag represents the configuration used for that ver‐ sion, and the head of source control continues to iterate forward. the world becomes somewhat more complicated when you need to update the release configuration, but the approach models what you would do in source control. first, you commit the change to the head of the repository. then you create a new branch named v1 at the v1.0 tag. you then cherry-pick the desired change onto the release branch (git cherry-pick <edit>), and finally, you tag this branch with the v1.1 tag to indicate a new point release. one common error when cherry-picking fixes into a release branch is to only pick the change into the latest release. it’s a good idea to cherry-pick it into all active releases, in case for some reason you need to roll back versions but the fix is still needed. versioning with directories an alternative to using source-control features is to use filesystem features. in this approach, each versioned deployment exists within its own directory. for example, the filesystem for your application might look like this: frontend/  v1/  frontend-deployment.yaml  frontend-service.yaml  current/  frontend-deployment.yaml  frontend-service.yaml service-1/  v1/  service-1-deployment.yaml  service-1-service.yaml  v2/  service-1-deployment.yaml  service-1-service.yaml  current/  service-1-deployment.yaml  service-1-service.yaml ... thus, each revision exists in a parallel directory structure within a directory associ‐ ated with the release. all deployments occur from head instead of from specific revi‐ sions or tags. when adding a new configuration, it is done to the files in the current directory. when creating a new release, the current directory is copied to create a new directory associated with the new release. 226 | chapter 18: organizing your application when performing a bugfix change to a release, the pull request must modify the yaml file in all the relevant release directories. this is a slightly better experience than the cherry-picking approach described earlier, since it is clear in a single change request that all of the relevant versions are being updated with the same change, instead of requiring a cherry-pick per version. structuring your application for development, testing, and deployment in addition to structuring your application for a periodic release cadence, you also want to structure your application to enable agile development, quality testing, and safe deployment. this enables developers to rapidly make and test changes to the dis‐ tributed application, and to safely roll those changes out to customers. goals there are two goals for your application with regard to development and testing. the first is that each developer should be able to easily develop new features for the appli‐ cation. in most cases, the developer is only working on a single component, and yet that component is interconnected to all of the other microservices within the cluster. thus, to facilitate development it is essential that developers be able to work in their own environment, yet with all services available. the other goal for structuring your application for testing is the ability to easily and accurately test your application prior to deployment. this is essential to the ability to quickly roll out features while maintaining high reliability. progression of a release to achieve both of these goals, it is important to relate the stages of development to the release versions described earlier. the stages of a release are: head the bleeding edge of the configuration; the latest changes. development largely stable, but not ready for deployment. suitable for developers to use for building features. staging the beginnings of testing, unlikely to change unless problems are found. canary the first real release to users, used to test for problems with real-world traffic and likewise give users a chance to test what is coming next. structuring your application for development, testing, and deployment | 227 release the current production release. introducing a development tag regardless of whether you structure releases using the filesystem or version control, the right way to model the development stage is via a source-control tag. this is because development is necessarily fast-moving as it tracks stability only slightly behind head. to introduce a development stage, a new development tag is added to the sourcecontrol system and an automated process is used to move this tag forward. on a peri‐ odic cadence, head is tested via automated integration testing. if these tests pass, the development tag is moved forward to head. thus, developers can track reasonably close to the latest changes when deploying their own environments, but they also can be assured that the deployed configurations have at least passed a limited smoke test. mapping stages to revisions it might be tempting to introduce a new set of configurations for each of these stages, but in reality, the cartesian product of versions and stages would create a mess that is very difficult to reason about. instead, the right practice is to introduce a mapping between revisions and stages. regardless of whether you are using the filesystem or source-control revisions to rep‐ resent different configuration versions, it is easy to implement a map from stage to revision. in the filesystem case you can use symbolic links to map a stage name to a revision: frontend/  canary/ -> v2/  release/ -> v1/  v1/  frontend-deployment.yaml ... in the case of version control, it is simply an additional tag at the same revision as the appropriate version. in either case, the versioning of releases proceeds using the processes described previ‐ ously, and separately the stages are moved forward to new versions as appropriate. effectively this means that there are two simultaneous processes, the first for cutting new release versions and the second for qualifying a release version for a particular stage in the application lifecycle. 228 | chapter 18: organizing your application parameterizing your application with templates once you have a cartesian product of environments and stages, it becomes clear that it is impractical or impossible to keep them all entirely identical. and yet, it is impor‐ tant to strive for the environments to be as identical as possible. variance and drift between different environments produces snowflakes and systems that are hard to reason about. if your staging environment is different than your release environment, can you really trust the load tests that you ran in the staging environment to qualify a release? to ensure that your environments stay as similar as possible, it is useful to use parameterized environments. parameterized environments use templates for the bulk of their configuration, but they mix in a limited set of parameters to produce the final configuration. in this way most of the configuration is contained within a shared template, while the parameterization is limited in scope and maintained in a small parameters file for easy visualization of differences between environments. parameterizing with helm and templates there are a variety of different languages for creating parameterized configurations. in general they all divide the files into a template file, which contains the bulk of the configuration, and a parameters file, which can be combined with the template to produce a complete configuration. in addition to parameters, most templating lan‐ guages allow parameters to have default values if no value is specified. the following gives examples of how to parameterize configurations using helm, a package manager for kubernetes. despite what devotees of various languages may say, all parameterization languages are largely equivalent, and as with programming langauges, which one you prefer is largely a matter of personal or team style. thus, the same patterns described here for helm apply regardless of the templating lan‐ guage you choose. the helm template language uses the “mustache” syntax, so for example: metadata:  name: {{ .release.name }}-deployment indicates that release.name should be substituted into the name of a deployment. to pass a parameter for this value you use a values.yaml file with contents like: release:  name: my-release which after parameter substitution results in: metadata:  name: my-release-deployment parameterizing your application with templates | 229 filesystem layout for parameterization now that you understand how to parameterize your configurations, how do you apply that to the filesystem layouts we have described previously? to achieve this, instead of treating each deployment lifecycle stage as a pointer to a version, each deployment lifecycle is the combination of a parameters file and a pointer to a spe‐ cific version. for example, in a directory-based layout this might look like: frontend/  staging/  templates -> ../v2  staging-parameters.yaml  production/  templates -> ../v1  production-parameters.yaml  v1/  frontend-deployment.yaml  frontend-service.yaml  v2/  frontend-deployment.yaml  frontend-service.yaml ... doing this with version control looks similar, except that the parameters for each life‐ cycle stage are kept at the root of the configuration directory tree: frontend/  staging-parameters.yaml  templates/  frontend-deployment.yaml .... deploying your application around the world now that you have multiple versions of your application moving through multiple stages of deployment, the final step in structuring your configurations is to deploy your application around the world. but don’t think that these approaches are only for large-scale applications. in reality, they can be used to scale from two different regions to tens or hundreds around the world. in the world of the cloud, where an entire region can fail, deploying to multiple regions (and managing that deployment) is the only way to achieve sufficient uptime for demanding users. architectures for worldwide deployment generally speaking, each kubernetes cluster is intended to live in a single region, and each kubernetes cluster is expected to contain a single, complete deployment of your application. consequently, a worldwide deployment of an application consists of mul‐ tiple different kubernetes clusters, each with its own application configuration. 230 | chapter 18: organizing your application describing how to actually build a worldwide application, especially with complex subjects like data replication, is beyond the scope of this chapter, but we will describe how to arrange the application configurations in the filesystem. ultimately, a particular region’s configuration is conceptually the same as a stage in the deployment lifecycle. thus, adding multiple regions to your configuration is iden‐ tical to adding new lifecycle stages. for example, instead of: • development • staging • canary • production you might have: • development • staging • canary • eastus • westus • europe • asia modeling this in the filesystem for configuration, this looks like: frontend/  staging/  templates -> ../v3/  parameters.yaml  eastus/  templates -> ../v1/  parameters.yaml  westus/  templates -> ../v2/  parameters.yaml  ... if you instead are using version control and tags, the filesystem would look like: frontend/  staging-parameters.yaml  eastus-parameters.yaml  westus-parameters.yaml  templates/  frontend-deployment.yaml ... deploying your application around the world | 231 using this structure, you would introduce a new tag for each region and use the file contents at that tag to deploy to that region. implementing worldwide deployment now that you have configurations for each region around the world, the question becomes one of how to update those various regions. one of the primary goals of using multiple regions is to ensure very high reliability and uptime. while it would be tempting to assume that cloud and data center outages are the primary causes of downtime, the truth is that outages are generally caused by new versions of software rolling out. because of this, the key to a highly available system is limiting the effect or “blast radius” of any change that you might make. thus, as you roll out a version across a variety of regions, it makes sense to move carefully from region to region in order to validate and gain confidence in one region before moving on to the next. rolling out software across the world generally looks more like a workflow than a single declarative update: you begin by updating the version in staging to the latest version and then proceed through all regions until it is rolled out everywhere. but how should you structure the various regions, and how long should you wait to vali‐ date between regions? to determine the length of time between rollouts to regions, you want to consider the “mean time to smoke” for your software. this is the time it takes on average after a new release is rolled out to a region for a problem (if it exists) to be discovered. obvi‐ ously, each problem is unique and can take a varying amount of time to make itself known, and that is why you want to understand the average time. managing software at scale is a business of probability, not certainty, so you want to wait for a time that makes the probability of an error low enough that you are comfortable moving on to the next region. something like two to three times the mean time to smoke is proba‐ bly a reasonable place to start, but it is highly variable depending on your application. to determine the order of regions, it is important to consider the characteristics of various regions. for example, you are likely to have high-traffic regions and lowtraffic regions. depending on your application, you may have features that are more popular in one geographic area or another. all of these characteristics should be con‐ sidered when putting together a release schedule. you likely want to begin by rolling out to a low-traffic region. this ensures that any early problems you catch are limited to an area of little impact. though it is not a hard-and-fast rule, early problems are often the most severe, since they manifest quickly enough to be caught in the first region you roll out to. thus, minimizing the impact of such problems on your cus‐ tomers makes sense. next, you likely want to roll out to a high-traffic region. once you have successfully validated that your release works correctly via the low-traffic region, you want to validate that it works correctly at scale. the only way to do this is to roll it out to a single high-traffic region. when you have successfully rolled out to 232 | chapter 18: organizing your application both a low- and a high-traffic region, you may have confidence that your application can safely roll out everywhere. however, if there are regional variations, you may want to also test more slowly across a variety of geographies before pushing your release more broadly. when you put your release schedule together, it is important to follow it completely for every release, no matter how big or how small. many outages have been caused by people accelerating releases either to fix some other problem, or because they believed it to be “safe.” dashboards and monitoring for worldwide deployments it may seem an odd concept when you are developing at a small scale, but one signifi‐ cant problem that you will likely run into at a medium or large scale is having differ‐ ent versions of your application deployed to different regions. this can happen for a variety of reasons (e.g., because a release has failed, been aborted, or had problems in a particular region), and if you don’t track things carefully you can rapidly end up with an unmanageable snowflake of different versions deployed around the world. furthermore, as customers inquire about fixes to bugs they are experiencing, a com‐ mon question will become: “is it deployed yet?” thus, it is essential to develop dashboards that can tell you at a glance what version is running in which region, as well as alerting that will fire when too many different ver‐ sions of your application are deployed. a best practice is to limit the number of active versions to no more than three: one testing, one rolling out, and one being replaced by the rollout. any more active versions than this is asking for trouble. summary this chapter provides guidance on how to manage a kubernetes application through software versions, deployment stages, and regions around the world. it highlights the principles that are the foundation of organizing your application: relying on the file‐ system for organization, using code review to ensure quality changes, and relying on feature flags or gates to make it easy to incrementally add and remove functionality. as with everything, the recipes in this chapter should be taken as inspiration, rather than absolute truth. read the guidance, and find the mix of approaches that works best for the particular circumstances of your application. but keep in mind that in laying out your application for deployment, you are setting a process that you will likely have to live with for a number of years. summary | 233  appendix a building a raspberry pi kubernetes cluster while kubernetes is often experienced through the virtual world of public cloud computing, where the closest you get to your cluster is a web browser or a terminal, it can be a very rewarding experience to physically build a kubernetes cluster on bare metal. likewise, nothing compares to physically pulling the power or network on a node and watching how kubernetes reacts to heal your application to convince you of its utility. building your own cluster might seem like both a challenging and an expensive effort, but fortunately it is neither. the ability to purchase low-cost, system-on-chip computer boards, as well as a great deal of work by the community to make kuber‐ netes easier to install, means that it is possible to build a small kubernetes cluster in a few hours. in the following instructions, we focus on building a cluster of raspberry pi machines, but with slight adaptations the same instructions could be made to work with a variety of different single-board machines. parts list the first thing you need to do is assemble the pieces for your cluster. in all of the examples here, we’ll assume a four-node cluster. you could build a cluster of three nodes, or even a cluster of a hundred nodes if you wanted to, but four is a pretty good number. to start, you’ll need to purchase (or scrounge) the various pieces needed to build the cluster. here is the shopping list, with some approximate prices as of the time of writing: 235 1. four raspberry pi 3 boards (raspberry pi 2 will also work)—$160 2. four sdhc memory cards, at least 8 gb (buy high-quality ones!)—$30–50 3. four 12-inch cat. 6 ethernet cables—$10 4. four 12-inch usb a–micro usb cables—$10 5. one 5-port 10/100 fast ethernet switch—$10 6. one 5-port usb charger—$25 7. one raspberry pi stackable case capable of holding four pis—$40 (or build your own) 8. one usb-to-barrel plug for powering the ethernet switch (optional)—$5 the total for the cluster comes out to be about $300, which you can drop down to $200 by building a three-node cluster and skipping the case and the usb power cable for the switch (though the case and the cable really clean up the whole cluster). one other note on memory cards: do not scrimp here. low-end memory cards behave unpredictably and make your cluster really unstable. if you want to save some money, buy a smaller, high-quality card. high-quality 8 gb cards can be had for around $7 each online. once you have your parts, you’re ready to move on to building the cluster. these instructions also assume that you have a device capable of flashing an sdhc card. if you do not, you will need to purchase a usb → memory card reader/writer. flashing images the default raspbian image now supports docker through the standard install meth‐ ods, but to make things even easier, the hypriot project provides images with docker preinstalled. visit the hypriot downloads page and download the latest stable image. unzip the image, and you should now have an .img file. the hypriot project also provides really excellent documentation for writing this image to your memory card for each of these platforms: • macos • windows • linux 236 | appendix a: building a raspberry pi kubernetes cluster write the same image onto each of your memory cards. first boot: master the first thing to do is to boot just your master node. assemble your cluster, and decide which is going to be the master node. insert the memory card, plug the board into an hdmi output, and plug a keyboard into the usb port. next, attach the power to boot the board. log in at the prompt using the username pirate and the password hypriot. the very first thing you should do with your raspberry pi (or any new device) is to change the default password. the default pass‐ word for every type of install everywhere is well known by people who will misbehave given a default login to a system. this makes the internet less safe for everyone. please change your default passwords! setting up networking the next step is to set up networking on the master. first, set up wifi. this is going to be the link between your cluster and the outside world. edit the /boot/user-data file. update the wifi ssid and password to match your environment. if you ever want to switch networks, this is the file you need to edit. once you have edited this, reboot with sudo reboot and validate that your net‐ working is working. the next step in networking is to set up a static ip address for your cluster’s internal network. to do this, edit /etc/network/interfaces.d/eth0 to read: allow-hotplug eth0 iface eth0 inet static  address 10.0.0.1  netmask 255.255.255.0  broadcast 10.0.0.255  gateway 10.0.0.1 this sets the main ethernet interface to have the statically allocated address 10.0.0.1. reboot the machine to claim the 10.0.0.1 address. next, we’re going to install dhcp on this master so it will allocate addresses to the worker nodes. run: $ apt-get install isc-dhcp-server then configure the dhcp server as follows (/etc/dhcp/dhcpd.conf): building a raspberry pi kubernetes cluster | 237 # set a domain name, can basically be anything option domain-name \"cluster.home\"; # use google dns by default, you can substitute isp-supplied values here option domain-name-servers 8.8.8.8, 8.8.4.4; # we\\'ll use 10.0.0.x for our subnet subnet 10.0.0.0 netmask 255.255.255.0 {  range 10.0.0.1 10.0.0.10;  option subnet-mask 255.255.255.0;  option broadcast-address 10.0.0.255;  option routers 10.0.0.1; } default-lease-time 600; max-lease-time 7200; authoritative; you may also need to edit /etc/defaults/isc-dhcp-server to set the interfaces environ‐ ment variable to eth0. restart the dhcp server with sudo systemctl restart isc-dhcp-server. now your machine should be handing out ip addresses. you can test this by hooking up a second machine to the switch via ethernet. this second machine should get the address 10.0.0.2 from the dhcp server. remember to edit the /etc/hostname file to rename this machine to node-1. the final step in setting up networking is setting up network address translation (nat) so that your nodes can reach the public internet (if you want them to be able to do so). edit /etc/sysctl.conf and set net.ipv4.ipforward=1 to turn on ip forwarding. you should then reboot the server for this to take effect. alternately you can run sudo sysctl net.ipv4.ipforward=1 to make the change without rebooting. if you choose to do this you will still want to edit /etc/sysctl.conf to make the setting permanent. then edit /etc/rc.local (or the equivalent) and add iptables rules for forwarding from eth0 to wlan0 (and back): iptables -t nat -a postrouting -o wlan0 -j masquerade iptables -a forward -i wlan0 -o eth0 -m state \\\\  --state related,established -j accept iptables -a forward -i eth0 -o wlan0 -j accept at this point, the basic networking setup should be complete. plug in and power up the remaining two boards (you should see them assigned the addresses 10.0.0.3 and 10.0.0.4). edit the /etc/hostname file on each machine to name them node-2 and node-3, respectively. 238 | appendix a: building a raspberry pi kubernetes cluster validate this by first looking at /var/lib/dhcp/dhcpd.leases, and then ssh to the nodes (remember again to change the default password first thing). validate that the nodes can connect to the external internet. extra credit there are a couple of extra steps you can take that will make it easier to manage your cluster. the first is to edit /etc/hosts on each machine to map the names to the right addresses. on each machine, add: ... 10.0.0.1 kubernetes 10.0.0.2 node-1 10.0.0.3 node-2 10.0.0.4 node-3 ... now you can use those names when connecting to those machines. the second is to set up passwordless ssh access. to do this, run ssh-keygen and then copy the $home/.ssh/idrsa.pub file into /home/pirate/.ssh/authorizedkeys on node-1, node-2, and node-3. installing kubernetes at this point you should have all nodes up, with ip addresses and capable of accessing the internet. now it’s time to install kubernetes on all of the nodes. using ssh, run the following commands on all nodes to install the kubelet and kubeadm tools. you will need to be root to execute these commands. use sudo su to elevate to the root user. first, add the encryption key for the packages: # curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - then add the repository to your list of repositories: # echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" \\\\  >> /etc/apt/sources.list.d/kubernetes.list finally, update and install the kubernetes tools. this will also update all packages on your system for good measure: # apt-get update $ apt-get upgrade $ apt-get install -y kubelet kubeadm kubectl kubernetes-cni building a raspberry pi kubernetes cluster | 239 kubernetes uses a number of different kernel cgroups when it starts. it expects these capabilities to be present and can fail to start if they are not. make sure that you are running the latest kernel available in the raspberry pi distribution. setting up the cluster on the master node (the one running dhcp and connected to the internet), run: $ sudo kubeadm init --pod-network-cidr 10.244.0.0/16 \\\\  --apiserver-advertise-address 10.0.0.1 \\\\  --apiserver-cert-extra-sans kubernetes.cluster.home note that you are advertising your internal-facing ip address, not your external address. eventually, this will print out a command for joining nodes to your cluster. it will look something like: $ kubeadm join --token=<token> 10.0.0.1 ssh onto each of the worker nodes in your cluster and run that command. when all of that is done, you should be able to run this command and see your work‐ ing cluster: $ kubectl get nodes setting up cluster networking you have your node-level networking set up, but you need to set up the pod-to-pod networking. since all of the nodes in your cluster are running on the same physical ethernet network, you can simply set up the correct routing rules in the host kernels. the easiest way to manage this is to use the flannel tool created by coreos. flannel supports a number of different routing modes; we will use the host-gw mode. you can download an example configuration from the flannel project page: $ curl https://rawgit.com/coreos/flannel/master/documentation/kube-flannel.yml \\\\  > kube-flannel.yaml the default configuration that coreos supplies uses vxlan mode instead, and also uses the amd64 architecture instead of arm. to fix this, open up that configuration file in your favorite editor; replace vxlan with host-gw and replace all instances of amd64 with arm. 240 | appendix a: building a raspberry pi kubernetes cluster you can also do this with the sed tool in place: $ curl https://rawgit.com/coreos/flannel/master/documentation/kube-flannel.yml \\\\  | sed \"s/amd64/arm/g\" | sed \"s/vxlan/host-gw/g\" \\\\  > kube-flannel.yaml once you have your updated kube-flannel.yaml file, you can create the flannel net‐ working setup with: $ kubectl apply -f kube-flannel.yaml this will create two objects, a configmap used to configure flannel and a daemonset that runs the actual flannel daemon. you can inspect these with: $ kubectl describe --namespace=kube-system configmaps/kube-flannel-cfg $ kubectl describe --namespace=kube-system daemonsets/kube-flannel-ds setting up the gui kubernetes ships with a rich gui. you can install it by running: $ dashsrc=https://raw.githubusercontent.com/kubernetes/dashboard/master $ curl -ssl \\\\  $dashsrc/src/deploy/recommended/kubernetes-dashboard-arm-head.yaml \\\\  | kubectl apply -f - to access this ui, you can run kubectl proxy and then point your browser to http:// localhost:8001/ui, where localhost is local to the master node in your cluster. to view this from your laptop/desktop, you may need to set up an ssh tunnel to the root node using ssh -l8001:localhost:8001 <master-ip-address>. summary at this point you should have a working kubernetes cluster operating on your rasp‐ berry pis. this can be great for exploring kubernetes. schedule some jobs, open up the ui, and try breaking your cluster by rebooting machines or disconnecting the network. building a raspberry pi kubernetes cluster | 241  index a abstraction pattern, 205 admin role, 171 admission controllers, 196 built-in and custom, 196 creating ssl-based admission controller, 203 implementing to provide defaulting, 203 installing validating admission controller, 200 validating, 200 admissionreview type, 203 aggregating clusterroles, 173 ambassador ingress controller, 100 annotations, 65, 71-73 change-cause, 121 defining, 72 in deployment template for update, 119 kubectl annotate command, 40 kubernetes.io/created-by, 108 kubernetes.io/ingress.class, 97 uses of, 72 apis decoupling servers via, 6 kubernetes api server request flow, 196 kubernetes api server, versions, 31 application container images, 13 building with docker, 16-20 image security, 19 optimizing image size, 18 using dockerfiles, 16 multistage builds, 20 application containers, 16 application-oriented container apis, benefits of, 9 applications easy scaling for, 6 organizing kubernetes applications, 221-233 code review, 222 deploying your application worldwide, 230-233 feature flag gates and guards, 223 filesystem layout, 224 filesystems as source of truth, 222 guiding principles, 221 managing applications in source control, 224 managing releases, 225 parameterizing applications with tem‐ plates, 229 structuring for development, testing, and deployment, 227-228 versioning with branches and tags, 225 versioning with directories, 226 real-world, deploying, 207-219 ghost, 211-214 jupyter, 207-209 parse, 209-211 redis, 214-219 in same and different pods, 46 authentication, 168 authentication providers supported by kubernetes, 168 to container registries, 22 authorization, 168 testing with can-i, 172 243 autocompletion for commands and resources, 42 autoscaling, 6 replicasets, 110 based on cpu usage, 111 availability, 1 az cli tool, installing, 28 azure cloud shell, 28 azure kubernetes service, installing kuber‐ netes with, 28 b base64 encoding, 162 bash-completion package, 42 branches, versioning with, 226 build image, 21 c caching, using volumes for, 60 cert-manager project, 99 certificate signing request for kubernetes api server, 203 certificates tls, 99 creating secret to store, 158 webhook, 201 cgroup technology (linux kernel), 24 containers in pods, 46 change-cause annotation, 119, 121 cli (command-line interface) azure cloud shell, 28 deploying containers with docker cli, 23 elastic kubernetes service and eksctl tool, 29 clients client-go library, informers, 199 rolling service updates and, 124 cloud container registries for different cloud pro‐ viders, 22 dns name for databases and other services, 180 infrastructure provided by, drawbacks of, 9 ingress implementations by providers, 99 installing kubernetes on public cloud pro‐ vider, 28-29 azure kubernetes service, 28 google kubernetes engine (gke), 28 kubernetes services, 27 kubernetes-as-a-service (kaas) on public clouds, 8 load-balancing capabilities by providers, 90 loadbalancer type, using, 81 storage in, 178 using daemonsets to install software on nodes, 132 volume types for providers, 182 cloud-native applications, 1 cluster ips, 77 dns address for, 77 environment variables, 85 kube-proxy and, 84 cluster-admin permissions, 91 cluster-admin role, 171 clusterrolebindings, 170 built-in, 171 clusterroles, 170 aggregating, 173 binding a group to, 174 built-in, 171 modifications of built-in roles, 171 clusters autoscaling, 111 cloud-native, 132 components, 34-36 kubernetes dns server, 34 kubernetes proxy, 34 kubernetes ui, 35 deploying, 27-31 installing kubernetes locally, 29 installing kubernetes on public cloud provider, 28-29 running kubernetes in docker, 30 running kubernetes on raspberry pi, 31 easy scaling for, 6 exploring with kubectl, 31-34 checking cluster status, 31 listing worker nodes, 32 listing running pods in, 49 mongodb cluster creation, automating, 189-192 viewing with tools other than kubectl, 42 cname records (dns), 180 code review for applications, 222 command-line arguments, configmap used for, 155, 156 communication/synchronization, using vol‐ umes, 60 244 | index compilers, 205 compute costs, forecasting with kubernetes, 6 configmaps, 153-157 creating, 153 creating for ghost, 212 ghost-config-mysql, 213 creating for redis installation, 216 data values, 162 managing, 162 creating configmaps, 163 listing all configmaps in a namespace, 162 updates, 163-165 viewing raw data, 163 mongodb configmap, 190 naming constraints, key names for data items, 161 using, 154 using to add script to mongodb image, 190 configurations container configuration file, 16 declarative configuration in kubernetes, 4 declarative configuration in pod manifests, 47 for deployments, 117 for rolling update, 125 ghost application, 211 ingress controller configuration, typical, 90 kubectl configuration file, 37 managing for load balancer, 90 parameterizing, 229 redis installation, 215 consumers job, creating for a work queue, 149 container images, 14-16 cleaning up or removing, 24 docker format, 15 making reusable, 153 multistage builds, 20 storing in remote registry, 22 updating, 119 container network interface (cni) plug-ins, 196 container registries, 14 (see also registries) containers and container apis, benefits of, 2 application container images, 13 copying files to and from, 53 creating for jupyter application, 208 decoupling application container image from machines, 7 docker runtime, 23 existing containers, adoption by replicasets, 105 grouping in pods, 46 criteria for, 47 immutable container images, 3 quarantining, 105 readiness checks, 55 resource requests per container, 57 restart policy for mongodb container, 191 running in a pod, information about, 51 system and application, 16 contexts, managing with kubectl, 37 contour ingress controller, installing, 91 configuring dns, 92 configuring local hosts file, 92 controller-manager, 32 controllers for custom resources, 199 core-dns server, 35 cpu-shares functionality (linux kernel), 58 cpus autoscaling based on usage, 111 capping usage with resource limits, 59 limiting usage with docker, 24 resource requests for, 57 cron jobs declaring a cronjob in kubernetes, 150 setting up to run image garbage collector, 25 curl utility, using to communicate with work queue, 148 custom resources, 197 naming, 197 patterns for, 204-205 compilers, 205 just data, 204 operators, 205 validation and defaulting, 200 customresourcedefinition, 91, 197 example definition, 197 spec subobject, 197 d daemonsets, 34, 131-138 creating, 132-134 deleting, 137 limiting to specific nodes, 134-136 adding labels to nodes, 135 index | 245 using node selectors, 135 scheduler, 132 similarities with replicasets, 131 updating, 136 rollingupdate strategy, 136 dashboards for worldwide application deployment, 233 kubernetes ui, 35 data items, specifying for configmaps or secrets, 163 debugging, kubectl commands for, 40-41, 52 declarative configuration, 47 and undoing rollouts, 122 declarative configuration objects, 4 decoupled architectures, 5 daemonsets and replicasets in kubernetes, 132 in kubernetes, 104 default-http-backend service, 95 dependencies, 19 deployment image, 21 deployment object, 72, 113 (see also deployments) deleting only the deployment object, 128 revision history attached to, 122 deployments, 113-129 creating, 116 creating with kubectl run, 76 deleting, 128 deploying real-world applications, 207-219 ghost, 211-214 jupyter, 207-209 parse, 209-211 redis, 214-219 deploying your application around the world, 230-233 architectures for worldwide deployment, 230 dashboards and monitoring, 233 implementing worldwide deployment, 232 editing to add readiness check, 78 internal workings of, 114-115 kubernetes deployment lifecycle, 128 managing, 117 monitoring, 128 strategies for, 123 recreate strategy, 123 rollingupdate strategy, 123-126 slowing rollouts to ensure service health, 126 updating, 118-123 container image, 119 rollout history, 120 scaling deployments, 118 development teams, scaling with microservices, 7 development, structuring your application for, 227 introducing a development tag, 228 directories, versioning with, 226 disk space on a node, getting information about, 33 distributed systems, reliable and scalable, 1 dns address for cluster ip, 77 configuring to external address for load bal‐ ancer, 92 entries for mongodb statefulset, 188 kubernetes dns server, 34 kubernetes dns service, 77 name resolution, limitations of, 75 names for external services, 179 docker, 14 building application images with, 16-20 cli tool, deploying container with, 23 container runtime, 23 image format, 15 private registries, storing access credentials, 160 running kubernetes in, 30 docker desktop, kubernetes installation with, 29 docker hub, 22 docker images command, 25 docker login command, 22 docker rmi command, 24 docker run command --cpu-shares flag, 24 --memory and --memory-swap flags, 24 --publish, -d, and --name flags, 23 docker system prune tool, 25 docker-in-docker clusters, 27 dockerfiles, 17 for multistage application builds, 21 .dockerignore file, 17 dynamic volume provisioning, 185 246 | index e eclipse, 42 edit role, 171 in clusterrole aggregation, 173 editors editing a deployment in, 78 plug-ins for integration with kubernetes, 42 efficiency provided by kubernetes, 10 eks (elastic kubernetes service), 29 eksctl command-line tool, 29 elasticsearch tool, 53 endpoints, 82 for default http backend service, 95 for external services, 180 watch command on, 79 environment variables for cluster ips, 85 for parse-server, 209 setting using configmap, 155, 156 envoy load balancer, 91 etcd server, 32 events related to pods, 50 getting information on, 51 killing a container, 55 exec probes, 56 running commands in your container with, 53 extending kubernetes, 195-206 getting started, using kubebuilder project library, 205 meaning of, 195 patterns for custom resources, 204-205 points of extensibility, 196-204 externalname type, 179 f feature flag gates and guards, 223 filesystems configmap defining small filesystem, 153, 154 layout for kubernetes application, 224 layout for parameterization, 230 mounting host filesystem using volumes, 61 overlay, 15 source of truth for kubernetes applications, 222 fluentd tool, 53 creating fluentd logging agent on every node in target cluster, 132-134 forecasting future compute costs, 6 g garbage collection, setting up for container images, 25 gcloud tool, installing, 28 ghost, 211-214 configuring, 211 using mysql for storage, 213 github cert-manager on, 99 minikube on, 30 parse-server page, 211 gloo ingress controller, 100 google cloud platform, 28 google container registry, 22 google kubernetes engine (gke), 28, 81 grace period for pod termination, 51 graceful shutdown, 79 groups, 168 identity and, 174 using for role bindings, 173 h headless services, 188 health checks, 54-56 external services and, 181 for replicated redis cluster, 214 liveness probes, 54 readiness probe, 55, 78, 127 tcpsocket and exec probes, 56 heapster pod, 110 helm, parameterizing configurations with, 229 heptio-contour namespace, 91 horizontal pod autoscaling (hpa), 110 no direct link between replicasets and, 111 horizontal vs. vertical scaling, 111 hostnames ingress and namespaces, 97 using with ingress, 94 hostpath volume, 61 hosts configuring local hosts file for contour, 92 multiple paths on same host in ingress sys‐ tem, 96 http health checks, 54, 79 http load balancing (see load balancing) https, webhook access via, 201 hypervisors, 30 index | 247 i identity in kubernetes, 168 groups, 174 image pull secrets, 161 immutability declarative configuration and, 4 value of, 3 imperative vs. declarative configuration, 4, 47 informer pattern, 199 infrastructure abstracting, 9 as code, 4 immutable, 3, 132 ingress, 7, 89-101 advanced topics and gotchas, 96-99 ingress and namespaces, 97 multiple ingress objects, 97 path rewriting, 98 running multiple ingress controllers, 97 serving tls, 98 alternate implementations, 99 installing contour controller, 91 configuring dns, 92 configuring local hosts file, 92 resource specification vs. controller imple‐ mentation, 90 typical software ingress controller configu‐ ration, 90 using, 92 hostnames, 94 paths, 95 simplest usage, 93 intellij, 42 ip addresses cluster ip, 77 external ip for contour, 91 external services and, 179 for watched service endpoints, 82 hosting many http sites on single address, 89 kube-proxy and cluster ips, 84 unique, for services of type loadbalancer, 92 iptables rules, 84 istio project, 100 j jobs, 139-151 cronjob, 150 job object, 139 patterns, 140-150 work queues, 146-150 json files representing kubernetes objects, 39 work queue items, 148 jupyter, 207-209 just data pattern, 204 k kaas (kubernetes-as-a-service), 8 kube-apiserver, --service-cluster-ip-range flag, 85 kube-dns server, 35 kube-proxy, 34, 86 cluster ips and, 84 kube-system namespace, 34, 95 listing pods in, 110 kubeadm, 27 kubebuilder project, library for kubernetes api extensions, 205 kubectl tool, 31-34 checking cluster status, 31 commands, 37-43 apply, 49, 91, 93, 107, 110 apply -f, 164 auth can-i, 172 auth reconcile, 172 autocompletion, 42 autoscale, 111 contexts, 37 cp, 53 create, 114 create configmap, 163 create secret, 159, 163 create secret docker-registry, 161 create secret generic, 164 create secret tls, 98 creating, updating, and destroying kubernetes objects, 39 debugging, 40-41 delete, 48, 51, 111, 128, 137 describe, 50, 80, 93, 108, 117, 159 edit, 78, 80 edit configmap, 165 exec, 53 expose, 76 get, 48, 49, 68, 93 get clusterrolebindings, 171 248 | index get clusterroles, 171 get configmaps, 162 get secrets, 162 label, 68, 135 labeling and annotating objects, 40 logs, 52 namespaces, 37 port-forward, 52, 77 replace --save-config, 117 replace -f, 164, 164 rolling-update, 114 rollout, 118, 120, 137 rollout history deployment, 120 rollout pause deployments, 120 rollout resume deployments, 120 rollout undo deployments, 121 run, 48 scale, 109, 115 viewing kubernetes api objects, 38 default-http-backend service, 95 getting more information via command-line flags, 50 installing, 29, 29 listing worker nodes on a cluster, 32 kubelet tool, 49 managing secrets volumes, 159 terminating containers for excess memory usage, 58 kubernetes objects, 38, 39 (see also objects (kubernetes)) kubernetes service, 76 kubernetes-as-a-service (kaas), 8 kubernetes.io/created-by annotation, 108 kubernetes.io/ingress.class annotation, 97 l labels, 65-71 adding to nodes, 135 applying, 67 deployment object, 114 external services without label selectors, 179 for kubernetes objects, using kubectl, 40 for pods managed by a replicaset, 108 for pods, use by replicasets, 107 in kubernetes architecture, 71 key/value pairs, rules for, 66 modifying, 68 modifying on sick pod, 105 motivations for using, 66 node selectors, 135 selectors, 68, 76 in api objects, 70 using in service discovery, 83 using to run daemonset pods on specific nodes, 131 “let\\'s encrypt” free certificate authority, 99 libraries, external and shared, 13 lifecycle stages, mapping to revisions, 228 linkerd project, 100 live updates, 165 liveness probes, 54 load balancers cloud-based, configuring with ingress objects, 99 decoupling components via, 6 load balancing, 7 for kubernetes dashboard server, 35 for kubernetes dns server, 35 http load balancing with ingress, 89-101 alternate ingress implementations, 99 ingress spec vs. ingress controllers, 90 installing contour controller, 91-92 using ingress, 92-96 loadbalancer type, 81, 89 loadtest custom resource, 198 creating, 199 validation, 200 logging agent fluentd, creating on every node in target cluster, 132 logical and, 69 login token for jupyter application, 208 logs for jupyter application container, 208 getting information on pods from, 52 log aggregation services, 53 testing with auth can-i, 172 m machine/operating system (os), decoupling from application container, 8 manifests (pod), 47 adding volume to, 59 created by replicasets, 106 creating, 48 declaring secrets volume, 160 using to delete a pod, 51 master and slave replicas, redis installation, 215 index | 249 master nodes, 32 maxsurge parameter (rolling updates), 126 maxunavailable parameter (rolling updates), 125, 137 memory capping usage with resource limits, 59 limiting usage for applications in container, 24 on a node, getting information about, 33 requests for, 58 meta-resources, 197 metadata labels providing metadata for objects, 65 metadata section in kubernetes objects, annotation definitions in, 73 metadata section (pod manifests), 49 microservices, 6 building with kubernetes, advantages of, 7 represented by replicasets, 105 minikube, 27 minikube tunnel command, 91 using to install kubernetes locally, 29 minreadyseconds parameter for daemonsets, 137 for deployments), 127 mobile application, accessing kubernetes clus‐ ter from your phone, 42 modularity in kubernetes, 104 mongodb cluster creation, automating, 189-192 manually replicated with statefulsets, 187-189 readiness probes for mongo-serving con‐ tainers, 193 use of mongodb cluster by parse, 209 multistage image builds, 20 multitenant security, 167 mutable vs. immutable infrastructure, 3 mutatingwebhookconfiguration, 203 mysql databases running a mysql singleton, 181-185 using with ghost application, 213 n namespaces, 7, 11 creating for jupyter application, 207 default, changing with kubectl, 37 deployment of kubernetes objects into, 178 heptio-contour, 91 in annotation keys, 72 in kubectl commands, 37 ingress and, 97 network traffic, restricting in a cluster, 71 network-based storage, 61 networkpolicy, 71 newreplicaset, 118 nfs persistent volume object, 182 nginx ingress controller, 100 nodeports, 80, 89 services type for installing contour, 91 nodes limiting daemonsets to specific nodes, 134-136 adding labels to nodes, 135 using node selectors, 135 listing worker nodes for kubernetes cluster, 32 nodename field in pod specs, 132 resource use by, monitoring with kubectl, 41 “not my monkey, not my circus” line, 8 o objects (kubernetes), 38 annotations, 71-73 creating, updating, and destroying, 39 labeling and annotating with kubectl, 40 labels providing metadata for, 65 selectors in, 70 oldreplicasets, 118 openapi, 200 operations teams, decoupling of, 8 operators extensions using, 205 health detection and healing with, 5 osi model, 89 overlay filesystems, 15 p parse application, 209-211 building the parse-server, 209 deploying the parse-server, 209 prerequisites for, 209 testing, 210 paths in http requests, using with ingress, 95 ingress and namespaces, 97 rewriting with ingress controller, 98 pending state, 50 250 | index persistent volume claim template, 193 persistentvolume, 10 persistentvolumeclaim, 10, 183 reclamation policy, and lifespan of persis‐ tent volumes, 186 referring to a storage class, 185 persistentvolumes, 51 persisting data with volumes, 59-61 pods, 7, 45-63 accessing, 52 copying files to/from containers, 53 getting more information with logs, 52 running commands in container with exec, 53 using port forwarding, 52 configmaps and, 153 created by replicaset using pod template, 106 creating via kubectl run command, 48 currently running in a cluster, listing and showing labels, 69 currently running on a node, getting infor‐ mation about, 33 daemonsets determining which node pods run on, 132 decoupling from daemonsets, 132 deleting, 51 designing, question to ask, 46 example pod with two containers and shared filesystem, 45 finding a replicaset from, 108 finding set of pods for a replicaset, 108 getting details about, 50 health checks, 54-56 liveness probes, 54 other types of, 56 readiness probe, 55, 78, 127 horizontal pod autoscaling (hpa), 110 imagepullsecrets spec field, 161 in kubernetes, 46 managed by daemonsets, deleting (or not), 137 managed by replicasets automatic rescheduling in failures, 103 deleting, 111, 128 manifests, 47 creating, 48 declaring secrets volume, 160 maximum number unavailable during roll‐ ing updates, 125, 137 node selectors in pod spec when creating daemonsets, 135 persisting data with volumes, 59-61 adding volumes to pods, 59 using volumes for communication/ synchronization, 60 pod template in deployment specs, 117 relating pods and replicasets, 104 replicated sets of, 103 (see also replicasets) replicating a set of, reasons for, 131 resource management for, 56-59 capping usage with resource limits, 58 monitoring resource use with kubectl, 41 resource requests and minimum required resources, 56 running, 49 in parallel for consumers job, 149 listing running pods in a cluster, 49 singleton pod running mysql, 184 updating in rollingupdate strategy, 123 port forwarding, 77 for updated deployment definition, 79 setting up for jupyter container, 208 testing with auth can-i, 172 using to access a pod, 52 using to connect to work queue daemon, 147 port-forward command (kubectl), 41 ports in deployment definitions, 76 nodeport feature, 80 private container registries, 22 storing access credentials for private docker registries, 160 process health checks, 54 (see also health checks) progressdeadlineseconds parameter (deploy‐ ments), 127, 129 progressing type, 129 proxy (kubernetes), 34 (see also kube-proxy) public container registries, 22 r raspberry pi building a kubernetes cluster, 235-241 index | 251 running kubernetes on, 31 rbac (see role-based access control) rbac.authorization.kubernetes.io/autoupdate annotation, 171 readiness probes, 55, 127 for mongo-serving containers, 193 for services, 78 reconciliation loops, 104, 132 recreate strategy, 123 redis, 214-219 configuring, 215 creating a redis service, 216 deploying redis cluster, 217 redis-sentinel, 214 redis-server, 214 testing our redis cluster, 218 registries (container), 14 private docker registries, storing access cre‐ dentials, 160 storing container images in remote registry, 22 regular expressions for key names in configmap or secret key, 161 for path rewriting by ingress controllers, 98 releases, progression of, 227-228 development tag, 228 mapping stages to revisions, 228 reliability, 1 remote disks, persisting data with, 61 replicas (statefulsets), 187 replicasets, 48, 103-112 autoscaling no direct link between hpa and replica‐ sets, 111 creating, 107 creating for mysql singleton pod, 183 creating to manage singleton work queue daemon, 146 deleting, 111 designing with, 105 for mongodb, 189 inspecting, 108 finding a replicaset from a pod, 108 finding a set of pods for a replicaset, 108 managed by a deployment, 114 deleting, 128 fields pointing to, 118 old and new, managed by a deployment, 120 reconciliation loops, 104 relating pods and replicasets, 104 relationship between deployments and, 115 scaling, 109-111 autoscaling, 110 declarative scaling with kubectl apply, 109 imperative scaling with kubectl scale, 109 similarities with daemonsets, 131 specification for, 106 labels, 107 pod templates, 106 resource management, 56-59 capping usage with resource limits, 58 resource requests and minimum required resources, 56 request limit details, 57 resources custom, 197 (see also custom resources) external, connecting to kubernetes services, 86 isolation with containers, 46 limiting applications’ usage, 24 monitoring use of with kubectl top com‐ mand, 41 rest api, kubernetes, 159 restart policy for pods, 55, 191 revisionhistorylimit property, 123 revisions, 120 mapping to stages, 228 specifying revision of 0, 122 role object, 169 role-based access control (rbac), 167-175 advanced topics aggregating clusterroles, 173 using groups for bindings, 173 general concept of roles and role bindings, 169 identity in kubernetes, 168 multitenant security and, 167 roles and role bindings in kubernetes, 169 auto-reconciliation of built-in roles, 171 using built-in roles, 171 verbs for kubernetes roles, 170 techniques for managing, 172 managing in source control, 172 252 | index testing authorization with can-i, 172 rolebinding object, 169 creating, 170 rolling updates, 114 rollingupdate strategy, 123 configuring a rolling update, 125 managing multiple versions of your service, 124 using with daemonsets, 136 rollout commands for deployments, 118 rollouts current status of daemonset rollout, 137 history of, 120 and undoing a deployment, 122 monitoring, 120 pausing, 120 resuming, 120 slowing to ensure service health, 126 undoing last rollout, 121 root path (/), 96 s scalability, 1 scaling your service and your teams, 5-9 decoupled architectures, 6 easy scaling for applications and clusters, 6 scaling development teams with micro‐ services, 7 separation of concerns for consistency and scaling, 8 scaling deployments, 115, 118 replicasets, 109-111, 115 autoscaling, 110 declarative scaling with kubectl apply, 109 imperative scaling with kubectl scale, 109 scheduler, 32 daemonsets and, 132 placing pods onto nodes, 48 secrets, 158-161 consuming, 159 secrets volumes, 159 storing access credentials for private docker registries, 160 creating, 158 data values, 162 managing, 162 creating secrets, 163 listing all secrets in current namespace, 162 updates, 163-165 viewing raw data, 163 naming constraints, key names for data items, 161 specifying with tls certificate and keys, 98 security, 167 (see also authentication; role-based access control; secrets) for application container images, 19 selectors (label), 68, 76 filtering nodes based on labels, 135 finding pods matching, 108 identifying clusterroles to be aggregated, 173 in api objects, 70 in replicaset spec section, 107 in the kubernetes architecture, 71 selector operators, 70 self-healing systems, 5 separation of concerns, 8 service accounts, 168 service discovery, 75-87 advanced details, 82-85 cluster ip environment variables, 85 endpoints, 82 kube-proxy and cluster ips, 84 manual service discovery, 83 cloud integration, 81 connecting with other environments, 86 defined, 75 looking beyond the cluster, 79 readiness checks, 78 service dns, 77 service object, 76 service meshes, 100 service object, 84 endpoints object for, 82 operating at osi level 4, 89 services backend, creating, 92 creating redis service, 216 default-http-backend, 95 ensuring health by slowing rollouts, 126 exposing ghost service, 213 exposing mysql singleton as, 184 index | 253 hosting multiple services on paths of a sin‐ gle domain, 95 importing external storage services, 178-181 limitation, no health checking, 181 services without label selectors, 179 ingress and namespaces, 97 kubernetes, 7 managing multiple versions during rolling updates, 124 of type loadbalancer, 90 (see also ingress; loadbalancer type) queue service, creating, 147 stateless, replicasets designed for, 106 shutdown, graceful, 79 singletons running reliable singletons for storage, 181-186 dynamic volume provisioning, 185 software on a node, getting information about, 33 source control managing rbac in, 172 managing your application in, 224 storing declarative configuration in, 4 spec for customresourcedefinition, 197 spec for replicasets, 106 spec section (pod manifests), 49 spec.type field, 80, 81 spec.volume section (pod manifest), 59 ssh tunneling, 80 state daemonset management of, 132 desired state of a deployment, matching, 115 of all replicas managed by a replicaset, 108 reconciliation loop approach to managing, 104 stateless services, 106 updating desired state of a deployment, 119 statefulsets, 186-194 for redis cluster, 217 manually replicated mongodb with, 187-189 mongodb cluster creation, automating, 189-192 persistent volumes and, 192 properties of, 187 redis deployment, wrapper scripts for, 215 status.conditions array (deployments), 128 storage solutions, integrating with kubernetes, 177-194 importing external services, 178-181 limitation, no health checking, 181 services without selectors, 179 mysql database for ghost storage, 213 native storage with statefulsets, 186-194 manually replicated mongodb, 187-189 persistent volumes and statefulsets, 192 readiness probes for mongo-serving containers, 193 running reliable singletons, 181-186 dynamic volume provisioning, 185 mysql singleton, 181-185 storageclass objects, 185 strategy object, 117 system containers, 16 system daemons, deploying, 131 (see also daemonsets) system:unauthenticated group, 168 cluster role allowing access to api server, 171 t tab completion for commands and resources, 42 tags, source-control, 226 tcp, 89 tcpsocket health checks, 56 templates annotation in template for deployment with update information, 119 for replicasets and pods, 68 (see also deployments) parameterizing applications with, 229 pod template in deployment specs, 117 terminating state, 51 testing creating test environments with kubernetes, 11 structuring your application for, 227 timing out a rollout, 127, 128 tls key and certificate, creating secret to store, 158 tls, serving in ingress system, 98 tokens (login) for jupyter application, 208 traefik, 100 254 | index u udp, 89 ui (kubernetes), 35 undoing rollouts, 121 update strategy, configuring for daemonsets, 136 updating configmaps or secrets, 163 editing configmap current version, 164 live updates, 165 recreate and update, 164 update from file, 164 user accounts, 168 utf-8 text, configmap data values, 162 utilization, 56 v validatingwebhookconfiguration, 200 validation, adding to custom resource, 200 velocity in software development, 2 declarative configuration, 4 immutability, value of, 3 self-healing systems, 5 versioning in kubernetes, 31 managing periodic versions of applications, 225 using directories, 226 with branches and tags, 225 vertical vs. horizontal scaling, 111 view role, 171 virtual hosting, 89 virtualbox, 30 visual studio code, 42 volume.beta.kubernetes.io/storage-class anno‐ tation, 186 volumes, persisting data with, 59-61 configmap volume, creating, 155 different ways to use volumes with pods, 60 caching, 60 communication/synchronization, 60 dynamic volume provisioning, 185 mongo-init configmap volume, 190 persistent volume for mysql singleton, 182 persistent volumes and statefulsets, 192 secrets volumes, 159 using volumes with pods, 59 w watch command, 79, 208 watches, 199 webhooks mutatingwebhookconfiguration, 203 validatingwebhookconfiguration, 200 work queues, 146-150 creating the consumers job, 149 loading up, 148 starting, 146 worker nodes, listing for kubernetes cluster, 32 y yaml deployment.yaml file, 114, 116 files representing kubernetes objects, 39 for configmap object, 154 host-ingress.yaml file, 94 path-ingress.yaml file, 95 simple-ingress.yaml file, 93 tls-secret.yaml file, 98 index | 255 about the authors brendan burns began his career with a brief stint in the software industry followed by a phd in robotics focused on motion planning for human-like robot arms. this was followed by a brief stint as a professor of computer science. eventually, he returned to seattle and joined google, where he worked on web search infrastructure with a special focus on low-latency indexing. while at google, he created the kuber‐ netes project with joe beda and craig mcluckie. brendan is currently a director of engineering at microsoft azure. joe beda started his career at microsoft working on internet explorer (he was young and naive). throughout 7 years at microsoft and 10 at google, joe has worked on gui frameworks, real-time voice and chat, telephony, machine learning for ads, and cloud computing. most notably, while at google, joe started the google compute engine and, along with brendan burns and craig mcluckie, created kubernetes. along with craig, joe founded and sold a startup (heptio) to vmware, where he is now a principal engineer. joe proudly calls seattle home. kelsey hightower is a principal developer advocate at google working on google’s cloud platform. he has helped develop and refine many google cloud products including google’s kubernetes engine, cloud functions, and apigees’s api gateway. kelsey spends most of his time with executives and developers spanning the global fortune 1000, helping them understand and leverage google technologies and plat‐ forms to grow their businesses. kelsey is a huge open source contributor, maintaining projects that aid software developers and operations professionals in building and shipping cloud native applications. he is an accomplished author and keynote speaker, and was the inaugural winner of the cncf top ambassador award for help‐ ing bootstrap the kubernetes community. he is a mentor and technical advisor, help‐ ing founders turn their visions into reality. colophon the animal on the cover of kubernetes: up and running is an atlantic white-sided dolphin (lagenorhynchus acutus). as its name suggests, the white-sided dolphin has light patches on its sides and a light gray strip that runs from above the eye to below the dorsal fin. it is among the largest species of oceanic dolphins, and ranges throughout the north atlantic ocean. it prefers open water, so it is not often seen from the shore, but will readily approach boats and perform various acrobatic feats. white-sided dolphins are social animals commonly found in large groups (known as pods) of about 60 individuals, though the size will vary depending on location and the availability of food. dolphins often work as a team to harvest schools of fish, but they also hunt individually. they primarily search for prey using echolocation, which is similar to sonar. the bulk of this marine mammal’s diet consists of herring, mack‐ erel, and squid. the average lifespan of the white-sided dolphin is between 22–27 years. females only mate every 2–3 years, and the gestation period is 11 months. calves are typically born in june or july, and are weaned after 18 months. dolphins have very great intelligence and display complex social behaviors like grieving, cooperation, and problem solving, due to their high brain-to-body ratio (the highest among aquatic mammals). many of the animals on o’reilly covers are endangered; all of them are important to the world. the cover illustration is by karen montgomery, based on a black and white engraving from british quadrupeds. the cover fonts are gilroy semibold and guardian sans. the text font is adobe minion pro; the heading font is adobe myriad condensed; and the code font is dalton maag’s ubuntu mono. there’s much more where this came from. experience books, videos, live online training courses, and more from o’reilly and our 200+ partners—all in one place. learn more at oreilly.com/online-learning ©2019 o’reilly media, inc. o’reilly is a registered trademark of o’reilly media, inc. | 175'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict = {\"transcript\": text}\n",
    "text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brendan burns, joe beda &amp; kelsey hightower kub...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript\n",
       "0  brendan burns, joe beda & kelsey hightower kub..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.DataFrame(pd.Series(text_dict))\n",
    "data_df.reset_index(drop=True, inplace=True)\n",
    "data_df.columns = [\"transcript\"]\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_pickle(\"bin/corpus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>0104</th>\n",
       "      <th>0195713c8e65088650fdd8b599db377b7ce6c9b10bd13766</th>\n",
       "      <th>02</th>\n",
       "      <th>04</th>\n",
       "      <th>04653</th>\n",
       "      <th>05</th>\n",
       "      <th>0515</th>\n",
       "      <th>055095</th>\n",
       "      <th>06135271a273</th>\n",
       "      <th>...</th>\n",
       "      <th>zaig</th>\n",
       "      <th>zaz0</th>\n",
       "      <th>zcdnb8gefgt0</th>\n",
       "      <th>zero</th>\n",
       "      <th>zfnxdqvust</th>\n",
       "      <th>zon</th>\n",
       "      <th>zone</th>\n",
       "      <th>zr6l7</th>\n",
       "      <th>zsh</th>\n",
       "      <th>zzmlw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 5301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  0104  0195713c8e65088650fdd8b599db377b7ce6c9b10bd13766  02  04  04653  \\\n",
       "0   2     1                                                 1   2   2      2   \n",
       "\n",
       "   05  0515  055095  06135271a273  ...  zaig  zaz0  zcdnb8gefgt0  zero  \\\n",
       "0   4     1       1             1  ...     1     1             1     5   \n",
       "\n",
       "   zfnxdqvust  zon  zone  zr6l7  zsh  zzmlw  \n",
       "0           1    1     2      2    1      1  \n",
       "\n",
       "[1 rows x 5301 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words=\"english\")\n",
    "data_cv = cv.fit_transform(data_df.transcript)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_df.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm.to_pickle(\"bin/dtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv, open(\"bin/cv.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
